define(["require", "exports"], function (require, exports) {
    "use strict";
    Object.defineProperty(exports, "__esModule", { value: true });
    exports.sampleCode = "import io\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\nimport cv2\nimport multiprocessing\nimport multiprocessing.connection\nimport time\nimport gym\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot\n\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nclass Orthogonal(object):\n    def __init__(self, scale=1.):\n        self.scale = scale\n\n    def __call__(self, shape, dtype=None, partition_info=None):\n        shape = tuple(shape)\n        if len(shape) == 2:\n            flat_shape = shape\n        elif len(shape) == 4:  # assumes NHWC\n            flat_shape = (np.prod(shape[:-1]), shape[-1])\n        else:\n            raise NotImplementedError\n        a = np.random.normal(0.0, 1.0, flat_shape)\n        u, _, v = np.linalg.svd(a, full_matrices=False)\n        q = u if u.shape == flat_shape else v  # pick the one with the correct shape\n        q = q.reshape(shape)\n        return (self.scale * q[:shape[0], :shape[1]]).astype(np.float32)\n\n    def get_config(self):\n        return {\n            'scale': self.scale\n        }\n\n\nclass Game(object):\n    def __init__(self, seed: int):\n        self.env = gym.make('BreakoutNoFrameskip-v4')\n        self.env.seed(seed)\n\n        self.obs_2_max = np.zeros((2, 84, 84, 1), np.uint8)\n\n        self.obs_4 = np.zeros((84, 84, 4))\n\n        self.rewards = []\n\n        self.lives = 0\n\n    def step(self, action):\n\n        reward = 0.\n        done = None\n\n        for i in range(4):\n            obs, r, done, info = self.env.step(action)\n\n            if i >= 2:\n                self.obs_2_max[i % 2] = self._process_obs(obs)\n\n            reward += r\n\n            lives = self.env.unwrapped.ale.lives()\n            if lives < self.lives:\n                done = True\n            self.lives = lives\n\n            if done:\n                break\n\n        self.rewards.append(reward)\n\n        if done:\n            episode_info = {\"reward\": sum(self.rewards),\n                            \"length\": len(self.rewards)}\n            self.reset()\n        else:\n            episode_info = None\n            obs = self.obs_2_max.max(axis=0)\n\n            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=-1)\n            self.obs_4[..., -1:] = obs\n\n        return self.obs_4, reward, done, episode_info\n\n    def reset(self):\n        obs = self.env.reset()\n\n        obs = self._process_obs(obs)\n        self.obs_4[..., 0:] = obs\n        self.obs_4[..., 1:] = obs\n        self.obs_4[..., 2:] = obs\n        self.obs_4[..., 3:] = obs\n        self.rewards = []\n\n        self.lives = self.env.unwrapped.ale.lives()\n\n        return self.obs_4\n\n    @staticmethod\n    def _process_obs(obs):\n        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n        return obs[:, :, None]  # Shape (84, 84, 1)\n\n\ndef worker_process(remote: multiprocessing.connection.Connection, seed: int):\n    game = Game(seed)\n\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \"step\":\n            remote.send(game.step(data))\n        elif cmd == \"reset\":\n            remote.send(game.reset())\n        elif cmd == \"close\":\n            remote.close()\n            break\n        else:\n            raise NotImplementedError\n\n\nclass Worker(object):\n    child: multiprocessing.connection.Connection\n    process: multiprocessing.Process\n\n    def __init__(self, seed):\n        self.child, parent = multiprocessing.Pipe()\n        self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))\n        self.process.start()\n\n\nclass Model(object):\n    def __init__(self, *, reuse: bool, batch_size: int):\n        self.obs = tf.placeholder(shape=(batch_size, 84, 84, 4), name=\"obs\", dtype=np.uint8)\n        obs_float = tf.to_float(self.obs, name=\"obs_float\")\n\n        with tf.variable_scope(\"model\", reuse=reuse):\n            self.h = Model._cnn(obs_float)\n            self.pi_logits = Model._create_policy_network(self.h, 4)\n\n            self.value = Model._create_value_network(self.h)\n\n            self.params = tf.trainable_variables()\n\n        self.action = Model._sample(self.pi_logits)\n\n        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")\n\n        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)\n\n    @staticmethod\n    def _get_policy_entropy(logits: tf.Tensor):\n        a = logits - tf.reduce_max(logits, axis=-1, keepdims=True)\n        exp_a = tf.exp(a)\n        z = tf.reduce_sum(exp_a, axis=-1, keepdims=True)\n        p = exp_a / z\n\n        return tf.reduce_sum(p * (tf.log(z) - a), axis=-1)\n\n    def neg_log_prob(self, action: tf.Tensor, name: str) -> tf.Tensor:\n        one_hot_actions = tf.one_hot(action, 4)\n        return tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=self.pi_logits,\n            labels=one_hot_actions,\n            dim=-1,\n            name=name)\n\n    @staticmethod\n    def _sample(logits: tf.Tensor):\n        uniform = tf.random_uniform(tf.shape(logits))\n        return tf.argmax(logits - tf.log(-tf.log(uniform)),\n                         axis=-1,\n                         name=\"action\")\n\n    @staticmethod\n    def _cnn(unscaled_images: tf.Tensor):\n        scaled_images = tf.cast(unscaled_images, tf.float32) / 255.\n\n        h1 = tf.layers.conv2d(scaled_images,\n                              name=\"conv1\",\n                              filters=32,\n                              kernel_size=8,\n                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),\n                              strides=4,\n                              padding=\"valid\",\n                              activation=tf.nn.relu)\n\n        h2 = tf.layers.conv2d(h1,\n                              name=\"conv2\",\n                              filters=64,\n                              kernel_size=4,\n                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),\n                              strides=2,\n                              padding=\"valid\",\n                              activation=tf.nn.relu)\n\n        h3 = tf.layers.conv2d(h2,\n                              name=\"conv3\",\n                              filters=64,\n                              kernel_size=3,\n                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),\n                              strides=1,\n                              padding=\"valid\",\n                              activation=tf.nn.relu)\n\n        nh = np.prod([v.value for v in h3.get_shape()[1:]])\n        flat = tf.reshape(h3, [-1, nh])\n\n        h = tf.layers.dense(flat, 512,\n                            activation=tf.nn.relu,\n                            kernel_initializer=Orthogonal(scale=np.sqrt(2)),\n                            name=\"hidden\")\n\n        return h\n\n    @staticmethod\n    def _create_policy_network(h: tf.Tensor, n: int) -> tf.Tensor:\n        return tf.layers.dense(h, n,\n                               activation=None,\n                               kernel_initializer=Orthogonal(scale=0.01),\n                               name=\"logits\")\n\n    @staticmethod\n    def _create_value_network(h: tf.Tensor) -> tf.Tensor:\n        value = tf.layers.dense(h, 1,\n                                activation=None,\n                                kernel_initializer=Orthogonal(),\n                                name=\"value\")\n        return value[:, 0]\n\n    def step(self, session: tf.Session, obs: np.ndarray) -> (tf.Tensor, tf.Tensor, tf.Tensor):\n        return session.run([self.action, self.value, self.neg_log_pi],\n                           feed_dict={self.obs: obs})\n\n    def get_value(self, session: tf.Session, obs: np.ndarray) -> tf.Tensor:\n        return session.run(self.value,\n                           feed_dict={self.obs: obs})\n\n\nclass Trainer(object):\n    def __init__(self, model: Model):\n        self.model = model\n\n        self.sampled_obs = self.model.obs\n\n        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")\n        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")\n\n        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],\n                                                           name=\"sampled_normalized_advantage\")\n\n        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")\n        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")\n\n        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")\n\n        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")\n\n        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")\n\n        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")\n\n        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")\n        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,\n                                                       clipped_ratio * self.sampled_normalized_advantage),\n                                            name=\"policy_reward\")\n\n        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")\n\n        value = self.model.value\n\n        clipped_value = tf.add(self.sampled_value,\n                               tf.clip_by_value(value - self.sampled_value, -self.clip_range, self.clip_range),\n                               name=\"clipped_value\")\n        self.vf_loss = tf.multiply(0.5,\n                                   tf.reduce_mean(tf.maximum(tf.square(value - self.sampled_return),\n                                                             tf.square(clipped_value - self.sampled_return))),\n                                   name=\"vf_loss\")\n\n        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)\n\n        params = self.model.params\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)\n\n        adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-5)\n        grads_and_vars = list(zip(grads, params))\n        self.train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")\n\n        self.approx_kl_divergence = .5 * tf.reduce_mean(tf.square(neg_log_pi - self.sampled_neg_log_pi))\n        self.clip_fraction = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range)))\n\n        self.train_info_labels = ['policy_reward',\n                                  'value_loss',\n                                  'entropy_bonus',\n                                  'approx_kl_divergence',\n                                  'clip_fraction']\n\n    def train(self, session: tf.Session, samples: Dict[str, np.ndarray], learning_rate: float, clip_range: float):\n        feed_dict = {self.sampled_obs: samples['obs'],\n                     self.sampled_action: samples['actions'],\n                     self.sampled_return: samples['values'] + samples['advantages'],\n                     self.sampled_normalized_advantage: Trainer._normalize(samples['advantages']),\n                     self.sampled_value: samples['values'],\n                     self.sampled_neg_log_pi: samples['neg_log_pis'],\n                     self.learning_rate: learning_rate,\n                     self.clip_range: clip_range}\n\n        evals = [self.policy_reward,\n                 self.vf_loss,\n                 self.entropy_bonus,\n                 self.approx_kl_divergence,\n                 self.clip_fraction,\n                 self.train_op]\n\n        return session.run(evals, feed_dict=feed_dict)[:-1]\n\n    @staticmethod\n    def _normalize(adv: np.ndarray):\n        return (adv - adv.mean()) / (adv.std() + 1e-8)\n\n\nclass Main(object):\n    def __init__(self):\n        self.gamma = 0.99\n        self.lamda = 0.95\n\n        self.updates = 10000\n\n        self.epochs = 4\n        self.n_workers = 8\n        self.worker_steps = 128\n        self.n_mini_batch = 4\n        self.batch_size = self.n_workers * self.worker_steps\n        self.mini_batch_size = self.batch_size // self.n_mini_batch\n        assert (self.batch_size % self.n_mini_batch == 0)\n\n        Main._init_tf_session()\n\n        self.workers = [Worker(47 + i) for i in range(self.n_workers)]\n\n        self.obs = np.zeros((self.n_workers, 84, 84, 4), dtype=np.uint8)\n        for worker in self.workers:\n            worker.child.send((\"reset\", None))\n        for i, worker in enumerate(self.workers):\n            self.obs[i] = worker.child.recv()\n\n        self.sample_model = Model(reuse=False, batch_size=self.n_workers)\n        self.trainer = Trainer(Model(reuse=True, batch_size=self.mini_batch_size))\n        self.session: tf.Session = tf.get_default_session()\n\n        init_op = tf.global_variables_initializer()\n        self.session.run(init_op)\n\n    def sample(self) -> (Dict[str, np.ndarray], List):\n        rewards = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)\n        actions = np.zeros((self.n_workers, self.worker_steps), dtype=np.int32)\n        dones = np.zeros((self.n_workers, self.worker_steps), dtype=np.bool)\n        obs = np.zeros((self.n_workers, self.worker_steps, 84, 84, 4), dtype=np.uint8)\n        neg_log_pis = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)\n        values = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)\n        episode_infos = []\n\n        for t in range(self.worker_steps):\n            obs[:, t] = self.obs\n            actions[:, t], values[:, t], neg_log_pis[:, t] = self.sample_model.step(self.session, self.obs)\n\n            for w, worker in enumerate(self.workers):\n                worker.child.send((\"step\", actions[w, t]))\n\n            for w, worker in enumerate(self.workers):\n                self.obs[w], rewards[w, t], dones[w, t], info = worker.child.recv()\n\n                if info:\n                    info['obs'] = obs[w, t, :, :, 3]\n                    episode_infos.append(info)\n\n        advantages = self._calc_advantages(dones, rewards, values)\n        samples = {\n            'obs': obs,\n            'actions': actions,\n            'values': values,\n            'neg_log_pis': neg_log_pis,\n            'advantages': advantages\n        }\n\n        samples_flat = {}\n        for k, v in samples.items():\n            samples_flat[k] = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])\n\n        return samples_flat, episode_infos\n\n    def _calc_advantages(self, dones: np.ndarray, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:\n        advantages = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)\n        last_advantage = 0\n\n        last_value = self.sample_model.get_value(self.session, self.obs)\n\n        for t in reversed(range(self.worker_steps)):\n            mask = 1.0 - dones[:, t]\n            last_value = last_value * mask\n            last_advantage = last_advantage * mask\n            delta = rewards[:, t] + self.gamma * last_value - values[:, t]\n            last_advantage = delta + self.gamma * self.lamda * last_advantage\n            advantages[:, t] = last_advantage\n\n            last_value = values[:, t]\n\n        return advantages\n\n    def train(self, samples: Dict[str, np.ndarray], learning_rate: float, clip_range: float):\n        indexes = np.arange(self.batch_size)\n\n        train_info = []\n\n        for _ in range(self.epochs):\n            np.random.shuffle(indexes)\n\n            for start in range(0, self.batch_size, self.mini_batch_size):\n                end = start + self.mini_batch_size\n                mini_batch_indexes = indexes[start: end]\n                mini_batch = {}\n                for k, v in samples.items():\n                    mini_batch[k] = v[mini_batch_indexes]\n\n                res = self.trainer.train(session=self.session,\n                                         learning_rate=learning_rate,\n                                         clip_range=clip_range,\n                                         samples=mini_batch)\n\n                train_info.append(res)\n\n        return np.mean(train_info, axis=0)\n\n    def run_training_loop(self):\n        writer = self._create_summary_writer()\n        episode_info = deque(maxlen=100)\n        best_reward = 0\n\n        for update in range(self.updates):\n            time_start = time.time()\n            progress = update / self.updates\n\n            learning_rate = 2.5e-4 * (1 - progress)\n            clip_range = 0.1 * (1 - progress)\n\n            samples, sample_episode_info = self.sample()\n\n            train_info = self.train(samples, learning_rate, clip_range)\n\n            time_end = time.time()\n            fps = int(self.batch_size / (time_end - time_start))\n\n            episode_info.extend(sample_episode_info)\n\n            best_obs_frame = None\n            for info in sample_episode_info:\n                if info['reward'] > best_reward:\n                    best_reward = info['reward']\n                    best_obs_frame = info['obs']\n\n            reward_mean, length_mean = Main._get_mean_episode_info(episode_info)\n\n            self._write_summary(writer, best_obs_frame, update, fps,\n                                reward_mean, length_mean, train_info,\n                                clip_range, learning_rate)\n\n    @staticmethod\n    def _init_tf_session():\n        config = tf.ConfigProto(allow_soft_placement=True,\n                                log_device_placement=True)\n\n        config.gpu_options.allow_growth = True\n\n        tf.Session(config=config).__enter__()\n\n        np.random.seed(7)\n        tf.set_random_seed(7)\n\n    @staticmethod\n    def _get_mean_episode_info(episode_info):\n        if len(episode_info) > 0:\n            return (np.mean([info[\"reward\"] for info in episode_info]),\n                    np.mean([info[\"length\"] for info in episode_info]))\n        else:\n            return np.nan, np.nan\n\n    def _create_summary_writer(self) -> tf.summary.FileWriter:\n        log_dir = \"log/\" + Path(__file__).stem\n        if tf.gfile.Exists(log_dir):\n            tf.gfile.DeleteRecursively(log_dir)\n\n        return tf.summary.FileWriter(log_dir, self.session.graph)\n\n    def _write_summary(self, writer: tf.summary.Summary,\n                       best_obs_frame: Union[np.ndarray, None],\n                       update: int,\n                       fps: int,\n                       reward_mean: int,\n                       length_mean: int,\n                       train_info: np.ndarray,\n                       clip_range: float,\n                       learning_rate: float):\n        print(\"{:4} {:3} {:.2f} {:.3f}\".format(update, fps, reward_mean, length_mean))\n\n        summary = tf.Summary()\n\n        if best_obs_frame is not None:\n            sample_observation = best_obs_frame\n            observation_png = io.BytesIO()\n            pyplot.imsave(observation_png, sample_observation, format='png', cmap='gray')\n\n            observation_png = tf.Summary.Image(encoded_image_string=observation_png.getvalue(),\n                                               height=84,\n                                               width=84)\n            summary.value.add(tag=\"observation\", image=observation_png)\n\n        summary.value.add(tag=\"fps\", simple_value=fps)\n        for label, value in zip(self.trainer.train_info_labels, train_info):\n            summary.value.add(tag=label, simple_value=value)\n        summary.value.add(tag=\"reward_mean\", simple_value=reward_mean)\n        summary.value.add(tag=\"length_mean\", simple_value=length_mean)\n        summary.value.add(tag=\"clip_range\", simple_value=clip_range)\n        summary.value.add(tag=\"learning_rate\", simple_value=learning_rate)\n\n        writer.add_summary(summary, global_step=update)\n\n    def destroy(self):\n        for worker in self.workers:\n            worker.child.send((\"close\", None))\n\n\nif __name__ == \"__main__\":\n    m = Main()\n    m.run_training_loop()\n    m.destroy()\n\n";
});

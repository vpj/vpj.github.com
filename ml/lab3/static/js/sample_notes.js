define(["require", "exports"], function (require, exports) {
    "use strict";
    Object.defineProperty(exports, "__esModule", { value: true });
    exports.sampleNotes = [{ "pre": ["", "import os", "", "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"", ""], "post": ["", "", "class Game(object):", "    def __init__(self, seed: int):", "        self.env = gym.make('BreakoutNoFrameskip-v4')"], "code": ["class Orthogonal(object):", "    def __init__(self, scale=1.):", "        self.scale = scale", "", "    def __call__(self, shape, dtype=None, partition_info=None):", "        shape = tuple(shape)", "        if len(shape) == 2:", "            flat_shape = shape", "        elif len(shape) == 4:  # assumes NHWC", "            flat_shape = (np.prod(shape[:-1]), shape[-1])", "        else:", "            raise NotImplementedError", "        a = np.random.normal(0.0, 1.0, flat_shape)", "        u, _, v = np.linalg.svd(a, full_matrices=False)", "        q = u if u.shape == flat_shape else v  # pick the one with the correct shape", "        q = q.reshape(shape)", "        return (self.scale * q[:shape[0], :shape[1]]).astype(np.float32)", "", "    def get_config(self):", "        return {", "            'scale': self.scale", "        }"], "note": "## Orthogonal Initializer\nCoding this wasn't part of the plan.\nI previously used\n    [TensorFlow orthogonal initializer](https://www.tensorflow.org/api_docs/python/tf/orthogonal_initializer).\nBut it used a lot of GPU memory and sometimes crashed with a memory allocation error during initialization.\nI didn't test much to see what was happening;\n    instead, I just copied this code from OpenAI Baselines.\n" }, { "pre": ["            'scale': self.scale", "        }", "", "", "class Game(object):"], "post": ["", "    def step(self, action):", "", "        reward = 0.", "        done = None"], "code": ["    def __init__(self, seed: int):", "        self.env = gym.make('BreakoutNoFrameskip-v4')", "        self.env.seed(seed)", "", "        self.obs_2_max = np.zeros((2, 84, 84, 1), np.uint8)", "", "        self.obs_4 = np.zeros((84, 84, 4))", "", "        self.rewards = []", "", "        self.lives = 0"], "note": "### Initialize\n" }, { "pre": ["        }", "", "", "class Game(object):", "    def __init__(self, seed: int):"], "post": ["", "        self.obs_2_max = np.zeros((2, 84, 84, 1), np.uint8)", "", "        self.obs_4 = np.zeros((84, 84, 4))", ""], "code": ["        self.env = gym.make('BreakoutNoFrameskip-v4')", "        self.env.seed(seed)"], "note": "create environment" }, { "pre": ["class Game(object):", "    def __init__(self, seed: int):", "        self.env = gym.make('BreakoutNoFrameskip-v4')", "        self.env.seed(seed)", ""], "post": ["", "        self.obs_4 = np.zeros((84, 84, 4))", "", "        self.rewards = []", ""], "code": ["        self.obs_2_max = np.zeros((2, 84, 84, 1), np.uint8)"], "note": "buffer to take the maximum of last 2 frames for each action" }, { "pre": ["        self.env = gym.make('BreakoutNoFrameskip-v4')", "        self.env.seed(seed)", "", "        self.obs_2_max = np.zeros((2, 84, 84, 1), np.uint8)", ""], "post": ["", "        self.rewards = []", "", "        self.lives = 0", ""], "code": ["        self.obs_4 = np.zeros((84, 84, 4))"], "note": "tensor for a stack of 4 frames" }, { "pre": ["", "        self.obs_2_max = np.zeros((2, 84, 84, 1), np.uint8)", "", "        self.obs_4 = np.zeros((84, 84, 4))", ""], "post": ["", "        self.lives = 0", "", "    def step(self, action):", ""], "code": ["        self.rewards = []"], "note": "keep track of the episode rewards" }, { "pre": ["", "        self.obs_4 = np.zeros((84, 84, 4))", "", "        self.rewards = []", ""], "post": ["", "    def step(self, action):", "", "        reward = 0.", "        done = None"], "code": ["        self.lives = 0"], "note": "and number of lives left" }, { "pre": ["        return {", "            'scale': self.scale", "        }", "", ""], "post": ["", "", "def worker_process(remote: multiprocessing.connection.Connection, seed: int):", "    game = Game(seed)", ""], "code": ["class Game(object):", "    def __init__(self, seed: int):", "        self.env = gym.make('BreakoutNoFrameskip-v4')", "        self.env.seed(seed)", "", "        self.obs_2_max = np.zeros((2, 84, 84, 1), np.uint8)", "", "        self.obs_4 = np.zeros((84, 84, 4))", "", "        self.rewards = []", "", "        self.lives = 0", "", "    def step(self, action):", "", "        reward = 0.", "        done = None", "", "        for i in range(4):", "            obs, r, done, info = self.env.step(action)", "", "            if i >= 2:", "                self.obs_2_max[i % 2] = self._process_obs(obs)", "", "            reward += r", "", "            lives = self.env.unwrapped.ale.lives()", "            if lives < self.lives:", "                done = True", "            self.lives = lives", "", "            if done:", "                break", "", "        self.rewards.append(reward)", "", "        if done:", "            episode_info = {\"reward\": sum(self.rewards),", "                            \"length\": len(self.rewards)}", "            self.reset()", "        else:", "            episode_info = None", "            obs = self.obs_2_max.max(axis=0)", "", "            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=-1)", "            self.obs_4[..., -1:] = obs", "", "        return self.obs_4, reward, done, episode_info", "", "    def reset(self):", "        obs = self.env.reset()", "", "        obs = self._process_obs(obs)", "        self.obs_4[..., 0:] = obs", "        self.obs_4[..., 1:] = obs", "        self.obs_4[..., 2:] = obs", "        self.obs_4[..., 3:] = obs", "        self.rewards = []", "", "        self.lives = self.env.unwrapped.ale.lives()", "", "        return self.obs_4", "", "    @staticmethod", "    def _process_obs(obs):", "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)", "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)", "        return obs[:, :, None]  # Shape (84, 84, 1)"], "note": "## Game environment\nThis is a wrapper for OpenAI gym game environment.\nWe do a few things here:\n\n1. Apply the same action on four frames\n2. Convert observation frames to gray and scale it to (84, 84)\n3. Take the maximum of last two of those four frames\n4. Collect four such frames for last three actions\n5. Add episode information (total reward for the entire episode) for monitoring\n6. Restrict an episode to a single life (game has 5 lives, we reset after every single life)\n\n#### Observation format\nObservation is tensor of size (84, 84, 4). It is four frames\n(images of the game screen) stacked on last axis.\ni.e, each channel is a frame.\n\n    Frames    00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15\n    Actions   a1 a1 a1 a1 a2 a2 a2 a2 a3 a3 a3 a3 a4 a4 a4 a4\n    Max       -- -- MM MM -- -- MM MM -- -- MM MM -- -- MM MM\n    Stacked   -- -- Stack -- -- Stack -- -- Stack -- -- Stack\n" }, { "pre": ["", "        self.rewards = []", "", "        self.lives = 0", ""], "post": ["", "    @staticmethod", "    def _process_obs(obs):", "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)", "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)"], "code": ["    def step(self, action):", "", "        reward = 0.", "        done = None", "", "        for i in range(4):", "            obs, r, done, info = self.env.step(action)", "", "            if i >= 2:", "                self.obs_2_max[i % 2] = self._process_obs(obs)", "", "            reward += r", "", "            lives = self.env.unwrapped.ale.lives()", "            if lives < self.lives:", "                done = True", "            self.lives = lives", "", "            if done:", "                break", "", "        self.rewards.append(reward)", "", "        if done:", "            episode_info = {\"reward\": sum(self.rewards),", "                            \"length\": len(self.rewards)}", "            self.reset()", "        else:", "            episode_info = None", "            obs = self.obs_2_max.max(axis=0)", "", "            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=-1)", "            self.obs_4[..., -1:] = obs", "", "        return self.obs_4, reward, done, episode_info", "", "    def reset(self):", "        obs = self.env.reset()", "", "        obs = self._process_obs(obs)", "        self.obs_4[..., 0:] = obs", "        self.obs_4[..., 1:] = obs", "        self.obs_4[..., 2:] = obs", "        self.obs_4[..., 3:] = obs", "        self.rewards = []", "", "        self.lives = self.env.unwrapped.ale.lives()", "", "        return self.obs_4"], "note": "### Step\nExecutes `action` for 4 time steps and\n returns a tuple of (observation, reward, done, episode_info).\n\n* `observation`: stacked 4 frames (this frame and frames for last 3 actions) as described above\n* `reward`: total reward while the action was executed\n* `done`: whether the episode finished (a life lost)\n* `episode_info`: episode information if completed" }, { "pre": ["    def step(self, action):", "", "        reward = 0.", "        done = None", ""], "post": ["", "        self.rewards.append(reward)", "", "        if done:", "            episode_info = {\"reward\": sum(self.rewards),"], "code": ["        for i in range(4):", "            obs, r, done, info = self.env.step(action)", "", "            if i >= 2:", "                self.obs_2_max[i % 2] = self._process_obs(obs)", "", "            reward += r", "", "            lives = self.env.unwrapped.ale.lives()", "            if lives < self.lives:", "                done = True", "            self.lives = lives", "", "            if done:", "                break"], "note": "Repeat action for 4 steps." }, { "pre": ["", "        reward = 0.", "        done = None", "", "        for i in range(4):"], "post": ["", "            if i >= 2:", "                self.obs_2_max[i % 2] = self._process_obs(obs)", "", "            reward += r"], "code": ["            obs, r, done, info = self.env.step(action)"], "note": "execute the action in the OpenAI Gym environment" }, { "pre": ["        done = None", "", "        for i in range(4):", "            obs, r, done, info = self.env.step(action)", ""], "post": ["", "            reward += r", "", "            lives = self.env.unwrapped.ale.lives()", "            if lives < self.lives:"], "code": ["            if i >= 2:", "                self.obs_2_max[i % 2] = self._process_obs(obs)"], "note": "add last two frames to buffer" }, { "pre": ["            if i >= 2:", "                self.obs_2_max[i % 2] = self._process_obs(obs)", "", "            reward += r", ""], "post": ["            if lives < self.lives:", "                done = True", "            self.lives = lives", "", "            if done:"], "code": ["            lives = self.env.unwrapped.ale.lives()"], "note": "get number of lives left" }, { "pre": ["                self.obs_2_max[i % 2] = self._process_obs(obs)", "", "            reward += r", "", "            lives = self.env.unwrapped.ale.lives()"], "post": ["            self.lives = lives", "", "            if done:", "                break", ""], "code": ["            if lives < self.lives:", "                done = True"], "note": "reset if a life is lost" }, { "pre": ["            lives = self.env.unwrapped.ale.lives()", "            if lives < self.lives:", "                done = True", "            self.lives = lives", ""], "post": ["", "        self.rewards.append(reward)", "", "        if done:", "            episode_info = {\"reward\": sum(self.rewards),"], "code": ["            if done:", "                break"], "note": "stop if episode finished" }, { "pre": ["            if done:", "                break", "", "        self.rewards.append(reward)", ""], "post": ["        else:", "            episode_info = None", "            obs = self.obs_2_max.max(axis=0)", "", "            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=-1)"], "code": ["        if done:", "            episode_info = {\"reward\": sum(self.rewards),", "                            \"length\": len(self.rewards)}", "            self.reset()"], "note": "if finished, set episode information if episode is over, and reset" }, { "pre": ["            episode_info = {\"reward\": sum(self.rewards),", "                            \"length\": len(self.rewards)}", "            self.reset()", "        else:", "            episode_info = None"], "post": ["", "            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=-1)", "            self.obs_4[..., -1:] = obs", "", "        return self.obs_4, reward, done, episode_info"], "code": ["            obs = self.obs_2_max.max(axis=0)"], "note": "get the max of last two frames" }, { "pre": ["            self.reset()", "        else:", "            episode_info = None", "            obs = self.obs_2_max.max(axis=0)", ""], "post": ["", "        return self.obs_4, reward, done, episode_info", "", "    def reset(self):", "        obs = self.env.reset()"], "code": ["            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=-1)", "            self.obs_4[..., -1:] = obs"], "note": "push it to the stack of 4 frames" }, { "pre": ["            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=-1)", "            self.obs_4[..., -1:] = obs", "", "        return self.obs_4, reward, done, episode_info", ""], "post": ["", "    @staticmethod", "    def _process_obs(obs):", "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)", "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)"], "code": ["    def reset(self):", "        obs = self.env.reset()", "", "        obs = self._process_obs(obs)", "        self.obs_4[..., 0:] = obs", "        self.obs_4[..., 1:] = obs", "        self.obs_4[..., 2:] = obs", "        self.obs_4[..., 3:] = obs", "        self.rewards = []", "", "        self.lives = self.env.unwrapped.ale.lives()", "", "        return self.obs_4"], "note": "### Reset environment\nClean up episode info and 4 frame stack" }, { "pre": ["            self.obs_4[..., -1:] = obs", "", "        return self.obs_4, reward, done, episode_info", "", "    def reset(self):"], "post": ["", "        obs = self._process_obs(obs)", "        self.obs_4[..., 0:] = obs", "        self.obs_4[..., 1:] = obs", "        self.obs_4[..., 2:] = obs"], "code": ["        obs = self.env.reset()"], "note": "reset OpenAI Gym environment" }, { "pre": ["        return self.obs_4, reward, done, episode_info", "", "    def reset(self):", "        obs = self.env.reset()", ""], "post": ["", "        self.lives = self.env.unwrapped.ale.lives()", "", "        return self.obs_4", ""], "code": ["        obs = self._process_obs(obs)", "        self.obs_4[..., 0:] = obs", "        self.obs_4[..., 1:] = obs", "        self.obs_4[..., 2:] = obs", "        self.obs_4[..., 3:] = obs", "        self.rewards = []"], "note": "reset caches" }, { "pre": ["        self.lives = self.env.unwrapped.ale.lives()", "", "        return self.obs_4", "", "    @staticmethod"], "post": ["", "", "def worker_process(remote: multiprocessing.connection.Connection, seed: int):", "    game = Game(seed)", ""], "code": ["    def _process_obs(obs):", "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)", "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)", "        return obs[:, :, None]  # Shape (84, 84, 1)"], "note": "#### Process game frames\nConvert game frames to gray and rescale to 84x84\n" }, { "pre": ["        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)", "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)", "        return obs[:, :, None]  # Shape (84, 84, 1)", "", ""], "post": ["", "", "class Worker(object):", "    child: multiprocessing.connection.Connection", "    process: multiprocessing.Process"], "code": ["def worker_process(remote: multiprocessing.connection.Connection, seed: int):", "    game = Game(seed)", "", "    while True:", "        cmd, data = remote.recv()", "        if cmd == \"step\":", "            remote.send(game.step(data))", "        elif cmd == \"reset\":", "            remote.send(game.reset())", "        elif cmd == \"close\":", "            remote.close()", "            break", "        else:", "            raise NotImplementedError"], "note": "## Worker Process\n\nEach worker process runs this method\n" }, { "pre": ["        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)", "        return obs[:, :, None]  # Shape (84, 84, 1)", "", "", "def worker_process(remote: multiprocessing.connection.Connection, seed: int):"], "post": ["", "    while True:", "        cmd, data = remote.recv()", "        if cmd == \"step\":", "            remote.send(game.step(data))"], "code": ["    game = Game(seed)"], "note": "create game" }, { "pre": ["", "", "def worker_process(remote: multiprocessing.connection.Connection, seed: int):", "    game = Game(seed)", ""], "post": ["", "", "class Worker(object):", "    child: multiprocessing.connection.Connection", "    process: multiprocessing.Process"], "code": ["    while True:", "        cmd, data = remote.recv()", "        if cmd == \"step\":", "            remote.send(game.step(data))", "        elif cmd == \"reset\":", "            remote.send(game.reset())", "        elif cmd == \"close\":", "            remote.close()", "            break", "        else:", "            raise NotImplementedError"], "note": "wait for instructions from the connection and execute them" }, { "pre": ["            break", "        else:", "            raise NotImplementedError", "", ""], "post": ["", "", "class Model(object):", "    def __init__(self, *, reuse: bool, batch_size: int):", "        self.obs = tf.placeholder(shape=(batch_size, 84, 84, 4), name=\"obs\", dtype=np.uint8)"], "code": ["class Worker(object):", "    child: multiprocessing.connection.Connection", "    process: multiprocessing.Process", "", "    def __init__(self, seed):", "        self.child, parent = multiprocessing.Pipe()", "        self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))", "        self.process.start()"], "note": "## Worker\nCreates a new worker and runs it in a separate process.\n" }, { "pre": ["        self.child, parent = multiprocessing.Pipe()", "        self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))", "        self.process.start()", "", ""], "post": ["", "", "class Trainer(object):", "    def __init__(self, model: Model):", "        self.model = model"], "code": ["class Model(object):", "    def __init__(self, *, reuse: bool, batch_size: int):", "        self.obs = tf.placeholder(shape=(batch_size, 84, 84, 4), name=\"obs\", dtype=np.uint8)", "        obs_float = tf.to_float(self.obs, name=\"obs_float\")", "", "        with tf.variable_scope(\"model\", reuse=reuse):", "            self.h = Model._cnn(obs_float)", "            self.pi_logits = Model._create_policy_network(self.h, 4)", "", "            self.value = Model._create_value_network(self.h)", "", "            self.params = tf.trainable_variables()", "", "        self.action = Model._sample(self.pi_logits)", "", "        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")", "", "        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)", "", "    @staticmethod", "    def _get_policy_entropy(logits: tf.Tensor):", "        a = logits - tf.reduce_max(logits, axis=-1, keepdims=True)", "        exp_a = tf.exp(a)", "        z = tf.reduce_sum(exp_a, axis=-1, keepdims=True)", "        p = exp_a / z", "", "        return tf.reduce_sum(p * (tf.log(z) - a), axis=-1)", "", "    def neg_log_prob(self, action: tf.Tensor, name: str) -> tf.Tensor:", "        one_hot_actions = tf.one_hot(action, 4)", "        return tf.nn.softmax_cross_entropy_with_logits_v2(", "            logits=self.pi_logits,", "            labels=one_hot_actions,", "            dim=-1,", "            name=name)", "", "    @staticmethod", "    def _sample(logits: tf.Tensor):", "        uniform = tf.random_uniform(tf.shape(logits))", "        return tf.argmax(logits - tf.log(-tf.log(uniform)),", "                         axis=-1,", "                         name=\"action\")", "", "    @staticmethod", "    def _cnn(unscaled_images: tf.Tensor):", "        scaled_images = tf.cast(unscaled_images, tf.float32) / 255.", "", "        h1 = tf.layers.conv2d(scaled_images,", "                              name=\"conv1\",", "                              filters=32,", "                              kernel_size=8,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=4,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        h2 = tf.layers.conv2d(h1,", "                              name=\"conv2\",", "                              filters=64,", "                              kernel_size=4,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=2,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        h3 = tf.layers.conv2d(h2,", "                              name=\"conv3\",", "                              filters=64,", "                              kernel_size=3,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=1,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        nh = np.prod([v.value for v in h3.get_shape()[1:]])", "        flat = tf.reshape(h3, [-1, nh])", "", "        h = tf.layers.dense(flat, 512,", "                            activation=tf.nn.relu,", "                            kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                            name=\"hidden\")", "", "        return h", "", "    @staticmethod", "    def _create_policy_network(h: tf.Tensor, n: int) -> tf.Tensor:", "        return tf.layers.dense(h, n,", "                               activation=None,", "                               kernel_initializer=Orthogonal(scale=0.01),", "                               name=\"logits\")", "", "    @staticmethod", "    def _create_value_network(h: tf.Tensor) -> tf.Tensor:", "        value = tf.layers.dense(h, 1,", "                                activation=None,", "                                kernel_initializer=Orthogonal(),", "                                name=\"value\")", "        return value[:, 0]", "", "    def step(self, session: tf.Session, obs: np.ndarray) -> (tf.Tensor, tf.Tensor, tf.Tensor):", "        return session.run([self.action, self.value, self.neg_log_pi],", "                           feed_dict={self.obs: obs})", "", "    def get_value(self, session: tf.Session, obs: np.ndarray) -> tf.Tensor:", "        return session.run(self.value,", "                           feed_dict={self.obs: obs})"], "note": "## Neural Network model for policy and value function\nI initially implemented a simpler network with 2 fully connected layers,\nbut later decided to implement a convolution architecture similar to\nthe OpenAI baselines implementation.\n\nThe policy network and value function share the first 4 layers.\n" }, { "pre": ["        self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))", "        self.process.start()", "", "", "class Model(object):"], "post": ["", "    @staticmethod", "    def _get_policy_entropy(logits: tf.Tensor):", "        a = logits - tf.reduce_max(logits, axis=-1, keepdims=True)", "        exp_a = tf.exp(a)"], "code": ["    def __init__(self, *, reuse: bool, batch_size: int):", "        self.obs = tf.placeholder(shape=(batch_size, 84, 84, 4), name=\"obs\", dtype=np.uint8)", "        obs_float = tf.to_float(self.obs, name=\"obs_float\")", "", "        with tf.variable_scope(\"model\", reuse=reuse):", "            self.h = Model._cnn(obs_float)", "            self.pi_logits = Model._create_policy_network(self.h, 4)", "", "            self.value = Model._create_value_network(self.h)", "", "            self.params = tf.trainable_variables()", "", "        self.action = Model._sample(self.pi_logits)", "", "        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")", "", "        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)"], "note": "#### Initialize" }, { "pre": ["        self.process.start()", "", "", "class Model(object):", "    def __init__(self, *, reuse: bool, batch_size: int):"], "post": ["        obs_float = tf.to_float(self.obs, name=\"obs_float\")", "", "        with tf.variable_scope(\"model\", reuse=reuse):", "            self.h = Model._cnn(obs_float)", "            self.pi_logits = Model._create_policy_network(self.h, 4)"], "code": ["        self.obs = tf.placeholder(shape=(batch_size, 84, 84, 4), name=\"obs\", dtype=np.uint8)"], "note": "observations input `(B, 84, 84, 4)`" }, { "pre": ["    def __init__(self, *, reuse: bool, batch_size: int):", "        self.obs = tf.placeholder(shape=(batch_size, 84, 84, 4), name=\"obs\", dtype=np.uint8)", "        obs_float = tf.to_float(self.obs, name=\"obs_float\")", "", "        with tf.variable_scope(\"model\", reuse=reuse):"], "post": ["            self.pi_logits = Model._create_policy_network(self.h, 4)", "", "            self.value = Model._create_value_network(self.h)", "", "            self.params = tf.trainable_variables()"], "code": ["            self.h = Model._cnn(obs_float)"], "note": "output of the convolution network `(B, 512)`" }, { "pre": ["        self.obs = tf.placeholder(shape=(batch_size, 84, 84, 4), name=\"obs\", dtype=np.uint8)", "        obs_float = tf.to_float(self.obs, name=\"obs_float\")", "", "        with tf.variable_scope(\"model\", reuse=reuse):", "            self.h = Model._cnn(obs_float)"], "post": ["", "            self.value = Model._create_value_network(self.h)", "", "            self.params = tf.trainable_variables()", ""], "code": ["            self.pi_logits = Model._create_policy_network(self.h, 4)"], "note": "logits for policy network $\\pi_\\theta$, from which the action is sampled `(B, 4)`" }, { "pre": ["", "        with tf.variable_scope(\"model\", reuse=reuse):", "            self.h = Model._cnn(obs_float)", "            self.pi_logits = Model._create_policy_network(self.h, 4)", ""], "post": ["", "            self.params = tf.trainable_variables()", "", "        self.action = Model._sample(self.pi_logits)", ""], "code": ["            self.value = Model._create_value_network(self.h)"], "note": "value function $V(s_t)$" }, { "pre": ["            self.h = Model._cnn(obs_float)", "            self.pi_logits = Model._create_policy_network(self.h, 4)", "", "            self.value = Model._create_value_network(self.h)", ""], "post": ["", "        self.action = Model._sample(self.pi_logits)", "", "        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")", ""], "code": ["            self.params = tf.trainable_variables()"], "note": "all trainable variables" }, { "pre": ["", "            self.value = Model._create_value_network(self.h)", "", "            self.params = tf.trainable_variables()", ""], "post": ["", "        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")", "", "        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)", ""], "code": ["        self.action = Model._sample(self.pi_logits)"], "note": "sampled action $a_t$" }, { "pre": ["", "            self.params = tf.trainable_variables()", "", "        self.action = Model._sample(self.pi_logits)", ""], "post": ["", "        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)", "", "    @staticmethod", "    def _get_policy_entropy(logits: tf.Tensor):"], "code": ["        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")"], "note": "$-\\log(\\pi(a_t|s_t)$" }, { "pre": ["", "        self.action = Model._sample(self.pi_logits)", "", "        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")", ""], "post": ["", "    @staticmethod", "    def _get_policy_entropy(logits: tf.Tensor):", "        a = logits - tf.reduce_max(logits, axis=-1, keepdims=True)", "        exp_a = tf.exp(a)"], "code": ["        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)"], "note": "$ S\\bigl[\\pi_\\theta \\bigr] (s_t)$" }, { "pre": ["        self.neg_log_pi = self.neg_log_prob(self.action, \"neg_log_pi_old\")", "", "        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)", "", "    @staticmethod"], "post": ["", "    def neg_log_prob(self, action: tf.Tensor, name: str) -> tf.Tensor:", "        one_hot_actions = tf.one_hot(action, 4)", "        return tf.nn.softmax_cross_entropy_with_logits_v2(", "            logits=self.pi_logits,"], "code": ["    def _get_policy_entropy(logits: tf.Tensor):", "        a = logits - tf.reduce_max(logits, axis=-1, keepdims=True)", "        exp_a = tf.exp(a)", "        z = tf.reduce_sum(exp_a, axis=-1, keepdims=True)", "        p = exp_a / z", "", "        return tf.reduce_sum(p * (tf.log(z) - a), axis=-1)"], "note": "#### Policy Entropy\n$ S\\bigl[\\pi_\\theta\\\\bigr] (s) = \\sum_a \\pi_\\theta(a|s) \\log \\pi_\\theta(a|s)$\n" }, { "pre": ["", "        self.policy_entropy = Model._get_policy_entropy(self.pi_logits)", "", "    @staticmethod", "    def _get_policy_entropy(logits: tf.Tensor):"], "post": ["", "        return tf.reduce_sum(p * (tf.log(z) - a), axis=-1)", "", "    def neg_log_prob(self, action: tf.Tensor, name: str) -> tf.Tensor:", "        one_hot_actions = tf.one_hot(action, 4)"], "code": ["        a = logits - tf.reduce_max(logits, axis=-1, keepdims=True)", "        exp_a = tf.exp(a)", "        z = tf.reduce_sum(exp_a, axis=-1, keepdims=True)", "        p = exp_a / z"], "note": "we subtract the logit of the best action\n to make the floating point calculation stable\n (not give infinity and nan).\nWe can do this because\n $\\frac{e^{x_i - c}}{\\sum_j e^{x_j - c}} = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n for any constant $c$.\n" }, { "pre": ["        a = logits - tf.reduce_max(logits, axis=-1, keepdims=True)", "        exp_a = tf.exp(a)", "        z = tf.reduce_sum(exp_a, axis=-1, keepdims=True)", "        p = exp_a / z", ""], "post": ["", "    def neg_log_prob(self, action: tf.Tensor, name: str) -> tf.Tensor:", "        one_hot_actions = tf.one_hot(action, 4)", "        return tf.nn.softmax_cross_entropy_with_logits_v2(", "            logits=self.pi_logits,"], "code": ["        return tf.reduce_sum(p * (tf.log(z) - a), axis=-1)"], "note": "$S = -\\sum_i p  \\log(p) =\n  \\sum_i p  \\bigl(-\\log(p) \\bigr) =\n  \\sum_i p  \\bigl(\\log(z) - logits \\bigr)$" }, { "pre": ["        z = tf.reduce_sum(exp_a, axis=-1, keepdims=True)", "        p = exp_a / z", "", "        return tf.reduce_sum(p * (tf.log(z) - a), axis=-1)", ""], "post": ["", "    @staticmethod", "    def _sample(logits: tf.Tensor):", "        uniform = tf.random_uniform(tf.shape(logits))", "        return tf.argmax(logits - tf.log(-tf.log(uniform)),"], "code": ["    def neg_log_prob(self, action: tf.Tensor, name: str) -> tf.Tensor:", "        one_hot_actions = tf.one_hot(action, 4)", "        return tf.nn.softmax_cross_entropy_with_logits_v2(", "            logits=self.pi_logits,", "            labels=one_hot_actions,", "            dim=-1,", "            name=name)"], "note": "#### Negative log of probability of the action\n$-\\log\\pi(a_t|s_t) = \\sum_a p(a)$\n\nThis is equal to cross entropy with\n a probability distribution across actions, with a probability of 1 at `action`.\n$-\\sum_{a'} p(a') \\log \\pi(a'|s_t) = -\\log \\pi(a_t|s_t)$ since $p(a_t) = 1$." }, { "pre": ["            labels=one_hot_actions,", "            dim=-1,", "            name=name)", "", "    @staticmethod"], "post": ["", "    @staticmethod", "    def _cnn(unscaled_images: tf.Tensor):", "        scaled_images = tf.cast(unscaled_images, tf.float32) / 255.", ""], "code": ["    def _sample(logits: tf.Tensor):", "        uniform = tf.random_uniform(tf.shape(logits))", "        return tf.argmax(logits - tf.log(-tf.log(uniform)),", "                         axis=-1,", "                         name=\"action\")"], "note": "#### Sample from $\\pi_\\theta$\nSample using\n [Gumbel-Max Trick](\n https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/).\n" }, { "pre": ["        return tf.argmax(logits - tf.log(-tf.log(uniform)),", "                         axis=-1,", "                         name=\"action\")", "", "    @staticmethod"], "post": ["", "    @staticmethod", "    def _create_policy_network(h: tf.Tensor, n: int) -> tf.Tensor:", "        return tf.layers.dense(h, n,", "                               activation=None,"], "code": ["    def _cnn(unscaled_images: tf.Tensor):", "        scaled_images = tf.cast(unscaled_images, tf.float32) / 255.", "", "        h1 = tf.layers.conv2d(scaled_images,", "                              name=\"conv1\",", "                              filters=32,", "                              kernel_size=8,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=4,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        h2 = tf.layers.conv2d(h1,", "                              name=\"conv2\",", "                              filters=64,", "                              kernel_size=4,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=2,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        h3 = tf.layers.conv2d(h2,", "                              name=\"conv3\",", "                              filters=64,", "                              kernel_size=3,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=1,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        nh = np.prod([v.value for v in h3.get_shape()[1:]])", "        flat = tf.reshape(h3, [-1, nh])", "", "        h = tf.layers.dense(flat, 512,", "                            activation=tf.nn.relu,", "                            kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                            name=\"hidden\")", "", "        return h"], "note": "#### Convolutional Neural Network" }, { "pre": ["                         axis=-1,", "                         name=\"action\")", "", "    @staticmethod", "    def _cnn(unscaled_images: tf.Tensor):"], "post": ["", "        h1 = tf.layers.conv2d(scaled_images,", "                              name=\"conv1\",", "                              filters=32,", "                              kernel_size=8,"], "code": ["        scaled_images = tf.cast(unscaled_images, tf.float32) / 255."], "note": "scale image values to [0, 1] from [0, 255]" }, { "pre": ["", "    @staticmethod", "    def _cnn(unscaled_images: tf.Tensor):", "        scaled_images = tf.cast(unscaled_images, tf.float32) / 255.", ""], "post": ["", "        nh = np.prod([v.value for v in h3.get_shape()[1:]])", "        flat = tf.reshape(h3, [-1, nh])", "", "        h = tf.layers.dense(flat, 512,"], "code": ["        h1 = tf.layers.conv2d(scaled_images,", "                              name=\"conv1\",", "                              filters=32,", "                              kernel_size=8,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=4,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        h2 = tf.layers.conv2d(h1,", "                              name=\"conv2\",", "                              filters=64,", "                              kernel_size=4,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=2,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", "", "        h3 = tf.layers.conv2d(h2,", "                              name=\"conv3\",", "                              filters=64,", "                              kernel_size=3,", "                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=1,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)"], "note": "three convolution layers" }, { "pre": ["                              kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                              strides=1,", "                              padding=\"valid\",", "                              activation=tf.nn.relu)", ""], "post": ["", "        h = tf.layers.dense(flat, 512,", "                            activation=tf.nn.relu,", "                            kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                            name=\"hidden\")"], "code": ["        nh = np.prod([v.value for v in h3.get_shape()[1:]])", "        flat = tf.reshape(h3, [-1, nh])"], "note": "reshape to a 2-D tensor" }, { "pre": ["                              activation=tf.nn.relu)", "", "        nh = np.prod([v.value for v in h3.get_shape()[1:]])", "        flat = tf.reshape(h3, [-1, nh])", ""], "post": ["", "        return h", "", "    @staticmethod", "    def _create_policy_network(h: tf.Tensor, n: int) -> tf.Tensor:"], "code": ["        h = tf.layers.dense(flat, 512,", "                            activation=tf.nn.relu,", "                            kernel_initializer=Orthogonal(scale=np.sqrt(2)),", "                            name=\"hidden\")"], "note": "fully connected layer" }, { "pre": ["                            name=\"hidden\")", "", "        return h", "", "    @staticmethod"], "post": ["", "    @staticmethod", "    def _create_value_network(h: tf.Tensor) -> tf.Tensor:", "        value = tf.layers.dense(h, 1,", "                                activation=None,"], "code": ["    def _create_policy_network(h: tf.Tensor, n: int) -> tf.Tensor:", "        return tf.layers.dense(h, n,", "                               activation=None,", "                               kernel_initializer=Orthogonal(scale=0.01),", "                               name=\"logits\")"], "note": "#### Head for policy" }, { "pre": ["                               activation=None,", "                               kernel_initializer=Orthogonal(scale=0.01),", "                               name=\"logits\")", "", "    @staticmethod"], "post": ["", "    def step(self, session: tf.Session, obs: np.ndarray) -> (tf.Tensor, tf.Tensor, tf.Tensor):", "        return session.run([self.action, self.value, self.neg_log_pi],", "                           feed_dict={self.obs: obs})", ""], "code": ["    def _create_value_network(h: tf.Tensor) -> tf.Tensor:", "        value = tf.layers.dense(h, 1,", "                                activation=None,", "                                kernel_initializer=Orthogonal(),", "                                name=\"value\")", "        return value[:, 0]"], "note": "#### Head for value function" }, { "pre": ["                                activation=None,", "                                kernel_initializer=Orthogonal(),", "                                name=\"value\")", "        return value[:, 0]", ""], "post": ["", "    def get_value(self, session: tf.Session, obs: np.ndarray) -> tf.Tensor:", "        return session.run(self.value,", "                           feed_dict={self.obs: obs})", ""], "code": ["    def step(self, session: tf.Session, obs: np.ndarray) -> (tf.Tensor, tf.Tensor, tf.Tensor):", "        return session.run([self.action, self.value, self.neg_log_pi],", "                           feed_dict={self.obs: obs})"], "note": "#### Sample actions for given observations" }, { "pre": ["", "    def step(self, session: tf.Session, obs: np.ndarray) -> (tf.Tensor, tf.Tensor, tf.Tensor):", "        return session.run([self.action, self.value, self.neg_log_pi],", "                           feed_dict={self.obs: obs})", ""], "post": ["", "", "class Trainer(object):", "    def __init__(self, model: Model):", "        self.model = model"], "code": ["    def get_value(self, session: tf.Session, obs: np.ndarray) -> tf.Tensor:", "        return session.run(self.value,", "                           feed_dict={self.obs: obs})"], "note": "#### Get value function for a given observation" }, { "pre": ["    def get_value(self, session: tf.Session, obs: np.ndarray) -> tf.Tensor:", "        return session.run(self.value,", "                           feed_dict={self.obs: obs})", "", ""], "post": ["", "", "class Main(object):", "    def __init__(self):", "        self.gamma = 0.99"], "code": ["class Trainer(object):", "    def __init__(self, model: Model):", "        self.model = model", "", "        self.sampled_obs = self.model.obs", "", "        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")", "        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")", "", "        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],", "                                                           name=\"sampled_normalized_advantage\")", "", "        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")", "        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")", "", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")", "", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")", "", "        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")", "", "        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")", "", "        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")", "        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,", "                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")", "", "        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")", "", "        value = self.model.value", "", "        clipped_value = tf.add(self.sampled_value,", "                               tf.clip_by_value(value - self.sampled_value, -self.clip_range, self.clip_range),", "                               name=\"clipped_value\")", "        self.vf_loss = tf.multiply(0.5,", "                                   tf.reduce_mean(tf.maximum(tf.square(value - self.sampled_return),", "                                                             tf.square(clipped_value - self.sampled_return))),", "                                   name=\"vf_loss\")", "", "        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)", "", "        params = self.model.params", "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)", "", "        adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-5)", "        grads_and_vars = list(zip(grads, params))", "        self.train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")", "", "        self.approx_kl_divergence = .5 * tf.reduce_mean(tf.square(neg_log_pi - self.sampled_neg_log_pi))", "        self.clip_fraction = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range)))", "", "        self.train_info_labels = ['policy_reward',", "                                  'value_loss',", "                                  'entropy_bonus',", "                                  'approx_kl_divergence',", "                                  'clip_fraction']", "", "    def train(self, session: tf.Session, samples: Dict[str, np.ndarray], learning_rate: float, clip_range: float):", "        feed_dict = {self.sampled_obs: samples['obs'],", "                     self.sampled_action: samples['actions'],", "                     self.sampled_return: samples['values'] + samples['advantages'],", "                     self.sampled_normalized_advantage: Trainer._normalize(samples['advantages']),", "                     self.sampled_value: samples['values'],", "                     self.sampled_neg_log_pi: samples['neg_log_pis'],", "                     self.learning_rate: learning_rate,", "                     self.clip_range: clip_range}", "", "        evals = [self.policy_reward,", "                 self.vf_loss,", "                 self.entropy_bonus,", "                 self.approx_kl_divergence,", "                 self.clip_fraction,", "                 self.train_op]", "", "        return session.run(evals, feed_dict=feed_dict)[:-1]", "", "    @staticmethod", "    def _normalize(adv: np.ndarray):", "        return (adv - adv.mean()) / (adv.std() + 1e-8)"], "note": "## Trainer\n\nWe want to maximize policy reward\n $$\\max_\\theta J(\\pi_\\theta) =\n   \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_\\theta}\\Biggl[\\sum_{t=0}^\\infty \\gamma^t r_t \\Biggr]$$\n where $r$ is the reward, $\\pi$ is the policy, $\\tau$ is a trajectory sampled from policy,\n and $\\gamma$ is the discount factor between $[0, 1]$.\n\n\\begin{align}\n\\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n \\sum_{t=0}^\\infty \\gamma^t A^{\\pi_{OLD}}(s_t, a_t)\n\\Biggr] &=\n\\\\\n\\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n  \\sum_{t=0}^\\infty \\gamma^t \\Bigl(\n   Q^{\\pi_{OLD}}(s_t, a_t) - V^{\\pi_{OLD}}(s_t)\n  \\Bigr)\n \\Biggr] &=\n\\\\\n\\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n  \\sum_{t=0}^\\infty \\gamma^t \\Bigl(\n   Q^{\\pi_{OLD}}(s_t, a_t) - V^{\\pi_{OLD}}(s_t)\n  \\Bigr)\n \\Biggr] &=\n\\\\\n\\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n  \\sum_{t=0}^\\infty \\gamma^t \\Bigl(\n   r_t + V^{\\pi_{OLD}}(s_{t+1}) - V^{\\pi_{OLD}}(s_t)\n  \\Bigr)\n \\Biggr] &=\n\\\\\n\\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n  \\sum_{t=0}^\\infty \\gamma^t \\Bigl(\n   r_t\n  \\Bigr)\n \\Biggr] - \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n    \\Biggl[V^{\\pi_{OLD}}(s_0)\\Biggr] &=\nJ(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n\\end{align}\n\nSo,\n $$\\max_\\theta J(\\pi_\\theta) =\n   \\max_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n      \\sum_{t=0}^\\infty \\gamma^t A^{\\pi_{OLD}}(s_t, a_t)\n   \\Biggr]$$\n\nDefine discounted-future state distribution,\n $$d^\\pi(s) = (1 - \\gamma) \\sum_{t=0}^\\infty \\gamma^t P(s_t = s | \\pi)$$\n\nThen,\n\\begin{align}\nJ(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n&= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n \\sum_{t=0}^\\infty \\gamma^t A^{\\pi_{OLD}}(s_t, a_t)\n\\Biggr]\n\\\\\n&= \\frac{1}{1 - \\gamma}\n \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_\\theta} \\Bigl[\n  A^{\\pi_{OLD}}(s, a)\n \\Bigr]\n\\end{align}\n\nImportance sampling $a$ from $\\pi_{\\theta_{OLD}}$,\n\n\\begin{align}\nJ(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n&= \\frac{1}{1 - \\gamma}\n \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_\\theta} \\Bigl[\n  A^{\\pi_{OLD}}(s, a)\n \\Bigr]\n\\\\\n&= \\frac{1}{1 - \\gamma}\n \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_{\\theta_{OLD}}} \\Biggl[\n  \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{OLD}}(a|s)} A^{\\pi_{OLD}}(s, a)\n \\Biggr]\n\\end{align}\n\nThen we assume $d^\\pi_\\theta(s)$ and  $d^\\pi_{\\theta_{OLD}}(s)$ are similar.\nThe error we introduce to $J(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})$\n by this assumtion is bound by the KL divergence between\n $\\pi_\\theta$ and $\\pi_{\\theta_{OLD}}$.\n[Constrained Policy Optimization](https://arxiv.org/abs/1705.10528)\n shows the proof of this. I haven't read it.\n\n\n\\begin{align}\nJ(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n&= \\frac{1}{1 - \\gamma}\n \\mathop{\\mathbb{E}}_{s \\sim d^{\\pi_\\theta} \\atop a \\sim \\pi_{\\theta_{OLD}}} \\Biggl[\n  \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{OLD}}(a|s)} A^{\\pi_{OLD}}(s, a)\n \\Biggr]\n\\\\\n&\\approx \\frac{1}{1 - \\gamma}\n \\mathop{\\mathbb{E}}_{\\color{orange}{s \\sim d^{\\pi_{\\theta_{OLD}}}} \\atop a \\sim \\pi_{\\theta_{OLD}}} \\Biggl[\n  \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{OLD}}(a|s)} A^{\\pi_{OLD}}(s, a)\n \\Biggr]\n\\\\\n&= \\frac{1}{1 - \\gamma} \\mathcal{L}^{CPI}\n\\end{align}" }, { "pre": ["        return session.run(self.value,", "                           feed_dict={self.obs: obs})", "", "", "class Trainer(object):"], "post": ["", "    def train(self, session: tf.Session, samples: Dict[str, np.ndarray], learning_rate: float, clip_range: float):", "        feed_dict = {self.sampled_obs: samples['obs'],", "                     self.sampled_action: samples['actions'],", "                     self.sampled_return: samples['values'] + samples['advantages'],"], "code": ["    def __init__(self, model: Model):", "        self.model = model", "", "        self.sampled_obs = self.model.obs", "", "        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")", "        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")", "", "        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],", "                                                           name=\"sampled_normalized_advantage\")", "", "        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")", "        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")", "", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")", "", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")", "", "        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")", "", "        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")", "", "        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")", "        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,", "                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")", "", "        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")", "", "        value = self.model.value", "", "        clipped_value = tf.add(self.sampled_value,", "                               tf.clip_by_value(value - self.sampled_value, -self.clip_range, self.clip_range),", "                               name=\"clipped_value\")", "        self.vf_loss = tf.multiply(0.5,", "                                   tf.reduce_mean(tf.maximum(tf.square(value - self.sampled_return),", "                                                             tf.square(clipped_value - self.sampled_return))),", "                                   name=\"vf_loss\")", "", "        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)", "", "        params = self.model.params", "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)", "", "        adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-5)", "        grads_and_vars = list(zip(grads, params))", "        self.train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")", "", "        self.approx_kl_divergence = .5 * tf.reduce_mean(tf.square(neg_log_pi - self.sampled_neg_log_pi))", "        self.clip_fraction = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range)))", "", "        self.train_info_labels = ['policy_reward',", "                                  'value_loss',", "                                  'entropy_bonus',", "                                  'approx_kl_divergence',", "                                  'clip_fraction']"], "note": "### Initialization" }, { "pre": ["                           feed_dict={self.obs: obs})", "", "", "class Trainer(object):", "    def __init__(self, model: Model):"], "post": ["", "        self.sampled_obs = self.model.obs", "", "        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")", "        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")"], "code": ["        self.model = model"], "note": "model for training, $\\pi_\\theta$ and $V_\\theta$.\nThis model shares parameters with the sampling model so,\n updating variables affect both." }, { "pre": ["", "class Trainer(object):", "    def __init__(self, model: Model):", "        self.model = model", ""], "post": ["", "        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")", "        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")", "", "        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],"], "code": ["        self.sampled_obs = self.model.obs"], "note": "sampled observations are fed into the model to get $\\pi_\\theta(a_t|s_t)$; we are treating observations as state" }, { "pre": ["    def __init__(self, model: Model):", "        self.model = model", "", "        self.sampled_obs = self.model.obs", ""], "post": ["        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")", "", "        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],", "                                                           name=\"sampled_normalized_advantage\")", ""], "code": ["        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")"], "note": "$a_t$ actions sampled from $\\pi_{\\theta_{OLD}}$" }, { "pre": ["        self.model = model", "", "        self.sampled_obs = self.model.obs", "", "        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")"], "post": ["", "        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],", "                                                           name=\"sampled_normalized_advantage\")", "", "        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")"], "code": ["        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")"], "note": "$R_t$ returns sampled from $\\pi_{\\theta_{OLD}}$" }, { "pre": ["        self.sampled_obs = self.model.obs", "", "        self.sampled_action = tf.placeholder(dtype=tf.int32, shape=[None], name=\"sampled_action\")", "        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")", ""], "post": ["", "        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")", "        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")", "", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")"], "code": ["        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],", "                                                           name=\"sampled_normalized_advantage\")"], "note": "$\\bar{A_t} = \\frac{\\hat{A_t} - \\mu(\\hat{A_t})}{\\sigma(\\hat{A_t})}$, where $\\hat{A_t}$ is advantages sampled from $\\pi_{\\theta_{OLD}}$.\nRefer to sampling function in [Main class](#main) below for the calculation of $\\hat{A}_t$." }, { "pre": ["        self.sampled_return = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_return\")", "", "        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],", "                                                           name=\"sampled_normalized_advantage\")", ""], "post": ["        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")", "", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")", "", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")"], "code": ["        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")"], "note": "$-\\log \\pi_{\\theta_{OLD}} (a_t|s_t)$ log probabilities" }, { "pre": ["", "        self.sampled_normalized_advantage = tf.placeholder(dtype=tf.float32, shape=[None],", "                                                           name=\"sampled_normalized_advantage\")", "", "        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")"], "post": ["", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")", "", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")", ""], "code": ["        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")"], "note": "$\\hat{V_t}$ value estimates" }, { "pre": ["                                                           name=\"sampled_normalized_advantage\")", "", "        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")", "        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")", ""], "post": ["", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")", "", "        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")", ""], "code": ["        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")"], "note": "learning rate" }, { "pre": ["        self.sampled_neg_log_pi = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_neg_log_pi\")", "        self.sampled_value = tf.placeholder(dtype=tf.float32, shape=[None], name=\"sampled_value\")", "", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")", ""], "post": ["", "        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")", "", "        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")", ""], "code": ["        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")"], "note": "$\\epsilon$ for clipping" }, { "pre": ["", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")", "", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")", ""], "post": ["", "        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")", "", "        value = self.model.value", ""], "code": ["        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")", "", "        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")", "", "        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")", "        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,", "                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")"], "note": "#### Policy" }, { "pre": ["", "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")", "", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")", ""], "post": ["", "        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")", "", "        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")", "        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,"], "code": ["        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")"], "note": "$-\\log \\pi_\\theta (a_t|s_t)$" }, { "pre": ["", "        self.clip_range = tf.placeholder(dtype=tf.float32, shape=[], name=\"clip_range\")", "", "        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")", ""], "post": ["", "        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")", "        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,", "                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")"], "code": ["        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")"], "note": " ratio $r_t(\\theta) = \\frac{\\pi_\\theta (a_t|s_t)}{\\pi_{\\theta_{OLD}} (a_t|s_t)}$;\n*this is different from rewards* $r_t$." }, { "pre": ["", "        neg_log_pi = self.model.neg_log_prob(self.sampled_action, \"neg_log_pi\")", "", "        ratio = tf.exp(self.sampled_neg_log_pi - neg_log_pi, name=\"ratio\")", ""], "post": ["", "        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")", "", "        value = self.model.value", ""], "code": ["        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")", "        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,", "                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")"], "note": "\\begin{align}\n\\mathcal{L}^{CLIP}(\\theta) =\n \\mathbb{E}_{a_t, s_t \\sim \\pi_{\\theta{OLD}}} \\biggl[\n   min \\Bigl(r_t(\\theta) \\bar{A_t},\n             clip \\bigl(\n              r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon\n             \\bigr) \\bar{A_t}\n   \\Bigr)\n \\biggr]\n\\end{align}\n\nThe ratio is clipped to be close to 1.\nWe take the minimum so that the gradient will only pull\n$\\pi_\\theta$ towards $\\pi_{\\theta_{OLD}}$ if the ratio is\nnot between $1 - \\epsilon$ and $1 + \\epsilon$.\nThis keeps the KL divergence between $\\pi_\\theta$\n and $\\pi_{\\theta_{OLD}}$ constrained.\nLarge deviation can cause performance collapse;\n where the policy performance drops and doesn't recover because\n we are sampling from a bad policy.\n\nUsing the normalized advantage\n $\\bar{A_t} = \\frac{\\hat{A_t} - \\mu(\\hat{A_t})}{\\sigma(\\hat{A_t})}$\n introduces a bias to the policy gradient estimator,\n but it reduces variance a lot." }, { "pre": ["        clipped_ratio = tf.clip_by_value(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range, name=\"clipped_ratio\")", "        self.policy_reward = tf.reduce_mean(tf.minimum(ratio * self.sampled_normalized_advantage,", "                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")", ""], "post": ["", "        value = self.model.value", "", "        clipped_value = tf.add(self.sampled_value,", "                               tf.clip_by_value(value - self.sampled_value, -self.clip_range, self.clip_range),"], "code": ["        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")"], "note": "#### Entropy Bonus\n\n$\\mathcal{L}^{EB}(\\theta) =\n \\mathbb{E}\\Bigl[ S\\bigl[\\pi_\\theta\\bigr] (s_t) \\Bigr]$" }, { "pre": ["                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")", "", "        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")", ""], "post": ["", "        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)", "", "        params = self.model.params", "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)"], "code": ["        value = self.model.value", "", "        clipped_value = tf.add(self.sampled_value,", "                               tf.clip_by_value(value - self.sampled_value, -self.clip_range, self.clip_range),", "                               name=\"clipped_value\")", "        self.vf_loss = tf.multiply(0.5,", "                                   tf.reduce_mean(tf.maximum(tf.square(value - self.sampled_return),", "                                                             tf.square(clipped_value - self.sampled_return))),", "                                   name=\"vf_loss\")"], "note": "#### Value" }, { "pre": ["                                                       clipped_ratio * self.sampled_normalized_advantage),", "                                            name=\"policy_reward\")", "", "        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")", ""], "post": ["", "        clipped_value = tf.add(self.sampled_value,", "                               tf.clip_by_value(value - self.sampled_value, -self.clip_range, self.clip_range),", "                               name=\"clipped_value\")", "        self.vf_loss = tf.multiply(0.5,"], "code": ["        value = self.model.value"], "note": "$V^{\\pi_\\theta}(s_t)$" }, { "pre": ["", "        self.entropy_bonus = tf.reduce_mean(self.model.policy_entropy, name=\"entropy_bonus\")", "", "        value = self.model.value", ""], "post": ["", "        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)", "", "        params = self.model.params", "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)"], "code": ["        clipped_value = tf.add(self.sampled_value,", "                               tf.clip_by_value(value - self.sampled_value, -self.clip_range, self.clip_range),", "                               name=\"clipped_value\")", "        self.vf_loss = tf.multiply(0.5,", "                                   tf.reduce_mean(tf.maximum(tf.square(value - self.sampled_return),", "                                                             tf.square(clipped_value - self.sampled_return))),", "                                   name=\"vf_loss\")"], "note": "\\begin{align}\nV^{\\pi_\\theta}_{CLIP}(s_t)\n &= clip\\Bigl(V^{\\pi_\\theta}(s_t) - \\hat{V_t}, -\\epsilon, +\\epsilon\\Bigr)\n\\\\\n\\mathcal{L}^{VF}(\\theta)\n &= \\frac{1}{2} \\mathbb{E} \\biggl[\n  max\\Bigl(\\bigl(V^{\\pi_\\theta}(s_t) - R_t\\bigr)^2,\n      \\bigl(V^{\\pi_\\theta}_{CLIP}(s_t) - R_t\\bigr)^2\\Bigr)\n \\biggr]\n\\end{align}\n\nClipping makes sure the value function $V_\\theta$ doesn't deviate\n significantly from $V_{\\theta_{OLD}}$." }, { "pre": ["        self.vf_loss = tf.multiply(0.5,", "                                   tf.reduce_mean(tf.maximum(tf.square(value - self.sampled_return),", "                                                             tf.square(clipped_value - self.sampled_return))),", "                                   name=\"vf_loss\")", ""], "post": ["", "        params = self.model.params", "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)", "", "        adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-5)"], "code": ["        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)"], "note": "$\\mathcal{L}^{CLIP+VF+EB} (\\theta) =\n \\mathcal{L}^{CLIP} (\\theta) - c_1 \\mathcal{L}^{VF} (\\theta) + c_2 \\mathcal{L}^{EB}(\\theta)$\n\nwe want to maximize $\\mathcal{L}^{CLIP+VF+EB}(\\theta)$ so we take the negative of it as the loss" }, { "pre": ["                                                             tf.square(clipped_value - self.sampled_return))),", "                                   name=\"vf_loss\")", "", "        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)", ""], "post": ["", "        adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-5)", "        grads_and_vars = list(zip(grads, params))", "        self.train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")", ""], "code": ["        params = self.model.params", "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)"], "note": "compute gradients" }, { "pre": ["        self.loss = -(self.policy_reward - 0.5 * self.vf_loss + 0.01 * self.entropy_bonus)", "", "        params = self.model.params", "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, params), 0.5)", ""], "post": ["", "        self.approx_kl_divergence = .5 * tf.reduce_mean(tf.square(neg_log_pi - self.sampled_neg_log_pi))", "        self.clip_fraction = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range)))", "", "        self.train_info_labels = ['policy_reward',"], "code": ["        adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-5)", "        grads_and_vars = list(zip(grads, params))", "        self.train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")"], "note": "*Adam* optimizer" }, { "pre": ["", "        adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-5)", "        grads_and_vars = list(zip(grads, params))", "        self.train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")", ""], "post": ["", "        self.train_info_labels = ['policy_reward',", "                                  'value_loss',", "                                  'entropy_bonus',", "                                  'approx_kl_divergence',"], "code": ["        self.approx_kl_divergence = .5 * tf.reduce_mean(tf.square(neg_log_pi - self.sampled_neg_log_pi))", "        self.clip_fraction = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range)))"], "note": "for monitoring" }, { "pre": ["        self.train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")", "", "        self.approx_kl_divergence = .5 * tf.reduce_mean(tf.square(neg_log_pi - self.sampled_neg_log_pi))", "        self.clip_fraction = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range)))", ""], "post": ["", "    def train(self, session: tf.Session, samples: Dict[str, np.ndarray], learning_rate: float, clip_range: float):", "        feed_dict = {self.sampled_obs: samples['obs'],", "                     self.sampled_action: samples['actions'],", "                     self.sampled_return: samples['values'] + samples['advantages'],"], "code": ["        self.train_info_labels = ['policy_reward',", "                                  'value_loss',", "                                  'entropy_bonus',", "                                  'approx_kl_divergence',", "                                  'clip_fraction']"], "note": "labels training progress indicators for monitoring" }, { "pre": ["                                  'value_loss',", "                                  'entropy_bonus',", "                                  'approx_kl_divergence',", "                                  'clip_fraction']", ""], "post": ["", "    @staticmethod", "    def _normalize(adv: np.ndarray):", "        return (adv - adv.mean()) / (adv.std() + 1e-8)", ""], "code": ["    def train(self, session: tf.Session, samples: Dict[str, np.ndarray], learning_rate: float, clip_range: float):", "        feed_dict = {self.sampled_obs: samples['obs'],", "                     self.sampled_action: samples['actions'],", "                     self.sampled_return: samples['values'] + samples['advantages'],", "                     self.sampled_normalized_advantage: Trainer._normalize(samples['advantages']),", "                     self.sampled_value: samples['values'],", "                     self.sampled_neg_log_pi: samples['neg_log_pis'],", "                     self.learning_rate: learning_rate,", "                     self.clip_range: clip_range}", "", "        evals = [self.policy_reward,", "                 self.vf_loss,", "                 self.entropy_bonus,", "                 self.approx_kl_divergence,", "                 self.clip_fraction,", "                 self.train_op]", "", "        return session.run(evals, feed_dict=feed_dict)[:-1]"], "note": "### Train model with samples" }, { "pre": ["                 self.entropy_bonus,", "                 self.approx_kl_divergence,", "                 self.clip_fraction,", "                 self.train_op]", ""], "post": ["", "    @staticmethod", "    def _normalize(adv: np.ndarray):", "        return (adv - adv.mean()) / (adv.std() + 1e-8)", ""], "code": ["        return session.run(evals, feed_dict=feed_dict)[:-1]"], "note": "return all results except `train_op`" }, { "pre": ["                 self.train_op]", "", "        return session.run(evals, feed_dict=feed_dict)[:-1]", "", "    @staticmethod"], "post": ["", "", "class Main(object):", "    def __init__(self):", "        self.gamma = 0.99"], "code": ["    def _normalize(adv: np.ndarray):", "        return (adv - adv.mean()) / (adv.std() + 1e-8)"], "note": "#### Normalize advantage function" }];
});

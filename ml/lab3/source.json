{"lab/logger/store.py": ["from collections import deque", "from pathlib import PurePath", "from typing import Dict, List", "", "from lab.logger import internal", "from lab import util", "from .indicators import IndicatorType, IndicatorOptions, Indicator", "from .writers import Writer", "", "", "class Store:", "    indicators: Dict[str, Indicator]", "", "    def __init__(self, logger: 'internal.LoggerInternal'):", "        self.values = {}", "        # self.queues = {}", "        # self.histograms = {}", "        # self.pairs: Dict[str, List[Tuple[int, int]]] = {}", "        # self.scalars = {}", "        self.__logger = logger", "        self.indicators = {}", "        self.__indicators_file = None", "", "    def save_indicators(self, file: PurePath):", "        self.__indicators_file = file", "", "        indicators = {k: ind.to_dict() for k, ind in self.indicators.items()}", "        with open(str(file), \"w\") as file:", "            file.write(util.yaml_dump(indicators))", "", "    def add_indicator(self, indicator: Indicator):", "        \"\"\"", "        ### Add an indicator", "        \"\"\"", "", "        assert indicator.name not in self.indicators", "", "        self.indicators[indicator.name] = indicator", "", "        self.__init_value(indicator.name)", "", "        if self.__indicators_file is not None:", "            self.save_indicators(self.__indicators_file)", "", "    def __init_value(self, name):", "        ind = self.indicators[name]", "        if ind.type_ == 'queue':", "            self.values[name] = deque(maxlen=ind.options.queue_size)", "        else:", "            self.values[name] = []", "", "    def _store_list(self, items: List[Dict[str, float]]):", "        for item in items:", "            self.store(**item)", "", "    def _store_kv(self, k, v):", "        if k not in self.indicators:", "            self.__logger.add_indicator(k, IndicatorType.scalar, IndicatorOptions(is_print=True))", "", "        if self.indicators[k].type_ == IndicatorType.pair:", "            if type(v) == tuple:", "                assert len(v) == 2", "                self.values[k].append((v[0], v[1]))", "            else:", "                assert type(v) == list", "                self.values[k] += v", "        else:", "            self.values[k].append(v)", "", "    def _store_kvs(self, **kwargs):", "        for k, v in kwargs.items():", "            self._store_kv(k, v)", "", "    def store(self, *args, **kwargs):", "        \"\"\"", "        ### Stores a value in the logger.", "", "        This may be added to a queue, a list or stored as", "        a TensorBoard histogram depending on the", "        type of the indicator.", "        \"\"\"", "        assert len(args) <= 2", "", "        if len(args) == 0:", "            self._store_kvs(**kwargs)", "        elif len(args) == 1:", "            assert not kwargs", "            assert isinstance(args[0], list)", "            self._store_list(args[0])", "        elif len(args) == 2:", "            assert isinstance(args[0], str)", "            if isinstance(args[1], list):", "                for v in args[1]:", "                    self._store_kv(args[0], v)", "            else:", "                self._store_kv(args[0], args[1])", "", "    def clear(self):", "        for k, v in self.indicators.items():", "            if v.type_ != IndicatorType.queue:", "                self.__init_value(k)", "", "    def write(self, writer: Writer, global_step):", "        return writer.write(global_step=global_step,", "                            values=self.values,", "                            indicators=self.indicators)"], "lab/logger/internal.py": ["import typing", "from pathlib import PurePath", "from typing import List, Optional, Tuple, Union", "", "from . import colors", "from .delayed_keyboard_interrupt import DelayedKeyboardInterrupt", "from .indicators import IndicatorType, IndicatorOptions, Indicator", "from .iterator import Iterator", "from .loop import Loop", "from .sections import Section, section_factory", "from .store import Store", "from .writers import Writer, ScreenWriter", "", "", "class CheckpointSaver:", "    def save(self, global_step):", "        raise NotImplementedError()", "", "", "class LoggerInternal:", "    \"\"\"", "    ## \ud83d\udda8 Logger class", "    \"\"\"", "", "    def __init__(self):", "        \"\"\"", "        ### Initializer", "        \"\"\"", "        self.__store = Store(self)", "        self.__writers: List[Writer] = []", "", "        self.__loop: Optional[Loop] = None", "        self.__sections: List[Section] = []", "", "        self.__indicators_print = []", "", "        self.__screen_writer = ScreenWriter(True)", "        self.__writers.append(self.__screen_writer)", "", "        self.__checkpoint_saver: Optional[CheckpointSaver] = None", "", "        self.__start_global_step: Optional[int] = None", "        self.__global_step: Optional[int] = None", "        self.__last_global_step: Optional[int] = None", "", "    def set_checkpoint_saver(self, saver: CheckpointSaver):", "        self.__checkpoint_saver = saver", "", "    @property", "    def global_step(self) -> int:", "        if self.__global_step is not None:", "            return self.__global_step", "", "        global_step = 0", "        if self.__start_global_step is not None:", "            global_step = self.__start_global_step", "", "        if self.__loop is not None:", "            return global_step + self.__loop.counter", "", "        if self.__last_global_step is not None:", "            return self.__last_global_step", "", "        return global_step", "", "    @staticmethod", "    def ansi_code(text: str, color: List[colors.ANSICode] or colors.ANSICode or None):", "        \"\"\"", "        ### Add ansi color codes", "        \"\"\"", "        if color is None:", "            return text", "        elif type(color) is list:", "            return \"\".join(color) + f\"{text}{colors.Reset}\"", "        else:", "            return f\"{color}{text}{colors.Reset}\"", "", "    def add_writer(self, writer: Writer):", "        self.__writers.append(writer)", "", "    def log(self, message, *,", "            color: List[colors.ANSICode] or colors.ANSICode or None = None,", "            new_line=True):", "        \"\"\"", "        ### Print a message to screen in color", "        \"\"\"", "", "        message = self.ansi_code(message, color)", "", "        if new_line:", "            end_char = '\\n'", "        else:", "            end_char = ''", "", "        text = \"\".join(message)", "", "        print(\"\\r\" + text, end=end_char, flush=True)", "", "    def log_color(self, parts: List[Union[str, Tuple[str, colors.ANSICode]]], *,", "                  new_line=True):", "        \"\"\"", "        ### Print a message with different colors.", "        \"\"\"", "", "        tuple_parts = []", "        for p in parts:", "            if type(p) == str:", "                tuple_parts.append((p, None))", "            else:", "                tuple_parts.append(p)", "        coded = [self.ansi_code(text, color) for text, color in tuple_parts]", "        self.log(\"\".join(coded), new_line=new_line)", "", "    def add_indicator(self, name: str,", "                      type_: IndicatorType,", "                      options: IndicatorOptions = None):", "", "        \"\"\"", "        ### Add an indicator", "        \"\"\"", "", "        if options is None:", "            options = IndicatorOptions()", "", "        indicator = Indicator(name, type_, options)", "", "        self.__store.add_indicator(indicator)", "", "    def save_indicators(self, file: PurePath):", "        self.__store.save_indicators(file)", "", "    def store(self, *args, **kwargs):", "        \"\"\"", "        ### Stores a value in the logger.", "", "        This may be added to a queue, a list or stored as", "        a TensorBoard histogram depending on the", "        type of the indicator.", "        \"\"\"", "", "        self.__store.store(*args, **kwargs)", "", "    def set_global_step(self, global_step):", "        self.__global_step = global_step", "", "    def set_start_global_step(self, global_step):", "        self.__start_global_step = global_step", "", "    def add_global_step(self, global_step: int = 1):", "        if self.__global_step is None:", "            if self.__start_global_step is not None:", "                self.__global_step = self.__start_global_step", "            else:", "                self.__global_step = 0", "", "        self.__global_step += global_step", "", "    @staticmethod", "    def new_line():", "        print()", "", "    def write(self):", "        \"\"\"", "        ### Output the stored log values to screen and TensorBoard summaries.", "        \"\"\"", "", "        global_step = self.global_step", "", "        for w in self.__writers:", "            self.__store.write(w, global_step)", "        self.__indicators_print = self.__store.write(self.__screen_writer, global_step)", "        self.__store.clear()", "", "        parts = [(f\"{self.global_step :8,}:  \", colors.BrightColor.orange)]", "        if self.__loop is None:", "            parts += self.__indicators_print", "            self.log_color(parts, new_line=False)", "        else:", "            self.__log_looping_line()", "", "    def save_checkpoint(self):", "        if self.__checkpoint_saver is None:", "            return", "", "        self.__checkpoint_saver.save(self.global_step)", "", "    def iterator(self, name, iterable: Union[typing.Iterable, typing.Sized, int],", "                 total_steps: Optional[int] = None, *,", "                 is_silent: bool = False,", "                 is_timed: bool = True):", "        return Iterator(logger=self,", "                        name=name,", "                        iterable=iterable,", "                        is_silent=is_silent,", "                        is_timed=is_timed,", "                        total_steps=total_steps,", "                        is_enumarate=False)", "", "    def enumerator(self, name, iterable: typing.Sized, *,", "                   is_silent: bool = False,", "                   is_timed: bool = True):", "        return Iterator(logger=self,", "                        name=name,", "                        iterable=iterable,", "                        is_silent=is_silent,", "                        is_timed=is_timed,", "                        total_steps=None,", "                        is_enumarate=True)", "", "    def section(self, name, *,", "                is_silent: bool = False,", "                is_timed: bool = True,", "                is_partial: bool = False,", "                total_steps: float = 1.0):", "", "        if self.__loop is not None:", "            if len(self.__sections) != 0:", "                raise RuntimeError(\"No nested sections within loop\")", "", "            section = self.__loop.get_section(name=name,", "                                              is_silent=is_silent,", "                                              is_timed=is_timed,", "                                              is_partial=is_partial,", "                                              total_steps=total_steps)", "            self.__sections.append(section)", "        else:", "            self.__sections.append(section_factory(logger=self,", "                                                   name=name,", "                                                   is_silent=is_silent,", "                                                   is_timed=is_timed,", "                                                   is_partial=is_partial,", "                                                   total_steps=total_steps,", "                                                   is_looping=False,", "                                                   level=len(self.__sections)))", "", "        return self.__sections[-1]", "", "    def progress(self, steps: float):", "        if len(self.__sections) == 0:", "            raise RuntimeError(\"You must be within a section to report progress\")", "", "        if self.__sections[-1].progress(steps):", "            self.__log_line()", "", "    def set_successful(self, is_successful=True):", "        if len(self.__sections) == 0:", "            raise RuntimeError(\"You must be within a section to report success\")", "", "        self.__sections[-1].is_successful = is_successful", "        self.__log_line()", "", "    def loop(self, iterator_: range, *,", "             is_print_iteration_time=True):", "        if len(self.__sections) != 0:", "            raise RuntimeError(\"Cannot start a loop within a section\")", "", "        self.__loop = Loop(iterator=iterator_, logger=self,", "                           is_print_iteration_time=is_print_iteration_time)", "        return self.__loop", "", "    def finish_loop(self):", "        if len(self.__sections) != 0:", "            raise RuntimeError(\"Cannot be within a section when finishing the loop\")", "        self.__last_global_step = self.global_step", "        self.__loop = None", "", "    def section_enter(self, section):", "        if len(self.__sections) == 0:", "            raise RuntimeError(\"Entering a section without creating a section.\\n\"", "                               \"Always use logger.section to create a section\")", "", "        if section is not self.__sections[-1]:", "            raise RuntimeError(\"Entering a section other than the one last_created\\n\"", "                               \"Always user with logger.section(...):\")", "", "        if len(self.__sections) > 1 and not self.__sections[-2].is_parented:", "            self.__sections[-2].make_parent()", "            self.new_line()", "", "        self.__log_line()", "", "    def __log_looping_line(self):", "        parts = [(f\"{self.global_step :8,}:  \", colors.BrightColor.orange)]", "        parts += self.__loop.log_sections()", "        parts += self.__indicators_print", "        parts += self.__loop.log_progress()", "", "        self.log_color(parts, new_line=False)", "", "    def __log_line(self):", "        if self.__loop is not None:", "            self.__log_looping_line()", "            return", "", "        if len(self.__sections) == 0:", "            return", "", "        self.log_color(self.__sections[-1].log(), new_line=False)", "", "    def section_exit(self, section):", "        if len(self.__sections) == 0:", "            raise RuntimeError(\"Impossible\")", "", "        if section is not self.__sections[-1]:", "            raise RuntimeError(\"Impossible\")", "", "        self.__log_line()", "        self.__sections.pop(-1)", "", "    def delayed_keyboard_interrupt(self):", "        \"\"\"", "        ### Create a section with a delayed keyboard interrupt", "        \"\"\"", "        return DelayedKeyboardInterrupt(self)", "", "    def _log_key_value(self, items: List[Tuple[any, any]], is_show_count=True):", "        max_key_len = 0", "        for k, v in items:", "            max_key_len = max(max_key_len, len(str(k)))", "", "        count = 0", "        for k, v in items:", "            count += 1", "            spaces = \" \" * (max_key_len - len(str(k)))", "            self.log_color([(f\"{spaces}{k}: \", None),", "                            (str(v), colors.Style.bold)])", "", "        if is_show_count:", "            self.log_color([", "                (\"Total \", None),", "                (str(count), colors.Style.bold),", "                (\" item(s)\", None)])", "", "    def info(self, *args, **kwargs):", "        \"\"\"", "        ### \ud83c\udfa8 Pretty prints a set of values.", "        \"\"\"", "", "        if len(args) == 0:", "            self._log_key_value([(k, v) for k, v in kwargs.items()], False)", "        elif len(args) == 1:", "            assert len(kwargs.keys()) == 0", "            arg = args[0]", "            if type(arg) == list:", "                self._log_key_value([(i, v) for i, v in enumerate(arg)])", "            elif type(arg) == dict:", "                self._log_key_value([(k, v) for k, v in arg.items()])", "        else:", "            assert len(kwargs.keys()) == 0", "            self._log_key_value([(i, v) for i, v in enumerate(args)], False)"], "lab/logger/sections.py": ["import math", "import time", "", "from lab.logger import internal, colors", "", "", "class Section:", "    def __init__(self, *,", "                 logger: 'internal.LoggerInternal',", "                 name: str,", "                 is_silent: bool,", "                 is_timed: bool,", "                 is_partial: bool,", "                 total_steps: float):", "        self._logger = logger", "        self._name = name", "        self._is_silent = is_silent", "        self._is_timed = is_timed", "        self._is_partial = is_partial", "        self._total_steps = total_steps", "", "        self._state = 'none'", "        self._has_entered_ever = False", "", "        self._start_time = 0", "        self._end_time = -1", "        self._progress = 0.", "        self._start_progress = 0", "        self._end_progress = 0", "        self._is_parented = False", "", "        self.is_successful = True", "", "    def __enter__(self):", "        self._state = 'entered'", "        self._has_entered_ever = True", "        self.is_successful = True", "", "        if not self._is_partial:", "            self._progress = 0", "", "        self._start_progress = self._progress", "", "        if self._is_timed:", "            self._start_time = time.time()", "", "        self._logger.section_enter(self)", "", "        return self", "", "    def __exit__(self, exc_type, exc_val, exc_tb):", "        self._state = 'exited'", "        if self._is_timed:", "            self._end_time = time.time()", "", "        if not self._is_partial:", "            self._progress = 1.", "", "        self._end_progress = self._progress", "", "        self._logger.section_exit(self)", "", "    def log(self):", "        raise NotImplementedError()", "", "    def progress(self, steps):", "        old_progress = self._progress", "        self._progress = steps / self._total_steps", "", "        if self._is_silent:", "            return False", "", "        if math.floor(self._progress * 100) != math.floor(old_progress * 100):", "            return True", "        else:", "            return False", "", "    @property", "    def is_parented(self):", "        return self._is_parented", "", "    def make_parent(self):", "        self._is_parented = True", "", "", "class OuterSection(Section):", "    def __init__(self, *,", "                 logger: 'internal.LoggerInternal',", "                 name: str,", "                 is_silent: bool,", "                 is_timed: bool,", "                 is_partial: bool,", "                 total_steps: float,", "                 level: int):", "        if is_partial:", "            raise RuntimeError(\"Only sections within the loop can be partial.\")", "", "        self._level = level", "", "        super().__init__(logger=logger,", "                         name=name,", "                         is_silent=is_silent,", "                         is_timed=is_timed,", "                         is_partial=is_partial,", "                         total_steps=total_steps)", "", "    def log(self):", "        if self._is_silent:", "            return", "", "        if self._state is 'none':", "            return", "", "        parts = [(\"  \" * self._level + f\"{self._name}\", None)]", "", "        if self._state is 'entered':", "            if self._progress == 0.:", "                parts.append((\"...\", None))", "            else:", "                parts.append((f\" {math.floor(self._progress * 100) :4.0f}%\", None))", "        else:", "            if self.is_successful:", "                parts.append((\"...[DONE]\", colors.BrightColor.green))", "            else:", "                parts.append((\"...[FAIL]\", colors.BrightColor.red))", "", "            if self._is_timed:", "                duration_ms = 1000 * (self._end_time - self._start_time)", "                parts.append((f\"\\t{duration_ms :,.2f}ms\",", "                              colors.BrightColor.cyan))", "", "            parts.append((\"\\n\", None))", "", "        return parts", "", "", "class LoopingSection(Section):", "    def __init__(self, *,", "                 logger: 'internal.LoggerInternal',", "                 name: str,", "                 is_silent: bool,", "                 is_timed: bool,", "                 is_partial: bool,", "                 total_steps: float):", "        super().__init__(logger=logger,", "                         name=name,", "                         is_silent=is_silent,", "                         is_timed=is_timed,", "                         is_partial=is_partial,", "                         total_steps=total_steps)", "        self._beta_pow = 1.", "        self._beta = 0.9", "        self._estimated_time = 0.", "        self._time_length = 7", "        self._last_end_time = -1.", "        self._last_start_time = -1.", "        self._last_step_time = -1.", "", "    def _get_estimated_time(self):", "        et = self._estimated_time * self._beta", "        et += (1 - self._beta) * self._last_step_time", "        return et / (1 - self._beta_pow * self._beta)", "", "    def _calc_estimated_time(self):", "        if self._state != 'entered':", "            if self._last_end_time == self._end_time:", "                return self._get_estimated_time()", "            end_time = self._end_time", "            end_progress = self._end_progress", "            self._last_end_time = self._end_time", "        else:", "            end_time = time.time()", "            end_progress = self._progress", "", "        if end_progress - self._start_progress < 1e-6:", "            return self._get_estimated_time()", "", "        current_estimate = ((end_time - self._start_time) /", "                            (end_progress - self._start_progress))", "", "        if self._last_start_time == self._start_time:", "            # print(current_estimate)", "            self._last_step_time = current_estimate", "        else:", "            if self._last_step_time >= 0.:", "                self._beta_pow *= self._beta", "                self._estimated_time *= self._beta", "                self._estimated_time += (1 - self._beta) * self._last_step_time", "            # print(self._last_step_time, current_estimate)", "            self._last_step_time = current_estimate", "            self._last_start_time = self._start_time", "", "        return self._get_estimated_time()", "", "    def log(self):", "        if self._is_silent:", "            return []", "", "        if self._state == 'none':", "            return []", "", "        parts = [(f\"{self._name}:\", None)]", "        color = None", "", "        if not self.is_successful:", "            color = colors.BrightColor.red", "", "        if self._progress == 0.:", "            parts.append((\"  ...\", colors.Color.orange))", "        else:", "            parts.append((f\"{math.floor(self._progress * 100) :4.0f}%\",", "                          color or colors.Color.orange))", "", "        if self._is_timed:", "            duration_ms = 1000 * self._calc_estimated_time()", "            s = f\" {duration_ms:,.0f}ms  \"", "            tl = len(s)", "            if tl > self._time_length:", "                self._time_length = tl", "            else:", "                s = (\" \" * (self._time_length - tl)) + s", "", "            parts.append((s, color or colors.BrightColor.cyan))", "", "        return parts", "", "", "def section_factory(logger: 'internal.LoggerInternal',", "                    name: str,", "                    is_looping: bool,", "                    is_silent: bool,", "                    is_timed: bool,", "                    is_partial: bool,", "                    total_steps: float,", "                    level: int = 0):", "    if is_looping:", "        return LoopingSection(logger=logger,", "                              name=name,", "                              is_silent=is_silent,", "                              is_timed=is_timed,", "                              is_partial=is_partial,", "                              total_steps=total_steps)", "    else:", "        return OuterSection(logger=logger,", "                            name=name,", "                            is_silent=is_silent,", "                            is_timed=is_timed,", "                            is_partial=is_partial,", "                            total_steps=total_steps,", "                            level=level)"], "lab/logger/writers/__init__.py": ["from typing import Dict", "", "import numpy as np", "", "from .. import colors", "from ..indicators import Indicator", "", "", "class Writer:", "    def write(self, *,", "              global_step: int,", "              values: Dict[str, any],", "              indicators: Dict[str, Indicator]):", "        raise NotImplementedError()", "", "", "class ScreenWriter(Writer):", "    def __init__(self, is_color=True):", "        super().__init__()", "", "        self.is_color = is_color", "", "    def write(self, *,", "              global_step: int,", "              values: Dict[str, any],", "              indicators: Dict[str, Indicator]):", "        parts = []", "", "        for k, ind in indicators.items():", "            if not ind.options.is_print:", "                continue", "", "            if len(values[k]) == 0:", "                continue", "", "            v = np.mean(values[k])", "", "            parts.append((f\" {k}: \", None))", "            if self.is_color:", "                parts.append((f\"{v :8,.2f}\", colors.Style.bold))", "            else:", "                parts.append((f\"{v :8,.2f}\", None))", "", "        return parts"], "lab/logger/writers/sqlite.py": ["import sqlite3", "from pathlib import PurePath", "from typing import Dict, Optional", "", "import numpy as np", "", "from . import Writer as WriteBase", "from ..indicators import Indicator", "", "", "class Writer(WriteBase):", "    conn: Optional[sqlite3.Connection]", "", "    def __init__(self, sqlite_path: PurePath):", "        super().__init__()", "", "        self.sqlite_path = sqlite_path", "        self.conn = None", "", "    def __connect(self):", "        if self.conn is not None:", "            return", "", "        self.conn = sqlite3.connect(str(self.sqlite_path))", "", "        try:", "            self.conn.execute(f\"CREATE TABLE scalars (indicator text, step integer, value real)\")", "", "        except sqlite3.OperationalError:", "            print('Scalar table exists')", "", "    @staticmethod", "    def _parse_key(key: str):", "        return key", "        # if we name tables", "        # return key.replace('.', '_')", "", "    def _get_key(self, indicator):", "        if indicator.type_ != 'scalar':", "            return self._parse_key(f'{indicator.name}.mean')", "        else:", "            return self._parse_key(f'{indicator.name}')", "", "    def write(self, *,", "              global_step: int,", "              values: Dict[str, any],", "              indicators: Dict[str, Indicator]):", "        self.__connect()", "", "        for k, ind in indicators.items():", "            v = values[k]", "            if len(v) == 0:", "                continue", "            key = self._get_key(ind)", "            self.conn.execute(", "                f\"INSERT INTO scalars VALUES ('{key}', {global_step}, {float(np.mean(v))})\")", "", "        self.conn.commit()"], "lab/logger/writers/tensorboard.py": ["from pathlib import PurePath", "from typing import Dict", "", "import numpy as np", "import tensorflow as tf", "", "from . import Writer as WriteBase", "from ..indicators import Indicator, IndicatorType", "", "", "class Writer(WriteBase):", "    def __init__(self, log_path: PurePath):", "        super().__init__()", "", "        self.__log_path = log_path", "        self.__writer = None", "", "    def __connect(self):", "        if self.__writer is not None:", "            return", "", "        self.__writer = tf.summary.create_file_writer(str(self.__log_path))", "", "    @staticmethod", "    def _parse_key(key: str):", "        return key.replace('.', '/')", "", "    def write(self, *,", "              global_step: int,", "              values: Dict[str, any],", "              indicators: Dict[str, Indicator]):", "        self.__connect()", "", "        with self.__writer.as_default():", "            for k, ind in indicators.items():", "                v = values[k]", "                if len(v) == 0:", "                    continue", "                if ind.type_ == IndicatorType.queue or ind.type_ == IndicatorType.histogram:", "                    tf.summary.histogram(self._parse_key(k), v, step=global_step)", "", "                if ind.type_ != IndicatorType.scalar:", "                    key = self._parse_key(f\"{k}.mean\")", "                else:", "                    key = self._parse_key(f\"{k}\")", "", "                tf.summary.scalar(key, float(np.mean(v)), step=global_step)"], "lab/logger/util.py": ["import torch", "", "from .indicators import IndicatorType, IndicatorOptions", "from .. import logger", "", "", "def add_model_indicators(model: torch.nn.Module, model_name: str = \"model\"):", "    for name, param in model.named_parameters():", "        if param.requires_grad:", "            logger.add_indicator(f\"{model_name}.{name}\",", "                                 IndicatorType.histogram,", "                                 IndicatorOptions(is_print=False))", "            logger.add_indicator(f\"{model_name}.{name}.grad\",", "                                 IndicatorType.histogram,", "                                 IndicatorOptions(is_print=False))", "", "", "def store_model_indicators(model: torch.nn.Module, model_name: str = \"model\"):", "    for name, param in model.named_parameters():", "        if param.requires_grad:", "            logger.store(f\"{model_name}.{name}\", param.data.cpu().numpy())", "            logger.store(f\"{model_name}.{name}.grad\", param.grad.cpu().numpy())"], "lab/logger/__init__.py": ["import typing", "", "from .colors import ANSICode", "from .indicators import IndicatorType, IndicatorOptions", "from .internal import LoggerInternal as _LoggerInternal", "", "_internal: typing.Optional[_LoggerInternal] = None", "", "", "def internal() -> _LoggerInternal:", "    global _internal", "    if _internal is None:", "        _internal = _LoggerInternal()", "", "    return _internal", "", "", "def log(message, *,", "        color: typing.List[ANSICode] or ANSICode or None = None,", "        new_line=True):", "    internal().log(message, color=color, new_line=new_line)", "", "", "def log_color(parts: typing.List[typing.Union[str, typing.Tuple[str, ANSICode]]], *,", "              new_line=True):", "    internal().log_color(parts, new_line=new_line)", "", "", "def add_indicator(name: str,", "                  type_: IndicatorType = IndicatorType.scalar,", "                  options: IndicatorOptions = None):", "    internal().add_indicator(name, type_, options)", "", "", "def store(*args, **kwargs):", "    \"\"\"", "    ### Stores a value in the logger.", "", "    This may be added to a queue, a list or stored as", "    a TensorBoard histogram depending on the", "    type of the indicator.", "    \"\"\"", "", "    internal().store(*args, **kwargs)", "", "", "def set_global_step(global_step):", "    internal().set_global_step(global_step)", "", "", "def add_global_step(global_step: int = 1):", "    internal().add_global_step(global_step)", "", "", "def new_line():", "    internal().new_line()", "", "", "def write():", "    \"\"\"", "    ### Output the stored log values to screen and TensorBoard summaries.", "    \"\"\"", "", "    internal().write()", "", "", "def save_checkpoint():", "    internal().save_checkpoint()", "", "", "def iterator(name, iterable: typing.Union[typing.Iterable, typing.Sized, int],", "             total_steps: typing.Optional[int] = None, *,", "             is_silent: bool = False,", "             is_timed: bool = True):", "    return internal().iterator(name, iterable, total_steps, is_silent=is_silent,", "                               is_timed=is_timed)", "", "", "def enumerator(name, iterable: typing.Sized, *,", "               is_silent: bool = False,", "               is_timed: bool = True):", "    return internal().enumerator(name, iterable, is_silent=is_silent, is_timed=is_timed)", "", "", "def section(name, *,", "            is_silent: bool = False,", "            is_timed: bool = True,", "            is_partial: bool = False,", "            total_steps: float = 1.0):", "    return internal().section(name, is_silent=is_silent,", "                              is_timed=is_timed,", "                              is_partial=is_partial,", "                              total_steps=total_steps)", "", "", "def progress(steps: float):", "    internal().progress(steps)", "", "", "def set_successful(is_successful=True):", "    internal().set_successful(is_successful)", "", "", "def loop(iterator_: range, *,", "         is_print_iteration_time=True):", "    return internal().loop(iterator_, is_print_iteration_time=is_print_iteration_time)", "", "", "def finish_loop():", "    internal().finish_loop()", "", "", "def delayed_keyboard_interrupt():", "    \"\"\"", "    ### Create a section with a delayed keyboard interrupt", "    \"\"\"", "    return internal().delayed_keyboard_interrupt()", "", "", "def info(*args, **kwargs):", "    \"\"\"", "    ### \ud83c\udfa8 Pretty prints a set of values.", "    \"\"\"", "", "    internal().info(*args, **kwargs)"], "lab/logger/indicators.py": ["from enum import Enum", "from typing import NamedTuple, Dict", "", "", "class IndicatorType(Enum):", "    queue = 'queue'", "    histogram = 'histogram'", "    scalar = 'scalar'", "    pair = 'pair'", "", "", "class IndicatorOptions(NamedTuple):", "    is_print: bool = False", "    queue_size: int = 10", "", "    def to_dict(self) -> Dict:", "        return dict(is_print=self.is_print,", "                    queue_size=self.queue_size)", "", "", "class Indicator(NamedTuple):", "    name: str", "    type_: IndicatorType", "    options: IndicatorOptions", "", "    def to_dict(self) -> Dict:", "        return dict(name=self.name,", "                    type=self.type_.value,", "                    options=self.options.to_dict())"], "lab/logger/iterator.py": ["import typing", "from typing import Optional, Iterable", "from lab.logger import internal", "", "", "class Iterator:", "    def __init__(self, *,", "                 logger: 'internal.LoggerInternal',", "                 name: str,", "                 iterable: typing.Union[Iterable, typing.Sized, int],", "                 is_silent: bool,", "                 is_timed: bool,", "                 total_steps: Optional[int],", "                 is_enumarate: bool):", "        if is_enumarate:", "            total_steps = len(iterable)", "            iterable = enumerate(iterable)", "        if type(iterable) is int:", "            total_steps = iterable", "            iterable = range(total_steps)", "        if total_steps is None:", "            sized: typing.Sized = iterable", "            total_steps = len(sized)", "", "        self._logger = logger", "        self._name = name", "        self._iterable: Iterable = iterable", "        self._iterator = Optional[typing.Iterator]", "        self._total_steps = total_steps", "        self._section = None", "        self._is_silent = is_silent", "        self._is_timed = is_timed", "        self._counter = -1", "", "    def __iter__(self):", "        self._section = self._logger.section(", "            self._name,", "            is_silent=self._is_silent,", "            is_timed=self._is_timed,", "            is_partial=False,", "            total_steps=self._total_steps)", "        self._iterator = iter(self._iterable)", "        self._section.__enter__()", "", "        return self", "", "    def __next__(self):", "        try:", "            self._counter += 1", "            self._logger.progress(self._counter)", "            next_value = next(self._iterator)", "        except StopIteration as e:", "            self._section.__exit__(None, None, None)", "            raise e", "", "        return next_value"], "lab/logger/delayed_keyboard_interrupt.py": ["import signal", "", "from lab.logger import colors", "import lab", "", "", "class DelayedKeyboardInterrupt:", "    \"\"\"", "    ### Capture `KeyboardInterrupt` and fire it later", "    \"\"\"", "", "    def __init__(self, logger: 'lab.logger.internal.LoggerInternal'):", "        self.signal_received = None", "        self.logger = logger", "", "    def __enter__(self):", "        self.signal_received = None", "        # Start capturing", "        self.old_handler = signal.signal(signal.SIGINT, self.handler)", "", "    def handler(self, sig, frame):", "        # Pass second interrupt without delaying", "        if self.signal_received is not None:", "            self.old_handler(*self.signal_received)", "            return", "", "        # Store the interrupt signal for later", "        self.signal_received = (sig, frame)", "        self.logger.log('\\nSIGINT received. Delaying KeyboardInterrupt.',", "                        color=colors.Color.red)", "", "    def __exit__(self, exc_type, exc_val, exc_tb):", "        # Reset handler", "        signal.signal(signal.SIGINT, self.old_handler)", "", "        # Pass on any captured interrupt signals", "        if self.signal_received is not None:", "            self.old_handler(*self.signal_received)"], "lab/logger/loop.py": ["import time", "from typing import Optional, Dict", "", "from lab.logger.sections import Section, section_factory", "from lab.logger import internal, colors", "", "", "class Loop:", "    def __init__(self, iterator: range, *,", "                 logger: 'internal.LoggerInternal',", "                 is_print_iteration_time: bool):", "        \"\"\"", "        Creates an iterator with a range `iterator`.", "", "        See example for usage.", "        \"\"\"", "        self.iterator = iterator", "        self.sections = {}", "        self._start_time = 0.", "        self._iter_start_time = 0.", "        self._init_time = 0.", "        self._iter_time = 0.", "        self._beta_pow = 1.", "        self._beta = 0.9", "        self.steps = len(iterator)", "        self.counter = 0", "        self.logger = logger", "        self.__global_step: Optional[int] = None", "        self.__looping_sections: Dict[str, Section] = {}", "        self._is_print_iteration_time = is_print_iteration_time", "", "    def __iter__(self):", "        self.iterator_iter = iter(self.iterator)", "        self._start_time = time.time()", "        self._init_time = 0.", "        self._iter_time = 0.", "        self.counter = 0", "        return self", "", "    def __next__(self):", "        try:", "            next_value = next(self.iterator_iter)", "        except StopIteration as e:", "            self.logger.finish_loop()", "            raise e", "", "        now = time.time()", "        if self.counter == 0:", "            self.__init_time = now - self._start_time", "        else:", "            self._beta_pow *= self._beta", "            self._iter_time *= self._beta", "            self._iter_time += (1 - self._beta) * (now - self._iter_start_time)", "", "        self._iter_start_time = now", "", "        self.counter += 1", "", "        return next_value", "", "    def log_progress(self):", "        \"\"\"", "        Show progress", "        \"\"\"", "        now = time.time()", "        spent = now - self._start_time", "        current_iter = now - self._iter_start_time", "", "        if self._iter_time != 0:", "            estimate = self._iter_time / (1 - self._beta_pow)", "        else:", "            estimate = current_iter", "", "        total_time = estimate * self.steps + self._init_time", "        remain = total_time - spent", "", "        remain /= 60", "        spent /= 60", "        estimate *= 1000", "", "        spent_h = int(spent // 60)", "        spent_m = int(spent % 60)", "        remain_h = int(remain // 60)", "        remain_m = int(remain % 60)", "", "        to_print = [(\"  \", None)]", "        if self._is_print_iteration_time:", "            to_print.append((f\"{estimate:,.0f}ms\", colors.BrightColor.cyan))", "        to_print.append((f\"{spent_h:3d}:{spent_m:02d}m/{remain_h:3d}:{remain_m:02d}m  \",", "                         colors.BrightColor.purple))", "", "        return to_print", "", "    def get_section(self, *, name: str,", "                    is_silent: bool,", "                    is_timed: bool,", "                    is_partial: bool,", "                    total_steps: float):", "        if name not in self.__looping_sections:", "            self.__looping_sections[name] = section_factory(logger=self.logger,", "                                                            name=name,", "                                                            is_silent=is_silent,", "                                                            is_timed=is_timed,", "                                                            is_partial=is_partial,", "                                                            total_steps=total_steps,", "                                                            is_looping=True)", "        return self.__looping_sections[name]", "", "    def log_sections(self):", "        parts = []", "        for name, section in self.__looping_sections.items():", "            parts += section.log()", "", "        return parts"], "lab/logger/colors.py": ["\"\"\"", "Console colors", "\"\"\"", "from enum import Enum", "", "", "class ANSICode(Enum):", "    def __str__(self):", "        return f\"\\33[{self.value}m\"", "", "", "class Style(ANSICode):", "    normal = 0", "    bold = 1", "    light = 2", "", "    italic = 3  # Not supported in PyCharm", "    underline = 4", "", "    highlight = 7", "", "", "class Color(ANSICode):", "    black = 30", "    red = 31", "    green = 32", "    orange = 33", "    blue = 34", "    purple = 35", "    cyan = 36", "    white = 37", "", "", "class BrightColor(ANSICode):", "    black = 90", "    red = 91", "    green = 92", "    orange = 93", "    blue = 94", "    purple = 95", "    cyan = 96", "    white = 97", "", "    gray = 37", "", "", "class Background(ANSICode):", "    black = 40", "    red = 41", "    green = 42", "    orange = 43", "    blue = 44", "    purple = 45", "    cyan = 46", "    white = 47", "", "", "class BrightBackground(ANSICode):", "    black = 100", "    red = 101", "    green = 102", "    orange = 103", "    blue = 104", "    purple = 105", "    cyan = 106", "    white = 107", "", "", "Reset = \"\\33[0m\"", "", "if __name__ == \"__main__\":", "    for i in [0, 38, 48]:", "        for j in [5]:", "            for k in range(16):", "                print(\"\\33[{};{};{}m{:02d},{},{:03d}\\33[0m\\t\".format(i, j, k, i, j, k),", "                      end='')", "                if (k + 1) % 6 == 0:", "                    print(\"\")", "            print(\"\")", "", "    for i in range(0, 128):", "        print(f\"\\33[{i}m{i :03d}\\33[0m \", end='')", "        if (i + 1) % 10 == 0:", "            print(\"\")", "", "    print(\"\")", "", "    for s in Style:", "        for c in Color:", "            for b in Background:", "                print(", "                    f\"{s}{c}{b}{s.name}, {c.name}, {b.name}{Reset}\")", "", "    print(Style.bold.name)"], "lab/util.py": ["import pathlib", "import random", "import string", "", "import yaml", "", "", "def yaml_load(s: str):", "    return yaml.load(s, Loader=yaml.FullLoader)", "", "", "def yaml_dump(obj: any):", "    return yaml.dump(obj, default_flow_style=False)", "", "", "def rm_tree(path_to_remove: pathlib.Path):", "    if path_to_remove.is_dir():", "        for f in path_to_remove.iterdir():", "            if f.is_dir():", "                rm_tree(f)", "            else:", "                f.unlink()", "        path_to_remove.rmdir()", "    else:", "        path_to_remove.unlink()", "", "", "def random_string(length=10):", "    letters = string.ascii_lowercase", "    return ''.join(random.choice(letters) for _ in range(length))"], "lab/__init__.py": ["from .logger import IndicatorType, IndicatorOptions", "import lab.logger as logger"], "lab/training_loop.py": ["import signal", "from typing import Any, Tuple, Optional", "", "from . import logger", "from .logger import colors", "from .configs import Configs", "", "", "class TrainingLoopConfigs(Configs):", "    loop_count: int = 10", "    is_save_models: bool = False", "    log_new_line_interval: int = 1", "    log_write_interval: int = 1", "    save_models_interval: int = 1", "", "    training_loop: 'TrainingLoop'", "", "", "@TrainingLoopConfigs.calc('training_loop')", "def _loop_configs(c: TrainingLoopConfigs):", "    return TrainingLoop(loop_count=c.loop_count,", "                        is_save_models=c.is_save_models,", "                        log_new_line_interval=c.log_new_line_interval,", "                        log_write_interval=c.log_write_interval,", "                        save_models_interval=c.save_models_interval)", "", "", "class TrainingLoop:", "    __signal_received: Optional[Tuple[Any, Any]]", "", "    def __init__(self, *,", "                 loop_count,", "                 is_save_models,", "                 log_new_line_interval,", "                 log_write_interval,", "                 save_models_interval):", "        self.__loop_count = loop_count", "        self.__is_save_models = is_save_models", "        self.__log_new_line_interval = log_new_line_interval", "        self.__log_write_interval = log_write_interval", "        self.__save_models_interval = save_models_interval", "        self.__loop = logger.loop(range(self.__loop_count))", "        self.__signal_received = None", "", "    def __iter__(self):", "        iter(self.__loop)", "        self.old_handler = signal.signal(signal.SIGINT, self.__handler)", "        return self", "", "    def __finish(self):", "        signal.signal(signal.SIGINT, self.old_handler)", "        logger.write()", "        logger.new_line()", "        if self.__is_save_models:", "            logger.save_checkpoint()", "", "    @staticmethod", "    def __is_interval(epoch, interval):", "        if epoch == 0:", "            return False", "", "        if epoch % interval == 0:", "            return True", "        else:", "            return False", "", "    def __next__(self):", "        if self.__signal_received is not None:", "            logger.log('\\nKilling Loop.',", "                       color=colors.Color.red)", "            logger.finish_loop()", "            self.__finish()", "            raise StopIteration(\"SIGINT\")", "", "        try:", "            epoch = next(self.__loop)", "        except StopIteration as e:", "            self.__finish()", "            raise e", "", "        if self.__is_interval(epoch, self.__log_write_interval):", "            logger.write()", "        if self.__is_interval(epoch, self.__log_new_line_interval):", "            logger.new_line()", "", "        if (self.__is_save_models and", "                self.__is_interval(epoch, self.__save_models_interval)):", "            logger.save_checkpoint()", "", "        return epoch", "", "    def __handler(self, sig, frame):", "        # Pass second interrupt without delaying", "        if self.__signal_received is not None:", "            self.old_handler(*self.__signal_received)", "            return", "", "        # Store the interrupt signal for later", "        self.__signal_received = (sig, frame)", "        logger.log('\\nSIGINT received. Delaying KeyboardInterrupt.',", "                   color=colors.Color.red)"], "lab/configs/__init__.py": ["from pathlib import PurePath", "from typing import List, Dict, Callable, Type, Optional, \\", "    Union", "", "from lab import util, logger", "from .calculator import Calculator", "from .config_function import ConfigFunction", "from .parser import Parser", "from ..logger.colors import BrightColor as Color", "", "_CALCULATORS = '_calculators'", "", "", "class Configs:", "    _calculators: Dict[str, List[ConfigFunction]] = {}", "", "    @classmethod", "    def calc(cls, name: Union[str, List[str]] = None,", "             option: str = None, *,", "             is_append: bool = False):", "        if _CALCULATORS not in cls.__dict__:", "            cls._calculators = {}", "", "        def wrapper(func: Callable):", "", "            calc = ConfigFunction(func, config_names=name, option_name=option, is_append=is_append)", "            if type(calc.config_names) == str:", "                config_names = [calc.config_names]", "            else:", "                config_names = calc.config_names", "", "            for n in config_names:", "                if n not in cls._calculators:", "                    cls._calculators[n] = []", "                cls._calculators[n].append(calc)", "", "            return func", "", "        return wrapper", "", "    @classmethod", "    def list(cls, name: str = None):", "        return cls.calc(name, f\"_{util.random_string()}\", is_append=True)", "", "", "def _get_base_classes(class_: Type[Configs]) -> List[Type[Configs]]:", "    classes = [class_]", "    level = [class_]", "    next_level = []", "", "    while len(level) > 0:", "        for c in level:", "            for b in c.__bases__:", "                if b == object:", "                    continue", "                next_level.append(b)", "        classes += next_level", "        level = next_level", "        next_level = []", "", "    classes.reverse()", "", "    return classes", "", "", "RESERVED = {'calc', 'list'}", "", "", "class ConfigProcessor:", "    def __init__(self, configs, values: Dict[str, any] = None):", "        self.parser = Parser(configs, values)", "        self.calculator = Calculator(configs=configs,", "                                     options=self.parser.options,", "                                     types=self.parser.types,", "                                     values=self.parser.values,", "                                     list_appends=self.parser.list_appends)", "", "    def __call__(self, run_order: Optional[List[Union[List[str], str]]]):", "        self.calculator(run_order)", "", "    def save(self, configs_path: PurePath):", "        configs = {", "            'values': self.parser.values,", "            'options': {},", "            'list_appends': {k: True for k in self.parser.list_appends},", "            'computed': {},", "            'order': self.calculator.topological_order", "        }", "", "        for k, opts in self.parser.options.items():", "            configs['options'][k] = list(opts.keys())", "", "        for k in self.parser.types:", "            computed = getattr(self.calculator.configs, k, None)", "            if computed is None:", "                continue", "", "            computed_str = str(computed)", "            if len(computed_str) > 100:", "                computed_str = computed_str[:150]", "", "            configs['computed'][k] = computed_str", "", "        with open(str(configs_path), \"w\") as file:", "            file.write(util.yaml_dump(configs))", "", "    @staticmethod", "    def __print_config(key, *, value=None, option=None,", "                       other_options=None, is_ignored=False, is_list=False):", "        parts = ['\\t']", "", "        if is_ignored:", "            parts.append((key, Color.gray))", "            return parts", "", "        parts.append(key)", "", "        if is_list:", "            parts.append(('[]', Color.gray))", "", "        parts.append((' = ', Color.gray))", "", "        if other_options is None:", "            other_options = []", "", "        if value is not None:", "            value_str = str(value)", "            value_str = value_str.replace('\\n', '')", "            if len(value_str) < 10:", "                parts.append((f\"{value_str}\", Color.cyan))", "            else:", "                parts.append((f\"{value_str[:10]}...\", Color.cyan))", "            parts.append('\\t')", "", "        if option is not None:", "            if len(other_options) == 0:", "                parts.append((option, Color.gray))", "            else:", "                parts.append((option, Color.orange))", "", "        if len(other_options) > 0:", "            parts.append(('\\t[', Color.gray))", "            for i, opt in enumerate(other_options):", "                if i > 0:", "                    parts.append((', ', Color.gray))", "                parts.append(opt)", "            parts.append((']', Color.gray))", "", "        return parts", "", "    def print(self):", "        order = self.calculator.topological_order.copy()", "        added = set(order)", "        ignored = set()", "", "        for k in self.parser.types:", "            if k not in added:", "                added.add(k)", "                order.append(k)", "                ignored.add(k)", "", "        for k in order:", "            computed = getattr(self.calculator.configs, k, None)", "", "            if k in ignored:", "                parts = self.__print_config(k, is_ignored=True)", "            elif k in self.parser.list_appends:", "                parts = self.__print_config(k,", "                                            value=computed,", "                                            is_list=True)", "            elif k in self.parser.options:", "                v = self.parser.values[k]", "                opts = self.parser.options[k]", "                lst = list(opts.keys())", "                if v in opts:", "                    lst.remove(v)", "                else:", "                    v = None", "", "                parts = self.__print_config(k,", "                                            value=computed,", "                                            option=v,", "                                            other_options=lst)", "            else:", "                parts = self.__print_config(k, value=computed)", "", "            logger.log_color(parts)"], "lab/configs/parser.py": ["from collections import OrderedDict", "from typing import List, Dict, Type, OrderedDict as OrderedDictType", "from typing import TYPE_CHECKING", "", "from .config_function import ConfigFunction", "", "if TYPE_CHECKING:", "    from . import Configs", "", "_CALCULATORS = '_calculators'", "", "", "def _get_base_classes(class_: Type['Configs']) -> List[Type['Configs']]:", "    classes = [class_]", "    level = [class_]", "    next_level = []", "", "    while len(level) > 0:", "        for c in level:", "            for b in c.__bases__:", "                if b == object:", "                    continue", "                next_level.append(b)", "        classes += next_level", "        level = next_level", "        next_level = []", "", "    classes.reverse()", "", "    return classes", "", "", "RESERVED = {'calc', 'list'}", "", "", "class Parser:", "    options: Dict[str, OrderedDictType[str, ConfigFunction]]", "    types: Dict[str, Type]", "    values: Dict[str, any]", "    list_appends: Dict[str, List[ConfigFunction]]", "", "    def __init__(self, configs: 'Configs', values: Dict[str, any] = None):", "        classes = _get_base_classes(type(configs))", "", "        self.values = {}", "        self.types = {}", "        self.options = {}", "        self.list_appends = {}", "        self.configs = configs", "", "        for c in classes:", "            for k, v in c.__annotations__.items():", "                self.__collect_annotation(k, v)", "", "            for k, v in c.__dict__.items():", "                self.__collect_value(k, v)", "", "        for c in classes:", "            if _CALCULATORS in c.__dict__:", "                for k, calcs in c.__dict__[_CALCULATORS].items():", "                    assert k in self.types, k", "                    for v in calcs:", "                        self.__collect_calculator(k, v)", "", "        for k, v in configs.__dict__.items():", "            assert k in self.types", "            self.__collect_value(k, v)", "", "        if values is not None:", "            for k, v in values.items():", "                assert k in self.types", "                self.__collect_value(k, v)", "", "        self.__calculate_missing_values()", "", "    @staticmethod", "    def is_valid(key):", "        if key.startswith('_'):", "            return False", "", "        if key in RESERVED:", "            return False", "", "        return True", "", "    def __collect_value(self, k, v):", "        if not self.is_valid(k):", "            return", "", "        self.values[k] = v", "        if k not in self.types:", "            self.types[k] = type(v)", "", "    def __collect_annotation(self, k, v):", "        if not self.is_valid(k):", "            return", "", "        self.types[k] = v", "", "    def __collect_calculator(self, k, v: ConfigFunction):", "        if v.is_append:", "            if k not in self.list_appends:", "                self.list_appends[k] = []", "            self.list_appends[k].append(v)", "        else:", "            if k not in self.options:", "                self.options[k] = OrderedDict()", "            self.options[k][v.option_name] = v", "", "    def __calculate_missing_values(self):", "        for k in self.types:", "            if k in self.values and self.values[k] is not None:", "                continue", "", "            if k in self.list_appends:", "                continue", "", "            if k in self.options:", "                self.values[k] = next(iter(self.options[k].keys()))", "                continue", "", "            if type(self.types[k]) == type:", "                self.options[k] = OrderedDict()", "                self.options[k][k] = ConfigFunction(self.types[k],", "                                                    config_names=k,", "                                                    option_name=k,", "                                                    is_append=False)", "                self.values[k] = k", "                continue", "", "            assert k in self.values, f\"Cannot compute {k}\""], "lab/configs/calculator.py": ["from typing import List, Dict, Type, Set, Optional, \\", "    OrderedDict as OrderedDictType, Union, Any, Tuple", "from typing import TYPE_CHECKING", "", "from .config_function import ConfigFunction", "", "if TYPE_CHECKING:", "    from . import Configs", "", "", "class Calculator:", "    options: Dict[str, OrderedDictType[str, ConfigFunction]]", "    types: Dict[str, Type]", "    values: Dict[str, any]", "    list_appends: Dict[str, List[ConfigFunction]]", "", "    configs: 'Configs'", "", "    dependencies: Dict[str, Set[str]]", "    topological_order: List[str]", "    stack: List[str]", "    visited: Set[str]", "    is_computed: Set[str]", "    is_top_sorted: Set[str]", "", "    def __init__(self, *,", "                 configs: 'Configs',", "                 options: Dict[str, OrderedDictType[str, ConfigFunction]],", "                 types: Dict[str, Type],", "                 values: Dict[str, any],", "                 list_appends: Dict[str, List[ConfigFunction]]):", "        self.configs = configs", "        self.options = options", "        self.types = types", "        self.values = values", "        self.list_appends = list_appends", "", "        self.visited = set()", "        self.stack = []", "        self.is_top_sorted = set()", "        self.topological_order = []", "        self.is_computed = set()", "", "    def __get_property(self, key) -> Tuple[Any, Union[None, ConfigFunction, List[ConfigFunction]]]:", "        if key in self.options:", "            value = self.values[key]", "            if value not in self.options[key]:", "                return value, None", "            return None, self.options[key][value]", "", "        if key in self.list_appends:", "            return None, [f for f in self.list_appends[key]]", "", "        return self.values[key], None", "", "    def __get_dependencies(self, key) -> Set[str]:", "        assert not (key in self.options and key in self.list_appends), \\", "            f\"{key} in options and appends\"", "", "        if key in self.options:", "            value = self.values[key]", "            if value not in self.options[key]:", "                return set()", "            return self.options[key][value].dependencies", "", "        if key in self.list_appends:", "            dep = set()", "            for func in self.list_appends[key]:", "                dep = dep.union(func.dependencies)", "", "            return dep", "", "        return set()", "", "    def __create_graph(self):", "        self.dependencies = {}", "        for k in self.types:", "            self.dependencies[k] = self.__get_dependencies(k)", "", "    def __add_to_topological_order(self, key):", "        assert self.stack.pop() == key", "        self.is_top_sorted.add(key)", "        self.topological_order.append(key)", "", "    def __traverse(self, key):", "        for d in self.dependencies[key]:", "            if d not in self.is_top_sorted:", "                self.__add_to_stack(d)", "                return", "", "        self.__add_to_topological_order(key)", "", "    def __add_to_stack(self, key):", "        if key in self.is_top_sorted:", "            return", "", "        assert key not in self.visited, f\"Cyclic dependency: {key}\"", "", "        self.visited.add(key)", "        self.stack.append(key)", "", "    def __dfs(self):", "        while len(self.stack) > 0:", "            key = self.stack[-1]", "            self.__traverse(key)", "", "    def __topological_sort(self, keys: List[str]):", "        for k in keys:", "            assert k not in self.is_top_sorted", "", "        for k in keys:", "            self.__add_to_stack(k)", "            self.__dfs()", "", "    def __set_configs(self, key, value):", "        assert key not in self.is_computed", "        self.is_computed.add(key)", "        self.configs.__setattr__(key, value)", "", "    def __compute(self, key):", "        if key in self.is_computed:", "            return", "", "        value, funcs = self.__get_property(key)", "        if value is not None:", "            self.__set_configs(key, value)", "        elif type(funcs) == list:", "            self.__set_configs(key, [f(self.configs) for f in funcs])", "        else:", "            value = funcs(self.configs)", "            if type(funcs.config_names) == str:", "                self.__set_configs(key, value)", "            else:", "                for i, k in enumerate(funcs.config_names):", "                    self.__set_configs(k, value[i])", "", "    def __compute_values(self):", "        for k in self.topological_order:", "            if k not in self.is_computed:", "                self.__compute(k)", "", "    def __call__(self, run_order: Optional[List[Union[List[str], str]]]):", "        if run_order is None:", "            run_order = [list(self.types.keys())]", "", "        for i in range(len(run_order)):", "            keys = run_order[i]", "            if type(keys) == str:", "                run_order[i] = [keys]", "", "        self.__create_graph()", "", "        self.visited = set()", "        self.stack = []", "        self.is_top_sorted = set()", "        self.topological_order = []", "        self.is_computed = set()", "", "        for keys in run_order:", "            self.__topological_sort(keys)", "            self.__compute_values()"], "lab/configs/sample.py": ["import inspect", "import typing", "", "from lab.configs import Configs, ConfigProcessor", "", "", "class SampleModel:", "    def __init__(self, c: 'Sample'):", "        self.w = c.workers_count", "", "", "class Sample(Configs):", "    total_global_steps: int = 10", "    workers_count: int = 10", "    # empty: str", "", "    x = 'string'", "", "    input_model: int", "    model: int", "", "    # get from type annotations", "    model_obj: SampleModel", "", "", "class SampleChild(Sample):", "    def __init__(self, *, test: int):", "        pass", "", "    new_attr = 2", "", "", "@Sample.calc()", "def input_model(c: Sample):", "    return c.workers_count * 2", "", "", "@Sample.calc('model')", "def simple_model(c: Sample):", "    return c.total_global_steps * 3", "", "", "# When collecting unordered items", "@Sample.list('steps')", "def remove_first():", "    return None", "", "", "@Sample.list('steps')", "def model_step(c: Sample):", "    return c.model", "", "", "configs = Sample()", "", "processor = ConfigProcessor(configs)", "processor.calculate()"], "lab/configs/config_function.py": ["import ast", "import inspect", "import textwrap", "import warnings", "from enum import Enum", "from typing import List, Callable, cast, Set, Union", "", "from typing import TYPE_CHECKING", "", "if TYPE_CHECKING:", "    from . import Configs", "", "", "class DependencyParser(ast.NodeVisitor):", "    def __init__(self, func: Callable):", "        if type(func) == type:", "            func = cast(object, func).__init__", "            spec: inspect.Signature = inspect.signature(func)", "            params = spec.parameters", "            assert len(params) == 2", "            param: inspect.Parameter = params[list(params.keys())[1]]", "            source = textwrap.dedent(inspect.getsource(func))", "", "        else:", "            spec: inspect.Signature = inspect.signature(func)", "            params = spec.parameters", "            assert len(params) == 1", "            param: inspect.Parameter = params[list(params.keys())[0]]", "            source = inspect.getsource(func)", "", "        assert (param.kind == param.POSITIONAL_ONLY or", "                param.kind == param.POSITIONAL_OR_KEYWORD)", "", "        self.arg_name = param.name", "", "        self.required = set()", "        self.is_referenced = False", "", "        parsed = ast.parse(source)", "        self.visit(parsed)", "", "    def visit_Attribute(self, node: ast.Attribute):", "        while not isinstance(node.value, ast.Name):", "            if not isinstance(node.value, ast.Attribute):", "                return", "", "            node = node.value", "", "        if node.value.id != self.arg_name:", "            return", "", "        self.required.add(node.attr)", "", "    # Only visits if not captured before", "    def visit_Name(self, node: ast.Name):", "        if node.id == self.arg_name:", "            self.is_referenced = True", "            print(f\"Referenced {node.id} in {node.lineno}:{node.col_offset}\")", "", "", "def _get_dependencies(func: Callable) -> Set[str]:", "    parser = DependencyParser(func)", "    assert not parser.is_referenced, f\"{func} should only use attributes of configs\"", "    return parser.required", "", "", "class FunctionKind(Enum):", "    pass_configs = 'pass_configs'", "    pass_parameters = 'pass_parameters'", "", "", "class ConfigFunction:", "    func: Callable", "    kind: FunctionKind", "    dependencies: Set[str]", "    config_names: Union[str, List[str]]", "    option_name: str", "    is_append: bool", "    params: List[inspect.Parameter]", "", "    def __get_type(self):", "        key, pos = 0, 0", "", "        for p in self.params:", "            if p.kind == p.POSITIONAL_OR_KEYWORD:", "                pos += 1", "            elif p.kind == p.KEYWORD_ONLY:", "                key += 1", "            else:", "                assert False, \"Only positional or keyword only arguments should be accepted\"", "", "        if pos == 1:", "            assert key == 0", "            return FunctionKind.pass_configs", "        else:", "            warnings.warn(\"Use configs object, because it's easier to refactor, find usage etc\",", "                          FutureWarning)", "            assert pos == 0", "            return FunctionKind.pass_parameters", "", "    def __get_dependencies(self):", "        if self.kind == FunctionKind.pass_configs:", "            parser = DependencyParser(self.func)", "            assert not parser.is_referenced, \\", "                f\"{self.func.__name__} should only use attributes of configs\"", "            return parser.required", "        else:", "            return {p.name for p in self.params}", "", "    def __get_option_name(self, option_name: str):", "        if option_name is not None:", "            return option_name", "        else:", "            return self.func.__name__", "", "    def __get_config_names(self, config_names: Union[str, List[str]]):", "        if config_names is None:", "            return self.func.__name__", "        elif type(config_names) == str:", "            return config_names", "        else:", "            assert type(config_names) == list", "            assert len(config_names) > 0", "            return config_names", "", "    def __get_params(self):", "        func_type = type(self.func)", "", "        if func_type == type:", "            init_func = cast(object, self.func).__init__", "            spec: inspect.Signature = inspect.signature(init_func)", "            params: List[inspect.Parameter] = list(spec.parameters.values())", "            assert len(params) > 0", "            assert params[0].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD", "            assert params[0].name == 'self'", "            return params[1:]", "        else:", "            spec: inspect.Signature = inspect.signature(self.func)", "            params: List[inspect.Parameter] = list(spec.parameters.values())", "            return params", "", "    def __init__(self, func, *,", "                 config_names: Union[str, List[str]],", "                 option_name: str,", "                 is_append: bool):", "        self.func = func", "        self.config_names = self.__get_config_names(config_names)", "        self.is_append = is_append", "        assert not (self.is_append and len(self.config_names) > 1)", "        self.option_name = self.__get_option_name(option_name)", "", "        self.params = self.__get_params()", "", "        self.kind = self.__get_type()", "        self.dependencies = self.__get_dependencies()", "", "    def __call__(self, configs: 'Configs'):", "        if self.kind == FunctionKind.pass_configs:", "            if len(self.params) == 1:", "                return self.func(configs)", "            else:", "                return self.func()", "        else:", "            kwargs = {p.name: configs.__getattribute__(p.name) for p in self.params}", "            return self.func(**kwargs)"], "lab/experiment/run.py": ["import time", "from pathlib import Path, PurePath", "from typing import List, Dict", "", "from lab import util", "", "", "def _struct_time_to_time(t: time.struct_time):", "    return f\"{t.tm_hour :02}:{t.tm_min :02}:{t.tm_sec :02}\"", "", "", "def _struct_time_to_date(t: time.struct_time):", "    return f\"{t.tm_year :04}-{t.tm_mon :02}-{t.tm_mday :02}\"", "", "", "_GLOBAL_STEP = 'global_step'", "", "", "class Run:", "    \"\"\"", "    # Trial \ud83c\udfc3\u200d", "", "    Every trial in an experiment has same configs.", "    It's just multiple runs.", "", "    A new trial will replace checkpoints and TensorBoard summaries", "    or previous trials, you should make a copy if needed.", "    The performance log in `trials.yaml` is not replaced.", "", "    You should run new trials after bug fixes or to see performance is", "     consistent.", "", "    If you want to try different configs, create multiple experiments.", "    \"\"\"", "", "    def __init__(self, *,", "                 python_file: str,", "                 trial_date: str,", "                 trial_time: str,", "                 comment: str,", "                 commit: str or None = None,", "                 commit_message: str or None = None,", "                 is_dirty: bool = True,", "                 diff: str or None = None,", "                 index: int,", "                 experiment_path: PurePath,", "                 start_step: int = 0):", "        self.index = index", "        self.commit = commit", "        self.is_dirty = is_dirty", "        self.diff = diff", "        self.python_file = python_file", "        self.trial_date = trial_date", "        self.trial_time = trial_time", "        self.comment = comment", "        self.commit_message = commit_message", "        self.start_step = start_step", "", "        self.experiment_path = experiment_path", "        self.run_path = experiment_path / str(index)", "        self.checkpoint_path = self.run_path / \"checkpoints\"", "        self.npy_path = self.run_path / \"npy\"", "", "        self.diff_path = self.run_path / \"source.diff\"", "", "        self.sqlite_path = self.run_path / \"sqlite.db\"", "        self.tensorboard_log_path = self.run_path / \"tensorboard\"", "", "        self.info_path = self.run_path / \"run.yaml\"", "        self.indicators_path = self.run_path / \"indicators.yaml\"", "        self.configs_path = self.run_path / \"configs.yaml\"", "", "    @classmethod", "    def create(cls, *,", "               experiment_path: PurePath,", "               python_file: str,", "               trial_time: time.struct_time,", "               comment: str):", "        \"\"\"", "        ## Create a new trial", "        \"\"\"", "        runs = [int(child.name) for child in Path(experiment_path).iterdir()]", "        runs.sort()", "", "        if len(runs) > 0:", "            this_run = runs[-1] + 1", "        else:", "            this_run = 1", "", "        return cls(python_file=python_file,", "                   trial_date=_struct_time_to_date(trial_time),", "                   trial_time=_struct_time_to_time(trial_time),", "                   index=this_run,", "                   experiment_path=experiment_path,", "                   comment=comment)", "", "    def __get_checkpoint(self, run: int):", "        run_path = self.experiment_path / str(run)", "        checkpoint_path = Path(run_path / \"checkpoints\")", "        if not checkpoint_path.exists():", "            return None", "", "        checkpoints = [int(child.name) for child in Path(checkpoint_path).iterdir()]", "        checkpoints.sort()", "        if len(checkpoints) == 0:", "            return None", "        else:", "            return checkpoints[-1]", "", "    def get_checkpoint(self, run: int, checkpoint: int):", "        if run is None:", "            run = -1", "        if checkpoint is None:", "            checkpoint = -1", "", "        if run == -1:", "            runs = [int(child.name) for child in Path(self.experiment_path).iterdir()]", "            runs.sort()", "", "            for r in reversed(runs):", "                if r == self.index:", "                    continue", "                checkpoint = self.__get_checkpoint(r)", "                if checkpoint is None:", "                    continue", "                run = r", "                break", "", "        if run == -1:", "            return None, None", "", "        if checkpoint == -1:", "            checkpoint = self.__get_checkpoint(run)", "", "        run_path = self.experiment_path / str(run)", "        checkpoint_path = run_path / \"checkpoints\"", "        return checkpoint_path / str(checkpoint), checkpoint", "", "    @classmethod", "    def from_dict(cls, data: Dict[str, any]):", "        \"\"\"", "        ## Create a new trial from a dictionary", "        \"\"\"", "        return cls(**data)", "", "    def to_dict(self):", "        \"\"\"", "        ## Convert trial to a dictionary for saving", "        \"\"\"", "        return dict(", "            python_file=self.python_file,", "            trial_date=self.trial_date,", "            trial_time=self.trial_time,", "            comment=self.comment,", "            commit=self.commit,", "            commit_message=self.commit_message,", "            is_dirty=self.is_dirty,", "            start_step=self.start_step", "        )", "", "    def pretty_print(self) -> List[str]:", "        \"\"\"", "        ## \ud83c\udfa8 Pretty print trial for the python file header", "        \"\"\"", "", "        # Trial information", "        commit_status = \"[dirty]\" if self.is_dirty else \"[clean]\"", "        res = [", "            f\"{self.trial_date} {self.trial_time}\",", "            self.comment,", "            f\"[{commit_status}]: {self.commit_message}\",", "            f\"start_step: {self.start_step}\"", "        ]", "", "        return res", "", "    def __str__(self):", "        return f\"Trial(comment=\\\"{self.comment}\\\",\" \\", "               f\" commit=\\\"{self.commit_message}\\\",\" \\", "               f\" date={self.trial_date}, time={self.trial_time}\"", "", "    def __repr__(self):", "        return self.__str__()", "", "    def save_info(self):", "        run_path = Path(self.run_path)", "        if not run_path.exists():", "            run_path.mkdir(parents=True)", "", "        with open(str(self.info_path), \"w\") as file:", "            file.write(util.yaml_dump(self.to_dict()))"], "lab/experiment/__init__.py": ["import pathlib", "import time", "from typing import Optional, List, Set, Dict, Union", "", "import git", "import numpy as np", "", "from lab import logger", "from lab.configs import Configs, ConfigProcessor", "from lab.experiment.run import Run", "from lab.lab import Lab", "from lab.logger import colors", "from lab.logger.writers import sqlite, tensorboard", "", "", "class Experiment:", "    \"\"\"", "    ## Experiment", "", "    Each experiment has different configurations or algorithms.", "    An experiment can have multiple trials.", "    \"\"\"", "    configs_processor: ConfigProcessor", "", "    # whether not to start the experiment if there are uncommitted changes.", "    check_repo_dirty: bool", "", "    def __init__(self, *,", "                 name: Optional[str],", "                 python_file: Optional[str],", "                 comment: Optional[str],", "                 writers: Set[str] = None):", "        \"\"\"", "        ### Create the experiment", "", "        :param name: name of the experiment", "        :param python_file: `__file__` that invokes this. This is stored in", "         the experiments list.", "        :param comment: a short description of the experiment", "", "        The experiments log keeps track of `python_file`, `name`, `comment` as", "         well as the git commit.", "", "        Experiment maintains the locations of checkpoints, logs, etc.", "        \"\"\"", "", "        if python_file is None:", "            python_file = self.__get_caller_file()", "", "        self.lab = Lab(python_file)", "", "        if name is None:", "            file_path = pathlib.PurePath(python_file)", "            name = file_path.stem", "", "        if comment is None:", "            comment = ''", "", "        self.name = name", "        self.experiment_path = self.lab.experiments / name", "", "        self.check_repo_dirty = self.lab.check_repo_dirty", "", "        self.configs_processor = None", "", "        experiment_path = pathlib.Path(self.experiment_path)", "        if not experiment_path.exists():", "            experiment_path.mkdir(parents=True)", "", "        self.run = Run.create(", "            experiment_path=self.experiment_path,", "            python_file=python_file,", "            trial_time=time.localtime(),", "            comment=comment)", "", "        repo = git.Repo(self.lab.path)", "", "        self.run.commit = repo.head.commit.hexsha", "        self.run.commit_message = repo.head.commit.message.strip()", "        self.run.is_dirty = repo.is_dirty()", "        self.run.diff = repo.git.diff()", "", "        checkpoint_saver = self._create_checkpoint_saver()", "        logger.internal().set_checkpoint_saver(checkpoint_saver)", "", "        if writers is None:", "            writers = {'sqlite', 'tensorboard'}", "", "        if 'sqlite' in writers:", "            logger.internal().add_writer(sqlite.Writer(self.run.sqlite_path))", "        if 'tensorboard' in writers:", "            logger.internal().add_writer(tensorboard.Writer(self.run.tensorboard_log_path))", "", "    @staticmethod", "    def __get_caller_file():", "        import inspect", "", "        frames: List[inspect.FrameInfo] = inspect.stack()", "        lab_src = pathlib.PurePath(__file__).parent.parent", "", "        for f in frames:", "            module_path = pathlib.PurePath(f.filename)", "            if str(module_path).startswith(str(lab_src)):", "                continue", "            return str(module_path)", "", "        return ''", "", "    def _create_checkpoint_saver(self):", "        return None", "", "    def __print_info_and_check_repo(self):", "        \"\"\"", "        ## \ud83d\udda8 Print the experiment info and check git repo status", "        \"\"\"", "        logger.log_color([", "            (self.name, colors.Style.bold),", "            ': ',", "            (str(self.run.index), colors.BrightColor.gray)", "        ])", "", "        if self.run.comment != '':", "            logger.log_color(['\\t', (self.run.comment, colors.BrightColor.cyan)])", "", "        logger.log_color([", "            \"\\t\"", "            \"[dirty]\" if self.run.is_dirty else \"[clean]\",", "            \": \",", "            (f\"\\\"{self.run.commit_message.strip()}\\\"\", colors.BrightColor.orange)", "        ])", "", "        # Exit if git repository is dirty", "        if self.check_repo_dirty and self.run.is_dirty:", "            logger.log(\"Cannot trial an experiment with uncommitted changes. \",", "                       new_line=False)", "            logger.log(\"[FAIL]\", color=colors.BrightColor.red)", "            exit(1)", "", "    def save_npy(self, array: np.ndarray, name: str):", "        \"\"\"", "        ## Save a single numpy array", "", "        This is used to save processed data", "        \"\"\"", "        npy_path = pathlib.Path(self.run.npy_path)", "        npy_path.mkdir(parents=True)", "        file_name = name + \".npy\"", "        np.save(str(self.run.npy_path / file_name), array)", "", "    def load_npy(self, name: str):", "        \"\"\"", "        ## Load a single numpy array", "", "        This is used to save processed data", "        \"\"\"", "        file_name = name + \".npy\"", "        return np.load(str(self.run.npy_path / file_name))", "", "    def _load_checkpoint(self, run: Optional[int], checkpoint: Optional[int]):", "        raise NotImplementedError()", "", "    def calc_configs(self,", "                     configs: Optional[Configs],", "                     configs_dict: Dict[str, any] = None,", "                     run_order: Optional[List[Union[List[str], str]]] = None):", "        if configs_dict is None:", "            configs_dict = {}", "        self.configs_processor = ConfigProcessor(configs, configs_dict)", "        self.configs_processor(run_order)", "", "    def start(self, *,", "              run: Optional[int] = None,", "              checkpoint: Optional[int] = None):", "        if run is not None:", "            with logger.section(\"Loading checkpoint\"):", "                global_step = self._load_checkpoint(run, checkpoint)", "                if global_step is None:", "                    logger.set_successful(False)", "                    global_step = 0", "        else:", "            global_step = 0", "", "        self.run.start_step = global_step", "        logger.internal().set_start_global_step(global_step)", "", "        self.__print_info_and_check_repo()", "        self.configs_processor.print()", "", "        self.run.save_info()", "", "        if self.configs_processor is not None:", "            self.configs_processor.save(self.run.configs_path)", "", "        logger.internal().save_indicators(self.run.indicators_path)", "", "        with open(str(self.run.diff_path), \"w\") as f:", "            f.write(self.run.diff)"], "lab/experiment/pytorch.py": ["import json", "import pathlib", "from typing import Optional, Dict, Set", "", "import numpy as np", "import torch.nn", "", "from lab import experiment", "from lab.logger.internal import CheckpointSaver", "", "", "class Checkpoint(CheckpointSaver):", "    __models: Dict[str, torch.nn.Module]", "", "    def __init__(self, path: pathlib.PurePath):", "        self.path = path", "        self.__models = {}", "", "    def add_models(self, models: Dict[str, torch.nn.Module]):", "        \"\"\"", "        ## Set variable for saving and loading", "        \"\"\"", "        self.__models.update(models)", "", "    def save(self, global_step):", "        \"\"\"", "        ## Save model as a set of numpy arrays", "        \"\"\"", "", "        checkpoints_path = pathlib.Path(self.path)", "        if not checkpoints_path.exists():", "            checkpoints_path.mkdir()", "", "        checkpoint_path = checkpoints_path / str(global_step)", "        assert not checkpoint_path.exists()", "", "        checkpoint_path.mkdir()", "", "        files = {}", "        for name, model in self.__models.items():", "            state: Dict[str, torch.Tensor] = model.state_dict()", "            files[name] = {}", "            for key, tensor in state.items():", "                if key == \"_metadata\":", "                    continue", "", "                file_name = f\"{name}_{key}.npy\"", "                files[name][key] = file_name", "", "                np.save(str(checkpoint_path / file_name), tensor.cpu().numpy())", "", "        # Save header", "        with open(str(checkpoint_path / \"info.json\"), \"w\") as f:", "            f.write(json.dumps(files))", "", "        # Delete old checkpoints", "        # for c in checkpoints_path.iterdir():", "        #     if c.name != checkpoint_path.name:", "        #         util.rm_tree(c)", "", "    def load(self, checkpoint_path):", "        \"\"\"", "        ## Load model as a set of numpy arrays", "        \"\"\"", "", "        with open(str(checkpoint_path / \"info.json\"), \"r\") as f:", "            files = json.loads(f.readline())", "", "        # Load each variable", "        for name, model in self.__models.items():", "            state: Dict[str, torch.Tensor] = model.state_dict()", "            for key, tensor in state.items():", "                file_name = files[name][key]", "                saved = np.load(str(checkpoint_path / file_name))", "                saved = torch.from_numpy(saved).to(tensor.device)", "                state[key] = saved", "", "            model.load_state_dict(state)", "", "        return True", "", "", "class Experiment(experiment.Experiment):", "    \"\"\"", "    ## Experiment", "", "    Each experiment has different configurations or algorithms.", "    An experiment can have multiple trials.", "    \"\"\"", "", "    __checkpoint_saver: Checkpoint", "", "    def __init__(self, *,", "                 name: Optional[str] = None,", "                 python_file: Optional[str] = None,", "                 comment: Optional[str] = None,", "                 writers: Set[str] = None):", "        \"\"\"", "        ### Create the experiment", "", "        :param name: name of the experiment", "        :param python_file: `__file__` that invokes this. This is stored in", "         the experiments list.", "        :param comment: a short description of the experiment", "", "        The experiments log keeps track of `python_file`, `name`, `comment` as", "         well as the git commit.", "", "        Experiment maintains the locations of checkpoints, logs, etc.", "        \"\"\"", "", "        super().__init__(name=name,", "                         python_file=python_file,", "                         comment=comment,", "                         writers=writers)", "", "    def _create_checkpoint_saver(self):", "        self.__checkpoint_saver = Checkpoint(self.run.checkpoint_path)", "        return self.__checkpoint_saver", "", "    def add_models(self, models: Dict[str, torch.nn.Module]):", "        \"\"\"", "        ## Set variable for saving and loading", "        \"\"\"", "        self.__checkpoint_saver.add_models(models)", "", "    def _load_checkpoint(self, run: Optional[int], checkpoint: Optional[int]):", "        checkpoint_path, checkpoint = self.run.get_checkpoint(run, checkpoint)", "        if checkpoint_path is None:", "            return None", "", "        is_successful = self.__checkpoint_saver.load(checkpoint_path)", "        if not is_successful:", "            return None", "        return checkpoint"], "lab/lab.py": ["from pathlib import PurePath, Path", "from typing import List", "", "from . import util", "", "_CONFIG_FILE_NAME = '.lab.yaml'", "", "", "class Lab:", "    \"\"\"", "    ### Lab", "", "    Lab contains the lab specific properties.", "    \"\"\"", "", "    def __init__(self, path: str):", "        configs = self.__get_config_files(path)", "", "        if len(configs) == 0:", "            raise RuntimeError(\"No '.lab.yaml' config file found.\")", "", "        config = self.__get_config(configs)", "", "        self.path = PurePath(config['path'])", "        self.check_repo_dirty = config['check_repo_dirty']", "", "    @staticmethod", "    def __get_config(configs):", "        config = dict(", "            path=None,", "            check_repo_dirty=True,", "            is_log_python_file=True,", "            config_file_path=None", "        )", "", "        for c in reversed(configs):", "            if 'path' in c:", "                c['path'] = c['config_file_path'] / c['path']", "            elif config['path'] is None:", "                c['path'] = c['config_file_path']", "", "            for k, v in c.items():", "                if k not in config:", "                    raise RuntimeError(f\"Unknown config parameter #{k} in file \"", "                                       f\"{c['config_file_path'] / _CONFIG_FILE_NAME}\")", "                else:", "                    config[k] = v", "", "        return config", "", "    @staticmethod", "    def __get_config_files(path: str):", "        path = Path(path).resolve()", "        configs = []", "", "        while path.exists():", "            if path.is_dir():", "                config_file = path / _CONFIG_FILE_NAME", "                if config_file.is_file():", "                    with open(str(config_file)) as f:", "                        config = util.yaml_load(f.read())", "                        if config is None:", "                            config = {}", "                        config['config_file_path'] = path", "                        configs.append(config)", "", "            if str(path) == path.root:", "                break", "", "            path = path.parent", "", "        return configs", "", "    @property", "    def experiments(self) -> PurePath:", "        \"\"\"", "        ### Experiments path", "        \"\"\"", "        return self.path / \"logs\"", "", "    def get_experiments(self) -> List[Path]:", "        \"\"\"", "        Get list of experiments", "        \"\"\"", "        experiments_path = Path(self.experiments)", "        return [child for child in experiments_path.iterdir()]"], "samples/tutorial/a_logger.py": ["import time", "", "from lab import logger, IndicatorOptions, IndicatorType", "", "", "def loop():", "    logger.info(a=2, b=1)", "", "    logger.add_indicator('loss_ma', IndicatorType.queue, IndicatorOptions(queue_size=10))", "    for i in range(10):", "        logger.add_global_step(1)", "        logger.store(loss=100 / (i + 1), loss_ma=100 / (i + 1))", "        logger.write()", "        if (i + 1) % 2 == 0:", "            logger.new_line()", "", "        time.sleep(2)", "", "", "if __name__ == '__main__':", "    loop()"], "samples/mnist_loop.py": ["from typing import Dict", "", "import torch", "import torch.nn as nn", "import torch.nn.functional as F", "import torch.optim as optim", "import torch.utils.data", "from torchvision import datasets, transforms", "", "from lab import logger, configs, IndicatorOptions, IndicatorType", "from lab import training_loop", "from lab.experiment.pytorch import Experiment", "from lab.logger import util as logger_util", "", "", "class Net(nn.Module):", "    def __init__(self):", "        super().__init__()", "        self.conv1 = nn.Conv2d(1, 20, 5, 1)", "        self.conv2 = nn.Conv2d(20, 50, 5, 1)", "        self.fc1 = nn.Linear(4 * 4 * 50, 500)", "        self.fc2 = nn.Linear(500, 10)", "", "    def forward(self, x):", "        x = F.relu(self.conv1(x))", "        x = F.max_pool2d(x, 2, 2)", "        x = F.relu(self.conv2(x))", "        x = F.max_pool2d(x, 2, 2)", "        x = x.view(-1, 4 * 4 * 50)", "        x = F.relu(self.fc1(x))", "        x = self.fc2(x)", "        return F.log_softmax(x, dim=1)", "", "", "class MNIST:", "    def __init__(self, c: 'Configs'):", "        self.model = c.model", "        self.device = c.device", "        self.train_loader = c.train_loader", "        self.test_loader = c.test_loader", "        self.optimizer = c.optimizer", "        self.train_log_interval = c.train_log_interval", "        self.loop = c.training_loop", "        self.__is_log_parameters = c.is_log_parameters", "", "    def _train(self):", "        self.model.train()", "        for i, (data, target) in logger.enumerator(\"Train\", self.train_loader):", "            data, target = data.to(self.device), target.to(self.device)", "            self.optimizer.zero_grad()", "            output = self.model(data)", "            loss = F.nll_loss(output, target)", "            loss.backward()", "            self.optimizer.step()", "", "            # Add training loss to the logger.", "            # The logger will queue the values and output the mean", "            logger.store(train_loss=loss.item())", "            logger.add_global_step()", "", "            # Print output to the console", "            if i % self.train_log_interval == 0:", "                # Output the indicators", "                logger.write()", "", "    def _test(self):", "        self.model.eval()", "        test_loss = 0", "        correct = 0", "        with torch.no_grad():", "            for data, target in logger.iterator(\"Test\", self.test_loader):", "                data, target = data.to(self.device), target.to(self.device)", "                output = self.model(data)", "                test_loss += F.nll_loss(output, target, reduction='sum').item()", "                pred = output.argmax(dim=1, keepdim=True)", "                correct += pred.eq(target.view_as(pred)).sum().item()", "", "        # Add test loss and accuracy to logger", "        logger.store(test_loss=test_loss / len(self.test_loader.dataset))", "        logger.store(accuracy=correct / len(self.test_loader.dataset))", "", "    def __log_model_params(self):", "        if not self.__is_log_parameters:", "            return", "", "        # Add histograms with model parameter values and gradients", "        logger_util.store_model_indicators(self.model)", "", "    def __call__(self):", "        # Training and testing", "        logger_util.add_model_indicators(self.model)", "", "        logger.add_indicator(\"train_loss\", IndicatorType.queue,", "                             IndicatorOptions(queue_size=20, is_print=True))", "        logger.add_indicator(\"test_loss\", IndicatorType.histogram,", "                             IndicatorOptions(is_print=True))", "        logger.add_indicator(\"accuracy\", IndicatorType.histogram,", "                             IndicatorOptions(is_print=True))", "", "        for _ in self.loop:", "            self._train()", "            self._test()", "            self.__log_model_params()", "", "", "class LoaderConfigs(configs.Configs):", "    train_loader: torch.utils.data.DataLoader", "    test_loader: torch.utils.data.DataLoader", "", "", "class Configs(training_loop.TrainingLoopConfigs, LoaderConfigs):", "    epochs: int", "", "    is_save_models = True", "    batch_size: int = 64", "    test_batch_size: int = 1000", "", "    # Reset epochs so that it'll be computed", "    use_cuda: float = True", "    cuda_device: str = \"cuda:1\"", "    seed: int = 5", "    train_log_interval: int = 10", "", "    is_log_parameters: bool = True", "", "    main: MNIST", "", "    is_cuda: bool", "", "    device: any", "", "    data_loader_args: Dict", "", "    model: nn.Module", "", "    learning_rate: float = 0.01", "    momentum: float = 0.5", "    optimizer: optim.SGD", "", "    set_seed = None", "", "", "@Configs.calc('loop_count')", "def from_batch(c: Configs):", "    return c.epochs", "", "", "@Configs.calc('epochs')", "def from_batch(c: Configs):", "    return 2 * c.batch_size", "", "", "@Configs.calc('epochs')", "def random(c: Configs):", "    return c.seed", "", "", "# Get dependencies from parameters.", "# The code looks cleaner, but might cause problems when you want to refactor", "# later.", "# It will be harder to use static analysis tools to find the usage of configs.", "@Configs.calc(['is_cuda', 'device', 'data_loader_args'])", "def cuda(*, use_cuda, cuda_device):", "    is_cuda = use_cuda and torch.cuda.is_available()", "    device = torch.device(cuda_device if is_cuda else \"cpu\")", "    dl_args = {'num_workers': 1, 'pin_memory': True} if is_cuda else {}", "    return is_cuda, device, dl_args", "", "", "def _data_loader(is_train, batch_size, data_loader_args):", "    return torch.utils.data.DataLoader(", "        datasets.MNIST('./data', train=is_train, download=True,", "                       transform=transforms.Compose([", "                           transforms.ToTensor(),", "                           transforms.Normalize((0.1307,), (0.3081,))", "                       ])),", "        batch_size=batch_size, shuffle=True, **data_loader_args)", "", "", "# The value name is inferred from the function name", "@Configs.calc()", "def train_loader(c: Configs):", "    with logger.section(\"Training data\"):", "        return _data_loader(True, c.batch_size, c.data_loader_args)", "", "", "@Configs.calc()", "def test_loader(c: Configs):", "    with logger.section(\"Testing data\"):", "        return _data_loader(False, c.test_batch_size, c.data_loader_args)", "", "", "# Compute multiple results from a single function", "@Configs.calc(['model', 'optimizer'])", "def model_optimizer(c: Configs):", "    with logger.section(\"Create model\"):", "        m: Net = Net()", "        m.to(c.device)", "", "    with logger.section(\"Create optimizer\"):", "        o = optim.SGD(m.parameters(), lr=c.learning_rate, momentum=c.momentum)", "", "    return m, o", "", "", "@Configs.calc()", "def set_seed(c: Configs):", "    with logger.section(\"Setting seed\"):", "        torch.manual_seed(c.seed)", "", "", "def main():", "    conf = Configs()", "    experiment = Experiment(writers={'sqlite', 'tensorboard'})", "    experiment.calc_configs(conf,", "                            {'epochs': 'random'},", "                            ['set_seed', 'main'])", "    experiment.add_models(dict(model=conf.model))", "    experiment.start()", "    conf.main()", "", "", "if __name__ == '__main__':", "    main()"], "samples/mnist_configs.py": ["from typing import Dict", "", "import torch", "import torch.nn as nn", "import torch.nn.functional as F", "import torch.optim as optim", "import torch.utils.data", "from torchvision import datasets, transforms", "", "from lab import logger, configs, IndicatorOptions, IndicatorType", "from lab.experiment.pytorch import Experiment", "from lab.logger import util as logger_util", "", "", "class Net(nn.Module):", "    def __init__(self):", "        super().__init__()", "        self.conv1 = nn.Conv2d(1, 20, 5, 1)", "        self.conv2 = nn.Conv2d(20, 50, 5, 1)", "        self.fc1 = nn.Linear(4 * 4 * 50, 500)", "        self.fc2 = nn.Linear(500, 10)", "", "    def forward(self, x):", "        x = F.relu(self.conv1(x))", "        x = F.max_pool2d(x, 2, 2)", "        x = F.relu(self.conv2(x))", "        x = F.max_pool2d(x, 2, 2)", "        x = x.view(-1, 4 * 4 * 50)", "        x = F.relu(self.fc1(x))", "        x = self.fc2(x)", "        return F.log_softmax(x, dim=1)", "", "", "class MNISTLoop:", "    def __init__(self, c: 'Configs'):", "        self.model = c.model", "        self.device = c.device", "        self.train_loader = c.train_loader", "        self.test_loader = c.test_loader", "        self.optimizer = c.optimizer", "        self.log_interval = c.log_interval", "        self.__epochs = c.epochs", "        self.__is_save_models = c.is_save_models", "        self.__is_log_parameters = c.is_log_parameters", "        self.__log_new_line_interval = c.log_new_line_interval", "", "    def startup(self):", "        logger_util.add_model_indicators(self.model)", "", "        logger.add_indicator(\"train_loss\", IndicatorType.queue,", "                             IndicatorOptions(queue_size=20, is_print=True))", "        logger.add_indicator(\"test_loss\", IndicatorType.histogram,", "                             IndicatorOptions(is_print=True))", "        logger.add_indicator(\"accuracy\", IndicatorType.histogram,", "                             IndicatorOptions(is_print=True))", "", "    def _train(self):", "        self.model.train()", "        for i, (data, target) in logger.enumerator(\"Train\", self.train_loader):", "            data, target = data.to(self.device), target.to(self.device)", "            self.optimizer.zero_grad()", "            output = self.model(data)", "            loss = F.nll_loss(output, target)", "            loss.backward()", "            self.optimizer.step()", "", "            # Add training loss to the logger.", "            # The logger will queue the values and output the mean", "            logger.store(train_loss=loss.item())", "            logger.add_global_step()", "", "            # Print output to the console", "            if i % self.log_interval == 0:", "                # Output the indicators", "                logger.write()", "", "    def _test(self):", "        self.model.eval()", "        test_loss = 0", "        correct = 0", "        with torch.no_grad():", "            for data, target in logger.iterator(\"Test\", self.test_loader):", "                data, target = data.to(self.device), target.to(self.device)", "                output = self.model(data)", "                test_loss += F.nll_loss(output, target, reduction='sum').item()", "                pred = output.argmax(dim=1, keepdim=True)", "                correct += pred.eq(target.view_as(pred)).sum().item()", "", "        # Add test loss and accuracy to logger", "        logger.store(test_loss=test_loss / len(self.test_loader.dataset))", "        logger.store(accuracy=correct / len(self.test_loader.dataset))", "", "    def __log_model_params(self):", "        if not self.__is_log_parameters:", "            return", "", "        # Add histograms with model parameter values and gradients", "        logger_util.store_model_indicators(self.model)", "", "    def loop(self):", "        # Loop through the monitored iterator", "        for epoch in logger.loop(range(0, self.__epochs)):", "            self._train()", "            self._test()", "", "            self.__log_model_params()", "", "            # Clear line and output to console", "            logger.write()", "", "            # Clear line and go to the next line;", "            # that is, we add a new line to the output", "            # at the end of each epoch", "            if (epoch + 1) % self.__log_new_line_interval == 0:", "                logger.new_line()", "", "            if self.__is_save_models:", "                logger.save_checkpoint()", "", "    def __call__(self):", "        self.startup()", "        self.loop()", "", "", "class LoopConfigs(configs.Configs):", "    epochs: int = 10", "    is_save_models: bool = False", "    is_log_parameters: bool = True", "    log_new_line_interval: int = 1", "", "", "class LoaderConfigs(configs.Configs):", "    train_loader: torch.utils.data.DataLoader", "    test_loader: torch.utils.data.DataLoader", "", "", "class Configs(LoopConfigs, LoaderConfigs):", "    batch_size: int = 64", "    test_batch_size: int = 1000", "", "    # Reset epochs so that it'll be computed", "    epochs: int = None", "    use_cuda: float = True", "    cuda_device: str = \"cuda:1\"", "    seed: int = 5", "    log_interval: int = 10", "", "    loop: MNISTLoop", "", "    device: any", "", "    data_loader_args: Dict", "", "    model: nn.Module", "", "    learning_rate: float = 0.01", "    momentum: float = 0.5", "    optimizer: optim.SGD", "", "    set_seed = None", "", "    not_used: bool = 10", "", "", "@Configs.calc('epochs')", "def from_batch(c: Configs):", "    return 2 * c.batch_size", "", "", "@Configs.calc('epochs')", "def random(c: Configs):", "    return c.seed", "", "", "# Get dependencies from parameters.", "# The code looks cleaner, but might cause problems when you want to refactor", "# later.", "# It will be harder to use static analysis tools to find the usage of configs.", "@Configs.calc(['device', 'data_loader_args'])", "def cuda(*, use_cuda, cuda_device):", "    is_cuda = use_cuda and torch.cuda.is_available()", "    device = torch.device(cuda_device if is_cuda else \"cpu\")", "    dl_args = {'num_workers': 1, 'pin_memory': True} if is_cuda else {}", "    return device, dl_args", "", "", "def _data_loader(is_train, batch_size, data_loader_args):", "    return torch.utils.data.DataLoader(", "        datasets.MNIST('./data', train=is_train, download=True,", "                       transform=transforms.Compose([", "                           transforms.ToTensor(),", "                           transforms.Normalize((0.1307,), (0.3081,))", "                       ])),", "        batch_size=batch_size, shuffle=True, **data_loader_args)", "", "", "# The value name is inferred from the function name", "@Configs.calc()", "def train_loader(c: Configs):", "    with logger.section(\"Training data\"):", "        return _data_loader(True, c.batch_size, c.data_loader_args)", "", "", "@Configs.calc()", "def test_loader(c: Configs):", "    with logger.section(\"Testing data\"):", "        return _data_loader(False, c.test_batch_size, c.data_loader_args)", "", "", "# Compute multiple results from a single function", "@Configs.calc(['model', 'optimizer'])", "def model_optimizer(c: Configs):", "    with logger.section(\"Create model\"):", "        m: Net = Net()", "        m.to(c.device)", "", "    with logger.section(\"Create optimizer\"):", "        o = optim.SGD(m.parameters(), lr=c.learning_rate, momentum=c.momentum)", "", "    return m, o", "", "", "@Configs.calc()", "def set_seed(c: Configs):", "    with logger.section(\"Setting seed\"):", "        torch.manual_seed(c.seed)", "", "", "def main():", "    conf = Configs()", "    experiment = Experiment(writers={'sqlite'})", "    experiment.calc_configs(conf,", "                            {'epochs': 'random'},", "                            ['set_seed', 'loop'])", "    experiment.add_models(dict(model=conf.model))", "    experiment.start(run=-1)", "    conf.loop()", "", "", "if __name__ == '__main__':", "    main()"], "setup.py": ["import setuptools", "", "with open(\"README.md\", \"r\") as f:", "    long_description = f.read()", "", "print(setuptools.find_packages())", "", "setuptools.setup(", "    name='lab',", "    version='3.0',", "    author=\"Varuna Jayasiri\",", "    author_email=\"vpjayasiri@gmail.com\",", "    description=\"\ud83e\uddea Organize Machine Learning Experiments\",", "    long_description=long_description,", "    long_description_content_type=\"text/markdown\",", "    url=\"https://github.com/vpj/lab\",", "    packages=setuptools.find_packages(exclude=('samples', 'samples.*')),", "    install_requires=['gitpython',", "                      'pyyaml',", "                      'numpy'],", "    classifiers=[", "        \"Programming Language :: Python :: 3\",", "        \"License :: OSI Approved :: MIT License\",", "        'Intended Audience :: Developers',", "        'Intended Audience :: Science/Research',", "        'Topic :: Scientific/Engineering',", "        'Topic :: Scientific/Engineering :: Mathematics',", "        'Topic :: Scientific/Engineering :: Artificial Intelligence',", "        'Topic :: Software Development',", "        'Topic :: Software Development :: Libraries',", "        'Topic :: Software Development :: Libraries :: Python Modules',", "    ],", "    keywords='machine learning',", ")"]}
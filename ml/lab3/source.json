{
  "lab/__init__.py": [
    "import lab.logger as logger",
    ""
  ],
  "lab/configs/__init__.py": [
    "from pathlib import PurePath",
    "from typing import List, Dict, Callable, Optional, \\",
    "    Union",
    "",
    "from lab import util, logger",
    "from .calculator import Calculator",
    "from .config_function import ConfigFunction",
    "from .config_item import ConfigItem",
    "from .config_item import ConfigItem",
    "from .parser import Parser",
    "from ..logger.colors import Text",
    "",
    "_CALCULATORS = '_calculators'",
    "_CONFIG_PRINT_LEN = 40",
    "",
    "",
    "class Configs:",
    "    _calculators: Dict[str, List[ConfigFunction]] = {}",
    "",
    "    def __init_subclass__(cls, **kwargs):",
    "        configs = {}",
    "",
    "        for k, v in cls.__annotations__.items():",
    "            if not Parser.is_valid(k):",
    "                continue",
    "",
    "            configs[k] = ConfigItem(k,",
    "                                    True, v,",
    "                                    k in cls.__dict__, cls.__dict__.get(k, None))",
    "",
    "        for k, v in cls.__dict__.items():",
    "            if not Parser.is_valid(k):",
    "                continue",
    "",
    "            configs[k] = ConfigItem(k,",
    "                                    k in cls.__annotations__, cls.__annotations__.get(k, None),",
    "                                    True, v)",
    "",
    "        for k, v in configs.items():",
    "            setattr(cls, k, v)",
    "",
    "    @classmethod",
    "    def calc(cls, name: Union[ConfigItem, str, List[ConfigItem], List[str]] = None,",
    "             option: str = None, *,",
    "             is_append: bool = False):",
    "        if _CALCULATORS not in cls.__dict__:",
    "            cls._calculators = {}",
    "",
    "        def wrapper(func: Callable):",
    "            calc = ConfigFunction(func, config_names=name, option_name=option, is_append=is_append)",
    "            if type(calc.config_names) == str:",
    "                config_names = [calc.config_names]",
    "            else:",
    "                config_names = calc.config_names",
    "",
    "            for n in config_names:",
    "                if n not in cls._calculators:",
    "                    cls._calculators[n] = []",
    "                cls._calculators[n].append(calc)",
    "",
    "            return func",
    "",
    "        return wrapper",
    "",
    "    @classmethod",
    "    def list(cls, name: str = None):",
    "        return cls.calc(name, f\"_{util.random_string()}\", is_append=True)",
    "",
    "",
    "class ConfigProcessor:",
    "    def __init__(self, configs, values: Dict[str, any] = None):",
    "        self.parser = Parser(configs, values)",
    "        self.calculator = Calculator(configs=configs,",
    "                                     options=self.parser.options,",
    "                                     types=self.parser.types,",
    "                                     values=self.parser.values,",
    "                                     list_appends=self.parser.list_appends)",
    "",
    "    def __call__(self, run_order: Optional[List[Union[List[str], str]]] = None):",
    "        self.calculator(run_order)",
    "",
    "    @staticmethod",
    "    def __is_primitive(value):",
    "        if value is None:",
    "            return True",
    "",
    "        if type(value) == str:",
    "            return True",
    "",
    "        if type(value) == int:",
    "            return True",
    "",
    "        if type(value) == bool:",
    "            return True",
    "",
    "        if type(value) == list and all([ConfigProcessor.__is_primitive(v) for v in value]):",
    "            return True",
    "",
    "        if type(value) == dict and all([ConfigProcessor.__is_primitive(v) for v in value.values()]):",
    "            return True",
    "",
    "        return False",
    "",
    "    @staticmethod",
    "    def __to_yaml(value):",
    "        if ConfigProcessor.__is_primitive(value):",
    "            return value",
    "        else:",
    "            return str(value)",
    "",
    "    def save(self, configs_path: PurePath):",
    "        orders = {k: i for i, k in enumerate(self.calculator.topological_order)}",
    "        configs = {}",
    "        for k, v in self.parser.types.items():",
    "            configs[k] = {",
    "                'name': k,",
    "                'type': str(v),",
    "                'value': self.__to_yaml(self.parser.values.get(k, None)),",
    "                'order': orders.get(k, -1),",
    "                'options': list(self.parser.options.get(k, {}).keys()),",
    "                'computed': self.__to_yaml(getattr(self.calculator.configs, k, None))",
    "            }",
    "",
    "        with open(str(configs_path), \"w\") as file:",
    "            file.write(util.yaml_dump(configs))",
    "",
    "    @staticmethod",
    "    def __print_config(key, *, value=None, option=None,",
    "                       other_options=None, is_ignored=False, is_list=False):",
    "        parts = ['\\t']",
    "",
    "        if is_ignored:",
    "            parts.append((key, Text.subtle))",
    "            return parts",
    "",
    "        parts.append((key, Text.key))",
    "",
    "        if is_list:",
    "            parts.append(('[]', Text.subtle))",
    "",
    "        parts.append((' = ', Text.subtle))",
    "",
    "        if other_options is None:",
    "            other_options = []",
    "",
    "        if value is not None:",
    "            value_str = str(value)",
    "            value_str = value_str.replace('\\n', '')",
    "            if len(value_str) < _CONFIG_PRINT_LEN:",
    "                parts.append((f\"{value_str}\", Text.value))",
    "            else:",
    "                parts.append((f\"{value_str[:_CONFIG_PRINT_LEN]}...\", Text.value))",
    "            parts.append('\\t')",
    "",
    "        if option is not None:",
    "            if len(other_options) == 0:",
    "                parts.append((option, Text.subtle))",
    "            else:",
    "                parts.append((option, Text.none))",
    "",
    "        if len(other_options) > 0:",
    "            parts.append(('\\t[', Text.subtle))",
    "            for i, opt in enumerate(other_options):",
    "                if i > 0:",
    "                    parts.append((', ', Text.subtle))",
    "                parts.append(opt)",
    "            parts.append((']', Text.subtle))",
    "",
    "        return parts",
    "",
    "    def print(self):",
    "        order = self.calculator.topological_order.copy()",
    "        added = set(order)",
    "        ignored = set()",
    "",
    "        for k in self.parser.types:",
    "            if k not in added:",
    "                added.add(k)",
    "                order.append(k)",
    "                ignored.add(k)",
    "",
    "        logger.log(\"Configs:\", Text.heading)",
    "",
    "        for k in order:",
    "            computed = getattr(self.calculator.configs, k, None)",
    "",
    "            if k in ignored:",
    "                parts = self.__print_config(k, is_ignored=True)",
    "            elif k in self.parser.list_appends:",
    "                parts = self.__print_config(k,",
    "                                            value=computed,",
    "                                            is_list=True)",
    "            elif k in self.parser.options:",
    "                v = self.parser.values[k]",
    "                opts = self.parser.options[k]",
    "                lst = list(opts.keys())",
    "                if v in opts:",
    "                    lst.remove(v)",
    "                else:",
    "                    v = None",
    "",
    "                parts = self.__print_config(k,",
    "                                            value=computed,",
    "                                            option=v,",
    "                                            other_options=lst)",
    "            else:",
    "                parts = self.__print_config(k, value=computed)",
    "",
    "            logger.log(parts)",
    "",
    "        logger.new_line()",
    ""
  ],
  "lab/configs/calculator.py": [
    "from typing import List, Dict, Type, Set, Optional, \\",
    "    OrderedDict as OrderedDictType, Union, Any, Tuple",
    "from typing import TYPE_CHECKING",
    "",
    "from .config_function import ConfigFunction",
    "from .. import logger",
    "",
    "if TYPE_CHECKING:",
    "    from . import Configs",
    "",
    "",
    "class Calculator:",
    "    options: Dict[str, OrderedDictType[str, ConfigFunction]]",
    "    types: Dict[str, Type]",
    "    values: Dict[str, any]",
    "    list_appends: Dict[str, List[ConfigFunction]]",
    "",
    "    configs: 'Configs'",
    "",
    "    dependencies: Dict[str, Set[str]]",
    "    topological_order: List[str]",
    "    stack: List[str]",
    "    visited: Set[str]",
    "    is_computed: Set[str]",
    "    is_top_sorted: Set[str]",
    "",
    "    def __init__(self, *,",
    "                 configs: 'Configs',",
    "                 options: Dict[str, OrderedDictType[str, ConfigFunction]],",
    "                 types: Dict[str, Type],",
    "                 values: Dict[str, any],",
    "                 list_appends: Dict[str, List[ConfigFunction]]):",
    "        self.configs = configs",
    "        self.options = options",
    "        self.types = types",
    "        self.values = values",
    "        self.list_appends = list_appends",
    "",
    "        self.visited = set()",
    "        self.stack = []",
    "        self.is_top_sorted = set()",
    "        self.topological_order = []",
    "        self.is_computed = set()",
    "",
    "    def __get_property(self, key) -> Tuple[Any, Union[None, ConfigFunction, List[ConfigFunction]]]:",
    "        if key in self.options:",
    "            value = self.values[key]",
    "            if value not in self.options[key]:",
    "                return value, None",
    "            return None, self.options[key][value]",
    "",
    "        if key in self.list_appends:",
    "            return None, [f for f in self.list_appends[key]]",
    "",
    "        return self.values[key], None",
    "",
    "    def __get_dependencies(self, key) -> Set[str]:",
    "        assert not (key in self.options and key in self.list_appends), \\",
    "            f\"{key} in options and appends\"",
    "",
    "        if key in self.options:",
    "            value = self.values[key]",
    "            if value not in self.options[key]:",
    "                return set()",
    "            return self.options[key][value].dependencies",
    "",
    "        if key in self.list_appends:",
    "            dep = set()",
    "            for func in self.list_appends[key]:",
    "                dep = dep.union(func.dependencies)",
    "",
    "            return dep",
    "",
    "        assert key in self.values, f\"Cannot compute {key}\"",
    "        assert self.values[key] is not None, f\"Cannot compute {key}\"",
    "",
    "        return set()",
    "",
    "    def __create_graph(self):",
    "        self.dependencies = {}",
    "        for k in self.types:",
    "            self.dependencies[k] = self.__get_dependencies(k)",
    "",
    "    def __add_to_topological_order(self, key):",
    "        assert self.stack.pop() == key",
    "        self.is_top_sorted.add(key)",
    "        self.topological_order.append(key)",
    "",
    "    def __traverse(self, key):",
    "        for d in self.dependencies[key]:",
    "            if d not in self.is_top_sorted:",
    "                self.__add_to_stack(d)",
    "                return",
    "",
    "        self.__add_to_topological_order(key)",
    "",
    "    def __add_to_stack(self, key):",
    "        if key in self.is_top_sorted:",
    "            return",
    "",
    "        assert key not in self.visited, f\"Cyclic dependency: {key}\"",
    "",
    "        self.visited.add(key)",
    "        self.stack.append(key)",
    "",
    "    def __dfs(self):",
    "        while len(self.stack) > 0:",
    "            key = self.stack[-1]",
    "            self.__traverse(key)",
    "",
    "    def __topological_sort(self, keys: List[str]):",
    "        for k in keys:",
    "            assert k not in self.is_top_sorted",
    "",
    "        for k in keys:",
    "            self.__add_to_stack(k)",
    "            self.__dfs()",
    "",
    "    def __set_configs(self, key, value):",
    "        assert key not in self.is_computed",
    "        self.is_computed.add(key)",
    "        self.configs.__setattr__(key, value)",
    "",
    "    def __compute(self, key):",
    "        if key in self.is_computed:",
    "            return",
    "",
    "        value, funcs = self.__get_property(key)",
    "        if funcs is None:",
    "            self.__set_configs(key, value)",
    "        elif type(funcs) == list:",
    "            self.__set_configs(key, [f(self.configs) for f in funcs])",
    "        else:",
    "            s = logger.section(f'Prepare {key}', is_new_line=False)",
    "            with s:",
    "                value = funcs(self.configs)",
    "            if s.get_estimated_time() >= 0.01:",
    "                logger.new_line()",
    "            else:",
    "                logger.log(' ' * 100, is_new_line=False)",
    "",
    "            if type(funcs.config_names) == str:",
    "                self.__set_configs(key, value)",
    "            else:",
    "                for i, k in enumerate(funcs.config_names):",
    "                    self.__set_configs(k, value[i])",
    "",
    "    def __compute_values(self):",
    "        for k in self.topological_order:",
    "            if k not in self.is_computed:",
    "                self.__compute(k)",
    "",
    "    def __call__(self, run_order: Optional[List[Union[List[str], str]]]):",
    "        if run_order is None:",
    "            run_order = [list(self.types.keys())]",
    "",
    "        for i in range(len(run_order)):",
    "            keys = run_order[i]",
    "            if type(keys) == str:",
    "                run_order[i] = [keys]",
    "",
    "        s = logger.section('Calculate config dependencies', is_new_line=False)",
    "        with s:",
    "            self.__create_graph()",
    "        if s.get_estimated_time() >= 0.01:",
    "            logger.new_line()",
    "",
    "        self.visited = set()",
    "        self.stack = []",
    "        self.is_top_sorted = set()",
    "        self.topological_order = []",
    "        self.is_computed = set()",
    "",
    "        for keys in run_order:",
    "            self.__topological_sort(keys)",
    "            self.__compute_values()",
    ""
  ],
  "lab/configs/config_function.py": [
    "import ast",
    "import inspect",
    "import textwrap",
    "import warnings",
    "from enum import Enum",
    "from typing import List, Callable, cast, Set, Union",
    "from typing import TYPE_CHECKING",
    "",
    "from .config_item import ConfigItem",
    "",
    "if TYPE_CHECKING:",
    "    from . import Configs",
    "",
    "",
    "class DependencyParser(ast.NodeVisitor):",
    "    def __init__(self, func: Callable):",
    "        if type(func) == type:",
    "            func = cast(object, func).__init__",
    "            spec: inspect.Signature = inspect.signature(func)",
    "            params = spec.parameters",
    "            assert len(params) == 2",
    "            param: inspect.Parameter = params[list(params.keys())[1]]",
    "            source = textwrap.dedent(inspect.getsource(func))",
    "",
    "        else:",
    "            spec: inspect.Signature = inspect.signature(func)",
    "            params = spec.parameters",
    "            assert len(params) == 1",
    "            param: inspect.Parameter = params[list(params.keys())[0]]",
    "            source = inspect.getsource(func)",
    "",
    "        assert (param.kind == param.POSITIONAL_ONLY or",
    "                param.kind == param.POSITIONAL_OR_KEYWORD)",
    "",
    "        self.arg_name = param.name",
    "",
    "        self.required = set()",
    "        self.is_referenced = False",
    "",
    "        parsed = ast.parse(source)",
    "        self.visit(parsed)",
    "",
    "    def visit_Attribute(self, node: ast.Attribute):",
    "        while not isinstance(node.value, ast.Name):",
    "            if not isinstance(node.value, ast.Attribute):",
    "                return",
    "",
    "            node = node.value",
    "",
    "        if node.value.id != self.arg_name:",
    "            return",
    "",
    "        self.required.add(node.attr)",
    "",
    "    # Only visits if not captured before",
    "    def visit_Name(self, node: ast.Name):",
    "        if node.id == self.arg_name:",
    "            self.is_referenced = True",
    "            print(f\"Referenced {node.id} in {node.lineno}:{node.col_offset}\")",
    "",
    "",
    "def _get_dependencies(func: Callable) -> Set[str]:",
    "    parser = DependencyParser(func)",
    "    assert not parser.is_referenced, f\"{func} should only use attributes of configs\"",
    "    return parser.required",
    "",
    "",
    "class FunctionKind(Enum):",
    "    pass_configs = 'pass_configs'",
    "    pass_parameters = 'pass_parameters'",
    "    pass_nothing = 'pass_nothing'",
    "",
    "",
    "class ConfigFunction:",
    "    func: Callable",
    "    kind: FunctionKind",
    "    dependencies: Set[str]",
    "    config_names: Union[str, List[str]]",
    "    option_name: str",
    "    is_append: bool",
    "    params: List[inspect.Parameter]",
    "",
    "    def __get_type(self):",
    "        key, pos = 0, 0",
    "",
    "        for p in self.params:",
    "            if p.kind == p.POSITIONAL_OR_KEYWORD:",
    "                pos += 1",
    "            elif p.kind == p.KEYWORD_ONLY:",
    "                key += 1",
    "            else:",
    "                assert False, \"Only positional or keyword only arguments should be accepted\"",
    "",
    "        if pos == 1:",
    "            assert key == 0",
    "            return FunctionKind.pass_configs",
    "        elif pos == 0 and key == 0:",
    "            return FunctionKind.pass_nothing",
    "        else:",
    "            warnings.warn(\"Use configs object, because it's easier to refactor, find usage etc\",",
    "                          FutureWarning, stacklevel=4)",
    "            assert pos == 0",
    "            return FunctionKind.pass_parameters",
    "",
    "    def __get_dependencies(self):",
    "        if self.kind == FunctionKind.pass_configs:",
    "            parser = DependencyParser(self.func)",
    "            assert not parser.is_referenced, \\",
    "                f\"{self.func.__name__} should only use attributes of configs\"",
    "            return parser.required",
    "        else:",
    "            return {p.name for p in self.params}",
    "",
    "    def __get_option_name(self, option_name: str):",
    "        if option_name is not None:",
    "            return option_name",
    "        else:",
    "            return self.func.__name__",
    "",
    "    def __get_config_names(self, config_names: Union[str, ConfigItem, List[ConfigItem], List[str]]):",
    "        if config_names is None:",
    "            warnings.warn(\"Use @Config.[name]\", FutureWarning, 4)",
    "            return self.func.__name__",
    "        elif type(config_names) == str:",
    "            warnings.warn(\"Use @Config.[name] instead of '[name]'\", FutureWarning, 4)",
    "            return config_names",
    "        elif type(config_names) == ConfigItem:",
    "            return config_names.key",
    "        else:",
    "            assert type(config_names) == list",
    "            assert len(config_names) > 0",
    "            if type(config_names[0]) == str:",
    "                warnings.warn(\"Use @Config.[name] instead of '[name]'\", FutureWarning, 4)",
    "                return config_names",
    "            else:",
    "                assert type(config_names[0]) == ConfigItem",
    "                return [c.key for c in config_names]",
    "",
    "    def __get_params(self):",
    "        func_type = type(self.func)",
    "",
    "        if func_type == type:",
    "            init_func = cast(object, self.func).__init__",
    "            spec: inspect.Signature = inspect.signature(init_func)",
    "            params: List[inspect.Parameter] = list(spec.parameters.values())",
    "            assert len(params) > 0",
    "            assert params[0].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD, self.config_names",
    "            assert params[0].name == 'self'",
    "            return params[1:]",
    "        else:",
    "            spec: inspect.Signature = inspect.signature(self.func)",
    "            params: List[inspect.Parameter] = list(spec.parameters.values())",
    "            return params",
    "",
    "    def __init__(self, func, *,",
    "                 config_names: Union[str, ConfigItem, List[ConfigItem], List[str]],",
    "                 option_name: str,",
    "                 is_append: bool):",
    "        self.func = func",
    "        self.config_names = self.__get_config_names(config_names)",
    "        self.is_append = is_append",
    "        assert not (self.is_append and type(self.config_names) != str)",
    "        self.option_name = self.__get_option_name(option_name)",
    "",
    "        self.params = self.__get_params()",
    "",
    "        self.kind = self.__get_type()",
    "        self.dependencies = self.__get_dependencies()",
    "",
    "    def __call__(self, configs: 'Configs'):",
    "        if self.kind == FunctionKind.pass_configs:",
    "            if len(self.params) == 1:",
    "                return self.func(configs)",
    "            else:",
    "                return self.func()",
    "        elif self.kind == FunctionKind.pass_parameters:",
    "            kwargs = {p.name: configs.__getattribute__(p.name) for p in self.params}",
    "            return self.func(**kwargs)",
    "        else:",
    "            return self.func()",
    ""
  ],
  "lab/configs/config_item.py": [
    "class ConfigItem:",
    "    def __init__(self, key: str,",
    "                 has_annotation: bool, annotation: any,",
    "                 has_value: bool, value: any):",
    "        self.key = key",
    "        if annotation is None:",
    "            annotation = type(value)",
    "        self.annotation = annotation",
    "        self.value = value",
    "        self.has_annotation = has_annotation",
    "        self.has_value = has_value",
    "",
    "    def update(self, k: 'ConfigItem'):",
    "        if k.has_annotation:",
    "            self.has_annotation = True",
    "            self.annotation = k.annotation",
    "",
    "        if k.has_value:",
    "            self.has_value = True",
    "            self.value = k.value"
  ],
  "lab/configs/parser.py": [
    "import warnings",
    "from collections import OrderedDict",
    "from typing import List, Dict, Type, OrderedDict as OrderedDictType, Set",
    "from typing import TYPE_CHECKING",
    "",
    "from .config_function import ConfigFunction",
    "from .config_item import ConfigItem",
    "",
    "if TYPE_CHECKING:",
    "    from . import Configs",
    "",
    "_CALCULATORS = '_calculators'",
    "",
    "",
    "def _get_base_classes(class_: Type['Configs']) -> List[Type['Configs']]:",
    "    classes = [class_]",
    "    level = [class_]",
    "    next_level = []",
    "",
    "    while len(level) > 0:",
    "        for c in level:",
    "            for b in c.__bases__:",
    "                if b == object:",
    "                    continue",
    "                next_level.append(b)",
    "        classes += next_level",
    "        level = next_level",
    "        next_level = []",
    "",
    "    classes.reverse()",
    "",
    "    unique_classes = []",
    "    hashes: Set[int] = set()",
    "    for c in classes:",
    "        if hash(c) not in hashes:",
    "            unique_classes.append(c)",
    "        hashes.add(hash(c))",
    "",
    "    return unique_classes",
    "",
    "",
    "RESERVED = {'calc', 'list'}",
    "_STANDARD_TYPES = {int, str, bool, Dict, List}",
    "",
    "",
    "class Parser:",
    "    config_items: Dict[str, ConfigItem]",
    "    options: Dict[str, OrderedDictType[str, ConfigFunction]]",
    "    types: Dict[str, Type]",
    "    values: Dict[str, any]",
    "    list_appends: Dict[str, List[ConfigFunction]]",
    "",
    "    def __init__(self, configs: 'Configs', values: Dict[str, any] = None):",
    "        classes = _get_base_classes(type(configs))",
    "",
    "        self.values = {}",
    "        self.types = {}",
    "        self.options = {}",
    "        self.list_appends = {}",
    "        self.config_items = {}",
    "        self.configs = configs",
    "",
    "        for c in classes:",
    "            # for k, v in c.__annotations__.items():",
    "            #     self.__collect_annotation(k, v)",
    "            #",
    "            for k, v in c.__dict__.items():",
    "                self.__collect_config_item(k, v)",
    "",
    "        for c in classes:",
    "            if _CALCULATORS in c.__dict__:",
    "                for k, calcs in c.__dict__[_CALCULATORS].items():",
    "                    assert k in self.types, \\",
    "                        f\"{k} calculator is present but the config declaration is missing\"",
    "                    for v in calcs:",
    "                        self.__collect_calculator(k, v)",
    "",
    "        for k, v in configs.__dict__.items():",
    "            assert k in self.types",
    "            self.__collect_value(k, v)",
    "",
    "        if values is not None:",
    "            for k, v in values.items():",
    "                assert k in self.types",
    "                self.__collect_value(k, v)",
    "",
    "        self.__calculate_missing_values()",
    "",
    "    @staticmethod",
    "    def is_valid(key):",
    "        if key.startswith('_'):",
    "            return False",
    "",
    "        if key in RESERVED:",
    "            return False",
    "",
    "        return True",
    "",
    "    def __collect_config_item(self, k, v: ConfigItem):",
    "        if not self.is_valid(k):",
    "            return",
    "",
    "        if v.has_value:",
    "            self.values[k] = v.value",
    "",
    "        if k in self.config_items:",
    "            self.config_items[k].update(v)",
    "        else:",
    "            self.config_items[k] = v",
    "",
    "        if k not in self.types:",
    "            self.types[k] = v.annotation",
    "",
    "    def __collect_value(self, k, v):",
    "        if not self.is_valid(k):",
    "            return",
    "",
    "        self.values[k] = v",
    "        if k not in self.types:",
    "            self.types[k] = type(v)",
    "",
    "    def __collect_annotation(self, k, v):",
    "        if not self.is_valid(k):",
    "            return",
    "",
    "        self.types[k] = v",
    "",
    "    def __collect_calculator(self, k, v: ConfigFunction):",
    "        if v.is_append:",
    "            if k not in self.list_appends:",
    "                self.list_appends[k] = []",
    "            self.list_appends[k].append(v)",
    "        else:",
    "            if k not in self.options:",
    "                self.options[k] = OrderedDict()",
    "            if v.option_name in self.options[k]:",
    "                if v != self.options[k][v.option_name]:",
    "                    warnings.warn(f\"Duplicate option for {k}: {v.option_name}\", Warning,",
    "                                  stacklevel=4)",
    "",
    "            self.options[k][v.option_name] = v",
    "",
    "    def __calculate_missing_values(self):",
    "        for k in self.types:",
    "            if k in self.values:",
    "                continue",
    "",
    "            if k in self.list_appends:",
    "                continue",
    "",
    "            if k in self.options:",
    "                self.values[k] = next(iter(self.options[k].keys()))",
    "                continue",
    "",
    "            if type(self.types[k]) == type:",
    "                if self.types[k] in _STANDARD_TYPES:",
    "                    continue",
    "",
    "                self.options[k] = OrderedDict()",
    "                self.options[k][k] = ConfigFunction(self.types[k],",
    "                                                    config_names=self.config_items[k],",
    "                                                    option_name=k,",
    "                                                    is_append=False)",
    "                self.values[k] = k",
    "                continue",
    "",
    "            assert k in self.values, f\"Cannot compute {k}\"",
    ""
  ],
  "lab/configs/sample.py": [
    "from typing import List",
    "",
    "from lab.configs import Configs, ConfigProcessor",
    "",
    "",
    "class SampleModel:",
    "    def __init__(self, c: 'Sample'):",
    "        self.w = c.workers_count",
    "",
    "",
    "class Sample(Configs):",
    "    total_global_steps: int = 10",
    "    workers_count: int = 10",
    "    # empty: str",
    "",
    "    x = 'string'",
    "",
    "    input_model: int",
    "    model: int",
    "",
    "    # get from type annotations",
    "    model_obj: SampleModel",
    "",
    "    steps: List[any]",
    "",
    "",
    "class SampleChild(Sample):",
    "    def __init__(self, *, test: int):",
    "        pass",
    "",
    "    new_attr = 2",
    "",
    "",
    "@Sample.calc()",
    "def input_model(c: Sample):",
    "    return c.workers_count * 2",
    "",
    "",
    "@Sample.calc(Sample.input_model)",
    "def input_model2(c: Sample):",
    "    return c.workers_count * 20",
    "",
    "",
    "@Sample.calc('model')",
    "def simple_model(c: Sample):",
    "    return c.total_global_steps * 3",
    "",
    "",
    "# When collecting unordered items",
    "@Sample.list('steps')",
    "def remove_first():",
    "    return None",
    "",
    "",
    "@Sample.list('steps')",
    "def model_step(c: Sample):",
    "    return c.model",
    "",
    "",
    "configs = Sample()",
    "",
    "processor = ConfigProcessor(configs)",
    "processor()",
    "",
    "print(configs.__dict__)",
    ""
  ],
  "lab/experiment/__init__.py": [
    "import os",
    "import pathlib",
    "import time",
    "from typing import Optional, List, Set, Dict, Union",
    "",
    "import git",
    "",
    "from lab import logger",
    "from lab.configs import Configs, ConfigProcessor",
    "from lab.experiment.experiment_run import Run",
    "from lab.lab import Lab",
    "from lab.logger.colors import Text",
    "from lab.logger.internal import CheckpointSaver",
    "from lab.logger.writers import sqlite, tensorboard",
    "from lab.util import is_ipynb",
    "",
    "",
    "class Experiment:",
    "    \"\"\"",
    "    ## Experiment",
    "",
    "    Each experiment has different configurations or algorithms.",
    "    An experiment can have multiple trials.",
    "    \"\"\"",
    "    run: Run",
    "    configs_processor: Optional[ConfigProcessor]",
    "",
    "    # whether not to start the experiment if there are uncommitted changes.",
    "    check_repo_dirty: bool",
    "",
    "    def __init__(self, *,",
    "                 name: Optional[str],",
    "                 python_file: Optional[str],",
    "                 comment: Optional[str],",
    "                 writers: Set[str] = None,",
    "                 ignore_callers: Set[str] = None):",
    "        \"\"\"",
    "        ### Create the experiment",
    "",
    "        :param name: name of the experiment",
    "        :param python_file: `__file__` that invokes this. This is stored in",
    "         the experiments list.",
    "        :param comment: a short description of the experiment",
    "",
    "        The experiments log keeps track of `python_file`, `name`, `comment` as",
    "         well as the git commit.",
    "",
    "        Experiment maintains the locations of checkpoints, logs, etc.",
    "        \"\"\"",
    "",
    "        if python_file is None:",
    "            python_file = self.__get_caller_file(ignore_callers)",
    "",
    "        if python_file.startswith('<ipython'):",
    "            assert is_ipynb()",
    "            if name is None:",
    "                raise ValueError(\"You must specify python_file or experiment name\"",
    "                                 \" when creating an experiment from a python notebook.\")",
    "            self.lab = Lab(os.getcwd())",
    "            python_file = 'notebook.ipynb'",
    "        else:",
    "            self.lab = Lab(python_file)",
    "",
    "            if name is None:",
    "                file_path = pathlib.PurePath(python_file)",
    "                name = file_path.stem",
    "",
    "        logger.internal().set_data_path(self.lab.data_path)",
    "",
    "        if comment is None:",
    "            comment = ''",
    "",
    "        self.name = name",
    "        self.experiment_path = self.lab.experiments / name",
    "",
    "        self.check_repo_dirty = self.lab.check_repo_dirty",
    "",
    "        self.configs_processor = None",
    "",
    "        experiment_path = pathlib.Path(self.experiment_path)",
    "        if not experiment_path.exists():",
    "            experiment_path.mkdir(parents=True)",
    "",
    "        self.run = Run.create(",
    "            experiment_path=self.experiment_path,",
    "            python_file=python_file,",
    "            trial_time=time.localtime(),",
    "            comment=comment)",
    "",
    "        repo = git.Repo(self.lab.path)",
    "",
    "        self.run.commit = repo.head.commit.hexsha",
    "        self.run.commit_message = repo.head.commit.message.strip()",
    "        self.run.is_dirty = repo.is_dirty()",
    "        self.run.diff = repo.git.diff()",
    "",
    "        checkpoint_saver = self._create_checkpoint_saver()",
    "        logger.internal().set_checkpoint_saver(checkpoint_saver)",
    "",
    "        if writers is None:",
    "            writers = {'sqlite', 'tensorboard'}",
    "",
    "        if 'sqlite' in writers:",
    "            logger.internal().add_writer(sqlite.Writer(self.run.sqlite_path))",
    "        if 'tensorboard' in writers:",
    "            logger.internal().add_writer(tensorboard.Writer(self.run.tensorboard_log_path))",
    "",
    "        logger.internal().set_numpy_path(self.run.numpy_path)",
    "",
    "    @staticmethod",
    "    def __get_caller_file(ignore_callers: Set[str] = None):",
    "        if ignore_callers is None:",
    "            ignore_callers = {}",
    "",
    "        import inspect",
    "",
    "        frames: List[inspect.FrameInfo] = inspect.stack()",
    "        lab_src = pathlib.PurePath(__file__).parent.parent",
    "",
    "        for f in frames:",
    "            module_path = pathlib.PurePath(f.filename)",
    "            if str(module_path).startswith(str(lab_src)):",
    "                continue",
    "            if str(module_path) in ignore_callers:",
    "                continue",
    "            return str(module_path)",
    "",
    "        return ''",
    "",
    "    def _create_checkpoint_saver(self) -> Optional[CheckpointSaver]:",
    "        return None",
    "",
    "    def __print_info_and_check_repo(self):",
    "        \"\"\"",
    "        ## 🖨 Print the experiment info and check git repo status",
    "        \"\"\"",
    "",
    "        logger.new_line()",
    "        logger.log([",
    "            (self.name, Text.title),",
    "            ': ',",
    "            (str(self.run.uuid), Text.meta)",
    "        ])",
    "",
    "        if self.run.comment != '':",
    "            logger.log(['\\t', (self.run.comment, Text.highlight)])",
    "",
    "        logger.log([",
    "            \"\\t\"",
    "            \"[dirty]\" if self.run.is_dirty else \"[clean]\",",
    "            \": \",",
    "            (f\"\\\"{self.run.commit_message.strip()}\\\"\", Text.highlight)",
    "        ])",
    "",
    "        # Exit if git repository is dirty",
    "        if self.check_repo_dirty and self.run.is_dirty:",
    "            logger.log([(\"[FAIL]\", Text.danger),",
    "                        \" Cannot trial an experiment with uncommitted changes.\"])",
    "            exit(1)",
    "",
    "    def _load_checkpoint(self, checkpoint_path: pathlib.PurePath):",
    "        raise NotImplementedError()",
    "",
    "    def calc_configs(self,",
    "                     configs: Optional[Configs],",
    "                     configs_dict: Dict[str, any] = None,",
    "                     run_order: Optional[List[Union[List[str], str]]] = None):",
    "        if configs_dict is None:",
    "            configs_dict = {}",
    "        self.configs_processor = ConfigProcessor(configs, configs_dict)",
    "        self.configs_processor(run_order)",
    "        logger.new_line()",
    "",
    "    def __start_from_checkpoint(self, run_uuid: Optional[str], checkpoint: int):",
    "        checkpoint_path, global_step = experiment_run.get_last_run_checkpoint(",
    "            self.experiment_path,",
    "            run_uuid,",
    "            checkpoint,",
    "            {self.run.uuid})",
    "",
    "        if global_step is None:",
    "            return 0",
    "        else:",
    "            with logger.section(\"Loading checkpoint\"):",
    "                self._load_checkpoint(checkpoint_path)",
    "",
    "        return global_step",
    "",
    "    def start(self, *,",
    "              run_uuid: Optional[str] = None,",
    "              checkpoint: Optional[int] = None):",
    "        if run_uuid is not None:",
    "            if checkpoint is None:",
    "                checkpoint = -1",
    "            if run_uuid == '':",
    "                run_uuid = None",
    "            global_step = self.__start_from_checkpoint(run_uuid, checkpoint)",
    "        else:",
    "            global_step = 0",
    "",
    "        self.run.start_step = global_step",
    "        logger.internal().set_start_global_step(global_step)",
    "",
    "        self.__print_info_and_check_repo()",
    "        if self.configs_processor is not None:",
    "            self.configs_processor.print()",
    "",
    "        self.run.save_info()",
    "",
    "        if self.configs_processor is not None:",
    "            self.configs_processor.save(self.run.configs_path)",
    "",
    "        logger.internal().save_indicators(self.run.indicators_path)",
    ""
  ],
  "lab/experiment/experiment_run.py": [
    "import time",
    "from pathlib import Path, PurePath",
    "from typing import List, Dict, Optional, Set",
    "",
    "import numpy as np",
    "",
    "from .. import util, logger",
    "from ..logger.colors import Text",
    "",
    "",
    "def _struct_time_to_time(t: time.struct_time):",
    "    return f\"{t.tm_hour :02}:{t.tm_min :02}:{t.tm_sec :02}\"",
    "",
    "",
    "def _struct_time_to_date(t: time.struct_time):",
    "    return f\"{t.tm_year :04}-{t.tm_mon :02}-{t.tm_mday :02}\"",
    "",
    "",
    "_GLOBAL_STEP = 'global_step'",
    "",
    "",
    "def _generate_uuid() -> str:",
    "    from uuid import uuid1",
    "    return uuid1().hex",
    "",
    "",
    "class RunInfo:",
    "    def __init__(self, *,",
    "                 uuid: str,",
    "                 python_file: str,",
    "                 trial_date: str,",
    "                 trial_time: str,",
    "                 comment: str,",
    "                 commit: Optional[str] = None,",
    "                 commit_message: Optional[str] = None,",
    "                 is_dirty: bool = True,",
    "                 experiment_path: PurePath,",
    "                 start_step: int = 0,",
    "                 notes: str = ''):",
    "        self.uuid = uuid",
    "        self.commit = commit",
    "        self.is_dirty = is_dirty",
    "        self.python_file = python_file",
    "        self.trial_date = trial_date",
    "        self.trial_time = trial_time",
    "        self.comment = comment",
    "        self.commit_message = commit_message",
    "        self.start_step = start_step",
    "",
    "        self.experiment_path = experiment_path",
    "        self.run_path = experiment_path / str(uuid)",
    "        self.checkpoint_path = self.run_path / \"checkpoints\"",
    "        self.numpy_path = self.run_path / \"numpy\"",
    "",
    "        self.diff_path = self.run_path / \"source.diff\"",
    "",
    "        self.sqlite_path = self.run_path / \"sqlite.db\"",
    "        self.tensorboard_log_path = self.run_path / \"tensorboard\"",
    "",
    "        self.info_path = self.run_path / \"run.yaml\"",
    "        self.indicators_path = self.run_path / \"indicators.yaml\"",
    "        self.configs_path = self.run_path / \"configs.yaml\"",
    "        self.notes = notes",
    "",
    "    @classmethod",
    "    def from_dict(cls, experiment_path: PurePath, data: Dict[str, any]):",
    "        \"\"\"",
    "        ## Create a new trial from a dictionary",
    "        \"\"\"",
    "        params = dict(experiment_path=experiment_path)",
    "        params.update(data)",
    "        return cls(**params)",
    "",
    "    def to_dict(self):",
    "        \"\"\"",
    "        ## Convert trial to a dictionary for saving",
    "        \"\"\"",
    "        return dict(",
    "            uuid=self.uuid,",
    "            python_file=self.python_file,",
    "            trial_date=self.trial_date,",
    "            trial_time=self.trial_time,",
    "            comment=self.comment,",
    "            commit=self.commit,",
    "            commit_message=self.commit_message,",
    "            is_dirty=self.is_dirty,",
    "            start_step=self.start_step,",
    "            notes=self.notes",
    "        )",
    "",
    "    def pretty_print(self) -> List[str]:",
    "        \"\"\"",
    "        ## 🎨 Pretty print trial for the python file header",
    "        \"\"\"",
    "",
    "        # Trial information",
    "        commit_status = \"[dirty]\" if self.is_dirty else \"[clean]\"",
    "        res = [",
    "            f\"{self.trial_date} {self.trial_time}\",",
    "            self.comment,",
    "            f\"[{commit_status}]: {self.commit_message}\",",
    "            f\"start_step: {self.start_step}\"",
    "        ]",
    "",
    "        return res",
    "",
    "    def __str__(self):",
    "        return f\"{self.__class__.__name__}(comment=\\\"{self.comment}\\\",\" \\",
    "               f\" commit=\\\"{self.commit_message}\\\",\" \\",
    "               f\" date={self.trial_date}, time={self.trial_time})\"",
    "",
    "    def __repr__(self):",
    "        return self.__str__()",
    "",
    "    def is_after(self, run: 'Run'):",
    "        if run.trial_date < self.trial_date:",
    "            return True",
    "        elif run.trial_date > self.trial_date:",
    "            return False",
    "        elif run.trial_time < self.trial_time:",
    "            return True",
    "        else:",
    "            return False",
    "",
    "",
    "class Run(RunInfo):",
    "    \"\"\"",
    "    # Trial 🏃‍",
    "",
    "    Every trial in an experiment has same configs.",
    "    It's just multiple runs.",
    "",
    "    A new trial will replace checkpoints and TensorBoard summaries",
    "    or previous trials, you should make a copy if needed.",
    "    The performance log in `trials.yaml` is not replaced.",
    "",
    "    You should run new trials after bug fixes or to see performance is",
    "     consistent.",
    "",
    "    If you want to try different configs, create multiple experiments.",
    "    \"\"\"",
    "",
    "    diff: Optional[str]",
    "",
    "    def __init__(self, *,",
    "                 uuid: str,",
    "                 python_file: str,",
    "                 trial_date: str,",
    "                 trial_time: str,",
    "                 comment: str,",
    "                 commit: Optional[str] = None,",
    "                 commit_message: Optional[str] = None,",
    "                 is_dirty: bool = True,",
    "                 experiment_path: PurePath,",
    "                 start_step: int = 0,",
    "                 notes: str = ''):",
    "        super().__init__(python_file=python_file, trial_date=trial_date, trial_time=trial_time,",
    "                         comment=comment, uuid=uuid, experiment_path=experiment_path,",
    "                         commit=commit, commit_message=commit_message, is_dirty=is_dirty,",
    "                         start_step=start_step, notes=notes)",
    "",
    "    @classmethod",
    "    def create(cls, *,",
    "               experiment_path: PurePath,",
    "               python_file: str,",
    "               trial_time: time.struct_time,",
    "               comment: str):",
    "        \"\"\"",
    "        ## Create a new trial",
    "        \"\"\"",
    "        return cls(python_file=python_file,",
    "                   trial_date=_struct_time_to_date(trial_time),",
    "                   trial_time=_struct_time_to_time(trial_time),",
    "                   uuid=_generate_uuid(),",
    "                   experiment_path=experiment_path,",
    "                   comment=comment)",
    "",
    "    def save_info(self):",
    "        run_path = Path(self.run_path)",
    "        if not run_path.exists():",
    "            run_path.mkdir(parents=True)",
    "",
    "        with open(str(self.info_path), \"w\") as file:",
    "            file.write(util.yaml_dump(self.to_dict()))",
    "",
    "        if self.diff is not None:",
    "            with open(str(self.diff_path), \"w\") as f:",
    "                f.write(self.diff)",
    "",
    "",
    "def get_checkpoints(experiment_path: PurePath, run_uuid: str):",
    "    run_path = experiment_path / run_uuid",
    "    checkpoint_path = Path(run_path / \"checkpoints\")",
    "    if not checkpoint_path.exists():",
    "        return {}",
    "",
    "    return {int(child.name) for child in Path(checkpoint_path).iterdir()}",
    "",
    "",
    "def get_runs(experiment_path: PurePath):",
    "    return {child.name for child in Path(experiment_path).iterdir()}",
    "",
    "",
    "def get_last_run(experiment_path: PurePath, runs: Set[str]) -> Run:",
    "    last: Optional[Run] = None",
    "    for run_uuid in runs:",
    "        run_path = experiment_path / run_uuid",
    "        info_path = run_path / \"run.yaml\"",
    "        with open(str(info_path), \"r\") as file:",
    "            run = Run.from_dict(experiment_path, util.yaml_load(file.read()))",
    "            if last is None:",
    "                last = run",
    "            elif run.is_after(last):",
    "                last = run",
    "",
    "    return last",
    "",
    "",
    "def get_run_checkpoint(experiment_path: PurePath,",
    "                       run_uuid: Optional[str] = None, checkpoint: int = -1,",
    "                       skip_uuid: Set[str] = None):",
    "    if skip_uuid is None:",
    "        skip_uuid = {}",
    "    runs = get_runs(experiment_path)",
    "    runs.difference_update(skip_uuid)",
    "",
    "    if len(runs) == 0:",
    "        return None, None",
    "",
    "    if run_uuid is None:",
    "        run_uuid = get_last_run(experiment_path, runs).uuid",
    "",
    "    checkpoints = get_checkpoints(experiment_path, run_uuid)",
    "    if len(checkpoints) == 0:",
    "        return None, None",
    "",
    "    if checkpoint < 0:",
    "        required_ci = np.max(list(checkpoints)) + checkpoint + 1",
    "    else:",
    "        required_ci = checkpoint",
    "",
    "    for ci in range(required_ci, -1, -1):",
    "        if ci not in checkpoints:",
    "            continue",
    "",
    "        return run_uuid, ci",
    "",
    "",
    "def get_last_run_checkpoint(experiment_path: PurePath,",
    "                            run_uuid: Optional[str] = None,",
    "                            checkpoint: int = -1,",
    "                            skip_uuid: Set[str] = None):",
    "    run_uuid, checkpoint = get_run_checkpoint(experiment_path, run_uuid,",
    "                                              checkpoint, skip_uuid)",
    "",
    "    if run_uuid is None:",
    "        logger.log(\"Couldn't find a previous run/checkpoint\")",
    "        return None, None",
    "",
    "    logger.log([\"Selected \",",
    "                (\"run\", Text.key),",
    "                \" = \",",
    "                (run_uuid, Text.value),",
    "                \" \",",
    "                (\"checkpoint\", Text.key),",
    "                \" = \",",
    "                (checkpoint, Text.value)])",
    "",
    "    run_path = experiment_path / str(run_uuid)",
    "    checkpoint_path = run_path / \"checkpoints\"",
    "    return checkpoint_path / str(checkpoint), checkpoint",
    ""
  ],
  "lab/experiment/pytorch.py": [
    "import json",
    "import pathlib",
    "from typing import Optional, Dict, Set",
    "",
    "import numpy as np",
    "import torch.nn",
    "",
    "from lab import experiment",
    "from lab.logger.internal import CheckpointSaver",
    "",
    "",
    "class Checkpoint(CheckpointSaver):",
    "    __models: Dict[str, torch.nn.Module]",
    "",
    "    def __init__(self, path: pathlib.PurePath):",
    "        self.path = path",
    "        self.__models = {}",
    "",
    "    def add_models(self, models: Dict[str, torch.nn.Module]):",
    "        \"\"\"",
    "        ## Set variable for saving and loading",
    "        \"\"\"",
    "        self.__models.update(models)",
    "",
    "    def save(self, global_step):",
    "        \"\"\"",
    "        ## Save model as a set of numpy arrays",
    "        \"\"\"",
    "",
    "        checkpoints_path = pathlib.Path(self.path)",
    "        if not checkpoints_path.exists():",
    "            checkpoints_path.mkdir()",
    "",
    "        checkpoint_path = checkpoints_path / str(global_step)",
    "        assert not checkpoint_path.exists()",
    "",
    "        checkpoint_path.mkdir()",
    "",
    "        files = {}",
    "        for name, model in self.__models.items():",
    "            state: Dict[str, torch.Tensor] = model.state_dict()",
    "            files[name] = {}",
    "            for key, tensor in state.items():",
    "                if key == \"_metadata\":",
    "                    continue",
    "",
    "                file_name = f\"{name}_{key}.npy\"",
    "                files[name][key] = file_name",
    "",
    "                np.save(str(checkpoint_path / file_name), tensor.cpu().numpy())",
    "",
    "        # Save header",
    "        with open(str(checkpoint_path / \"info.json\"), \"w\") as f:",
    "            f.write(json.dumps(files))",
    "",
    "        # Delete old checkpoints",
    "        # for c in checkpoints_path.iterdir():",
    "        #     if c.name != checkpoint_path.name:",
    "        #         util.rm_tree(c)",
    "",
    "    def load(self, checkpoint_path):",
    "        \"\"\"",
    "        ## Load model as a set of numpy arrays",
    "        \"\"\"",
    "",
    "        with open(str(checkpoint_path / \"info.json\"), \"r\") as f:",
    "            files = json.loads(f.readline())",
    "",
    "        # Load each variable",
    "        for name, model in self.__models.items():",
    "            state: Dict[str, torch.Tensor] = model.state_dict()",
    "            for key, tensor in state.items():",
    "                file_name = files[name][key]",
    "                saved = np.load(str(checkpoint_path / file_name))",
    "                saved = torch.from_numpy(saved).to(tensor.device)",
    "                state[key] = saved",
    "",
    "            model.load_state_dict(state)",
    "",
    "        return True",
    "",
    "",
    "class Experiment(experiment.Experiment):",
    "    \"\"\"",
    "    ## Experiment",
    "",
    "    Each experiment has different configurations or algorithms.",
    "    An experiment can have multiple trials.",
    "    \"\"\"",
    "",
    "    __checkpoint_saver: Checkpoint",
    "",
    "    def __init__(self, *,",
    "                 name: Optional[str] = None,",
    "                 python_file: Optional[str] = None,",
    "                 comment: Optional[str] = None,",
    "                 writers: Set[str] = None,",
    "                 ignore_callers: Set[str] = None):",
    "        \"\"\"",
    "        ### Create the experiment",
    "",
    "        :param name: name of the experiment",
    "        :param python_file: `__file__` that invokes this. This is stored in",
    "         the experiments list.",
    "        :param comment: a short description of the experiment",
    "",
    "        The experiments log keeps track of `python_file`, `name`, `comment` as",
    "         well as the git commit.",
    "",
    "        Experiment maintains the locations of checkpoints, logs, etc.",
    "        \"\"\"",
    "",
    "        super().__init__(name=name,",
    "                         python_file=python_file,",
    "                         comment=comment,",
    "                         writers=writers,",
    "                         ignore_callers=ignore_callers)",
    "",
    "    def _create_checkpoint_saver(self):",
    "        self.__checkpoint_saver = Checkpoint(self.run.checkpoint_path)",
    "        return self.__checkpoint_saver",
    "",
    "    def add_models(self, models: Dict[str, torch.nn.Module]):",
    "        \"\"\"",
    "        ## Set variable for saving and loading",
    "        \"\"\"",
    "        self.__checkpoint_saver.add_models(models)",
    "",
    "    def _load_checkpoint(self, checkpoint_path: pathlib.PurePath):",
    "        self.__checkpoint_saver.load(checkpoint_path)",
    ""
  ],
  "lab/lab.py": [
    "from pathlib import PurePath, Path",
    "from typing import List",
    "",
    "from . import util",
    "",
    "_CONFIG_FILE_NAME = '.lab.yaml'",
    "",
    "",
    "class Lab:",
    "    \"\"\"",
    "    ### Lab",
    "",
    "    Lab contains the lab specific properties.",
    "    \"\"\"",
    "",
    "    def __init__(self, path: str):",
    "        configs = self.__get_config_files(path)",
    "",
    "        if len(configs) == 0:",
    "            raise RuntimeError(\"No '.lab.yaml' config file found.\")",
    "",
    "        config = self.__get_config(configs)",
    "",
    "        self.path = PurePath(config['path'])",
    "        self.check_repo_dirty = config['check_repo_dirty']",
    "        self.data_path = self.path / config['data_path']",
    "        self.experiments = self.path / config['experiments_path']",
    "",
    "    def __str__(self):",
    "        return f\"<Lab path={self.path}>\"",
    "",
    "    def __repr__(self):",
    "        return str(self)",
    "",
    "    @staticmethod",
    "    def __get_config(configs):",
    "        config = dict(",
    "            path=None,",
    "            check_repo_dirty=True,",
    "            is_log_python_file=True,",
    "            config_file_path=None,",
    "            data_path='data',",
    "            experiments_path='logs',",
    "            analytics_path='analytics',",
    "            analytics_templates={}",
    "        )",
    "",
    "        for i, c in enumerate(reversed(configs)):",
    "            if config['path'] is None:",
    "                config['path'] = c['config_file_path']",
    "",
    "            assert 'path' not in c",
    "            assert i == 0 or 'experiments_path' not in c",
    "            assert i == 0 or 'analytics_path' not in c",
    "",
    "            for k, v in c.items():",
    "                if k not in config:",
    "                    raise RuntimeError(f\"Unknown config parameter #{k} in file \"",
    "                                       f\"{c['config_file_path'] / _CONFIG_FILE_NAME}\")",
    "                else:",
    "                    config[k] = v",
    "",
    "        return config",
    "",
    "    @staticmethod",
    "    def __get_config_files(path: str):",
    "        path = Path(path).resolve()",
    "        configs = []",
    "",
    "        while path.exists():",
    "            if path.is_dir():",
    "                config_file = path / _CONFIG_FILE_NAME",
    "                if config_file.is_file():",
    "                    with open(str(config_file)) as f:",
    "                        config = util.yaml_load(f.read())",
    "                        if config is None:",
    "                            config = {}",
    "                        config['config_file_path'] = path",
    "                        configs.append(config)",
    "",
    "            if str(path) == path.root:",
    "                break",
    "",
    "            path = path.parent",
    "",
    "        return configs",
    "",
    "    def get_experiments(self) -> List[Path]:",
    "        \"\"\"",
    "        Get list of experiments",
    "        \"\"\"",
    "        experiments_path = Path(self.experiments)",
    "        return [child for child in experiments_path.iterdir()]",
    ""
  ],
  "lab/logger/__init__.py": [
    "from typing import Union, List, Tuple, Optional, Iterable, Sized",
    "",
    "import numpy as np",
    "",
    "from .colors import StyleCode",
    "from .indicators import Indicator",
    "from .internal import LoggerInternal as _LoggerInternal",
    "",
    "_internal: Optional[_LoggerInternal] = None",
    "",
    "",
    "def internal() -> _LoggerInternal:",
    "    global _internal",
    "    if _internal is None:",
    "        _internal = _LoggerInternal()",
    "",
    "    return _internal",
    "",
    "",
    "def log(message: Union[str, List[Union[str, Tuple[str, StyleCode]]]],",
    "        color: List[StyleCode] or StyleCode or None = None,",
    "        *,",
    "        is_new_line=True):",
    "    if type(message) == str:",
    "        internal().log([(message, color)], is_new_line=is_new_line)",
    "    elif type(message) == list:",
    "        internal().log(message, is_new_line=is_new_line)",
    "",
    "",
    "def add_indicator(indicator: Indicator):",
    "    internal().add_indicator(indicator)",
    "",
    "",
    "def store(*args, **kwargs):",
    "    internal().store(*args, **kwargs)",
    "",
    "",
    "def write():",
    "    internal().write()",
    "",
    "",
    "def new_line():",
    "    internal().new_line()",
    "",
    "",
    "def set_global_step(global_step):",
    "    internal().set_global_step(global_step)",
    "",
    "",
    "def add_global_step(global_step: int = 1):",
    "    internal().add_global_step(global_step)",
    "",
    "",
    "def get_global_step() -> int:",
    "    return internal().global_step",
    "",
    "",
    "def iterate(name, iterable: Union[Iterable, Sized, int],",
    "            total_steps: Optional[int] = None, *,",
    "            is_silent: bool = False,",
    "            is_timed: bool = True):",
    "    return internal().iterate(name, iterable, total_steps, is_silent=is_silent,",
    "                              is_timed=is_timed)",
    "",
    "",
    "def enum(name, iterable: Sized, *,",
    "         is_silent: bool = False,",
    "         is_timed: bool = True):",
    "    return internal().enum(name, iterable, is_silent=is_silent, is_timed=is_timed)",
    "",
    "",
    "def section(name, *,",
    "            is_silent: bool = False,",
    "            is_timed: bool = True,",
    "            is_partial: bool = False,",
    "            is_new_line: bool = True,",
    "            total_steps: float = 1.0):",
    "    return internal().section(name, is_silent=is_silent,",
    "                              is_timed=is_timed,",
    "                              is_partial=is_partial,",
    "                              total_steps=total_steps,",
    "                              is_new_line=is_new_line)",
    "",
    "",
    "def progress(steps: float):",
    "    internal().progress(steps)",
    "",
    "",
    "def set_successful(is_successful=True):",
    "    internal().set_successful(is_successful)",
    "",
    "",
    "def loop(iterator_: range, *,",
    "         is_print_iteration_time=True):",
    "    return internal().loop(iterator_, is_print_iteration_time=is_print_iteration_time)",
    "",
    "",
    "def finish_loop():",
    "    internal().finish_loop()",
    "",
    "",
    "def save_checkpoint():",
    "    internal().save_checkpoint()",
    "",
    "",
    "def info(*args, **kwargs):",
    "    \"\"\"",
    "    ### 🎨 Pretty prints a set of values.",
    "    \"\"\"",
    "",
    "    internal().info(*args, **kwargs)",
    "",
    "",
    "def get_data_path():",
    "    return internal().get_data_path()",
    "",
    "",
    "def save_numpy(name: str, array: np.ndarray):",
    "    \"\"\"",
    "    ## Save a single numpy array",
    "",
    "    This is used to save processed data",
    "    \"\"\"",
    "    internal().save_numpy(name, array)",
    ""
  ],
  "lab/logger/colors.py": [
    "\"\"\"",
    "Console colors",
    "\"\"\"",
    "from enum import Enum",
    "",
    "_ANSI_CODES = dict(",
    "    normal=0,",
    "    bold=1,",
    "    light=2,  # - PyCharm/Jupyter",
    "",
    "    italic=3,  # - PyCharm/Jupyter",
    "    underline=4,",
    "",
    "    highlight=7,  # Changes background in PyCharm/Terminal",
    "",
    "    # Colors",
    "    black=30,",
    "    red=31,",
    "    green=32,",
    "    orange=33,",
    "    blue=34,",
    "    purple=35,",
    "    cyan=36,",
    "    white=37,",
    "",
    "    # Background [Not used anymore]",
    "    bg_black=40,",
    "    bg_red=41,",
    "    bg_green=42,",
    "    bg_orange=43,",
    "    bg_blue=44,",
    "    bg_purple=45,",
    "    bg_cyan=46,",
    "    bg_white=47,",
    "",
    "    # Bright Colors [Not used anymore]",
    "    bright_black=90,",
    "    bright_red=91,",
    "    bright_green=92,",
    "    bright_orange=93,",
    "    bright_blue=94,",
    "    bright_purple=95,",
    "    bright_cyan=96,",
    "    bright_white=97,",
    "",
    "    # Bright Background Colors [Not used anymore]",
    "    bg_bright_black=100,",
    "    bg_bright_red=101,",
    "    bg_bright_green=102,",
    "    bg_bright_orange=103,",
    "    bg_bright_blue=104,",
    "    bg_bright_purple=105,",
    "    bg_bright_cyan=106,",
    "    bg_bright_white=107",
    ")",
    "",
    "ANSI_RESET = \"\\33[0m\"",
    "",
    "_HTML_STYLES = dict(",
    "    normal=('', ''),",
    "    bold=('<strong>', '</strong>'),",
    "    underline=('<span style=\"text-decoration: underline\">', '</span>'),",
    "    light=('', ''),",
    "",
    "    # Colors",
    "    black=('<span style=\"color: #3E424D\">', '</span>'),",
    "    red=('<span style=\"color: #E75C58\">', '</span>'),",
    "    green=('<span style=\"color: #00A250\">', '</span>'),",
    "    orange=('<span style=\"color: #DDB62B\">', '</span>'),",
    "    blue=('<span style=\"color: #208FFB\">', '</span>'),",
    "    purple=('<span style=\"color: #D160C4\">', '</span>'),",
    "    cyan=('<span style=\"color: #60C6C8\">', '</span>'),",
    "    white=('<span style=\"color: #C5C1B4\">', '</span>')",
    ")",
    "",
    "",
    "class StyleCode(Enum):",
    "    def ansi(self):",
    "        if self.value is None:",
    "            return f\"\\33[{_ANSI_CODES['normal']}m\"",
    "        elif type(self.value) == str:",
    "            return f\"\\33[{_ANSI_CODES[self.value]}m\"",
    "        elif type(self.value) == list:",
    "            return ''.join([f\"\\33[{_ANSI_CODES[v]}m\" for v in self.value])",
    "        else:",
    "            assert False",
    "",
    "    def html_open(self):",
    "        if self.value is None:",
    "            return \"\"",
    "        elif type(self.value) == str:",
    "            return _HTML_STYLES[self.value][0]",
    "        elif type(self.value) == list:",
    "            return ''.join([_HTML_STYLES[v][0] for v in self.value])",
    "        else:",
    "            assert False",
    "",
    "    def html_close(self):",
    "        if self.value is None:",
    "            return \"\"",
    "        elif type(self.value) == str:",
    "            return _HTML_STYLES[self.value][1]",
    "        elif type(self.value) == list:",
    "            return ''.join([_HTML_STYLES[v][1] for v in reversed(self.value)])",
    "        else:",
    "            assert False",
    "",
    "",
    "class Style(StyleCode):",
    "    none = None",
    "    normal = 'normal'",
    "    bold = 'bold'",
    "    underline = 'underline'",
    "    light = 'light'",
    "",
    "",
    "class Color(StyleCode):",
    "    none = None",
    "    black = 'black'",
    "    red = 'red'",
    "    green = 'green'",
    "    orange = 'orange'",
    "    blue = 'blue'",
    "    purple = 'purple'",
    "    cyan = 'cyan'",
    "    white = 'white'",
    "",
    "",
    "class Text(StyleCode):",
    "    none = None",
    "    danger = Color.red.value",
    "    success = Color.green.value",
    "    warning = Color.orange.value",
    "    meta = Color.blue.value",
    "    key = Color.cyan.value",
    "    meta2 = Color.purple.value",
    "    title = [Style.bold.value, Style.underline.value]",
    "    heading = Style.underline.value",
    "    value = Style.bold.value",
    "    highlight = [Style.bold.value, Color.orange.value]",
    "    subtle = [Style.light.value, Color.white.value]",
    "",
    "",
    "def _test():",
    "    for i in [0, 38, 48]:",
    "        for j in [5]:",
    "            for k in range(16):",
    "                print(\"\\33[{};{};{}m{:02d},{},{:03d}\\33[0m\\t\".format(i, j, k, i, j, k),",
    "                      end='')",
    "                if (k + 1) % 6 == 0:",
    "                    print(\"\")",
    "            print(\"\")",
    "",
    "    for i in range(0, 128):",
    "        print(f\"\\33[{i}m{i :03d}\\33[0m \", end='')",
    "        if (i + 1) % 10 == 0:",
    "            print(\"\")",
    "",
    "    print(\"\")",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    _test()",
    ""
  ],
  "lab/logger/delayed_keyboard_interrupt.py": [
    "import signal",
    "",
    "import lab",
    "from .colors import Text",
    "",
    "",
    "class DelayedKeyboardInterrupt:",
    "    \"\"\"",
    "    ### Capture `KeyboardInterrupt` and fire it later",
    "    \"\"\"",
    "",
    "    def __init__(self, logger: 'lab.logger.internal.LoggerInternal'):",
    "        self.signal_received = None",
    "        self.logger = logger",
    "",
    "    def __enter__(self):",
    "        self.signal_received = None",
    "        # Start capturing",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)",
    "",
    "    def handler(self, sig, frame):",
    "        # Pass second interrupt without delaying",
    "        if self.signal_received is not None:",
    "            self.old_handler(*self.signal_received)",
    "            return",
    "",
    "        # Store the interrupt signal for later",
    "        self.signal_received = (sig, frame)",
    "        self.logger.log('\\nSIGINT received. Delaying KeyboardInterrupt.',",
    "                        color=Text.danger)",
    "",
    "    def __exit__(self, exc_type, exc_val, exc_tb):",
    "        # Reset handler",
    "        signal.signal(signal.SIGINT, self.old_handler)",
    "",
    "        # Pass on any captured interrupt signals",
    "        if self.signal_received is not None:",
    "            self.old_handler(*self.signal_received)",
    ""
  ],
  "lab/logger/destinations/__init__.py": [
    "from typing import List, Union, Tuple",
    "",
    "from lab.logger.colors import StyleCode",
    "",
    "",
    "class Destination:",
    "    def log(self, parts: List[Union[str, Tuple[str, StyleCode]]], *,",
    "            is_new_line=True):",
    "        raise NotImplementedError()",
    "",
    "    def new_line(self):",
    "        raise NotImplementedError()",
    ""
  ],
  "lab/logger/destinations/console.py": [
    "from typing import List, Union, Tuple",
    "",
    "from lab.logger.colors import StyleCode, ANSI_RESET",
    "from lab.logger.destinations import Destination",
    "",
    "",
    "class ConsoleDestination(Destination):",
    "    @staticmethod",
    "    def __ansi_code(text: str, color: List[StyleCode] or StyleCode or None):",
    "        \"\"\"",
    "        ### Add ansi color codes",
    "        \"\"\"",
    "        if color is None:",
    "            return text",
    "        elif type(color) is list:",
    "            return \"\".join([c.ansi() for c in color]) + f\"{text}{ANSI_RESET}\"",
    "        else:",
    "            return f\"{color.ansi()}{text}{ANSI_RESET}\"",
    "",
    "    def log(self, parts: List[Union[str, Tuple[str, StyleCode]]], *,",
    "            is_new_line=True):",
    "        tuple_parts = []",
    "        for p in parts:",
    "            if type(p) == str:",
    "                tuple_parts.append((p, None))",
    "            else:",
    "                tuple_parts.append(p)",
    "        coded = [self.__ansi_code(text, color) for text, color in tuple_parts]",
    "",
    "        if is_new_line:",
    "            end_char = '\\n'",
    "        else:",
    "            end_char = ''",
    "",
    "        text = \"\".join(coded)",
    "",
    "        print(\"\\r\" + text, end=end_char, flush=True)",
    "",
    "    def new_line(self):",
    "        print()",
    ""
  ],
  "lab/logger/destinations/factory.py": [
    "from lab.logger.destinations import Destination",
    "from lab.util import is_ipynb",
    "",
    "",
    "def create_destination() -> Destination:",
    "    if is_ipynb():",
    "        from lab.logger.destinations.ipynb import IpynbDestination",
    "        return IpynbDestination()",
    "    else:",
    "        from lab.logger.destinations.console import ConsoleDestination",
    "        return ConsoleDestination()",
    ""
  ],
  "lab/logger/destinations/ipynb.py": [
    "from typing import List, Union, Tuple",
    "",
    "from IPython.core.display import display, HTML",
    "",
    "from lab.logger.colors import StyleCode",
    "from lab.logger.destinations import Destination",
    "",
    "",
    "class IpynbDestination(Destination):",
    "    def __init__(self):",
    "        self.__last_handle = None",
    "        self.__last_id = 1",
    "        self.__cell_lines = []",
    "        self.__cell_count = 0",
    "",
    "    def is_same_cell(self):",
    "        cells = get_ipython().ev('len(In)')",
    "        if cells == self.__cell_count:",
    "            return True",
    "",
    "        self.__cell_count = cells",
    "        self.__cell_lines = []",
    "        self.__last_handle = None",
    "",
    "        return False",
    "",
    "    @staticmethod",
    "    def __html_code(text: str, color: List[StyleCode] or StyleCode or None):",
    "        \"\"\"",
    "        ### Add ansi color codes",
    "        \"\"\"",
    "        if text == '\\n':",
    "            assert color is None",
    "            return text",
    "",
    "        if color is None:",
    "            return text",
    "        elif type(color) is list:",
    "            open_tags = ''.join([c.html_open() for c in color])",
    "            close_tags = ''.join([c.html_close() for c in reversed(color)])",
    "        else:",
    "            open_tags = color.html_open()",
    "            close_tags = color.html_close()",
    "",
    "        return open_tags + text + close_tags",
    "",
    "    def log(self, parts: List[Union[str, Tuple[str, StyleCode]]], *,",
    "            is_new_line=True):",
    "        tuple_parts = []",
    "        for p in parts:",
    "            if type(p) == str:",
    "                text = p",
    "                style = None",
    "            else:",
    "                text, style = p",
    "            lines = text.split('\\n')",
    "            for line in lines[:-1]:",
    "                tuple_parts.append((line, style))",
    "                tuple_parts.append(('\\n', None))",
    "            tuple_parts.append((lines[-1], style))",
    "",
    "        coded = [self.__html_code(text, color) for text, color in tuple_parts]",
    "",
    "        text = \"\".join(coded)",
    "        lines = text.split('\\n')",
    "        if self.is_same_cell():",
    "            self.__cell_lines.pop()",
    "            self.__cell_lines += lines",
    "            text = '\\n'.join(self.__cell_lines)",
    "            html = HTML(f\"<pre>{text}</pre>\")",
    "            self.__last_handle.update(html)",
    "        else:",
    "            self.__cell_lines = lines",
    "            text = '\\n'.join(self.__cell_lines)",
    "            html = HTML(f\"<pre>{text}</pre>\")",
    "            self.__last_handle = display(html, display_id=self.__last_id)",
    "            self.__last_id += 1",
    "",
    "        # print(len(self.__cell_lines), self.__cell_lines[-1], is_new_line)",
    "        if is_new_line:",
    "            self.__cell_lines.append('')",
    "",
    "    def new_line(self):",
    "        self.__cell_lines.append('')",
    ""
  ],
  "lab/logger/indicators.py": [
    "from collections import deque",
    "from typing import Dict, Optional",
    "",
    "import numpy as np",
    "",
    "try:",
    "    import torch",
    "except ImportError:",
    "    torch = None",
    "",
    "",
    "def _to_numpy(value):",
    "    type_ = type(value)",
    "",
    "    if type_ == float or type_ == int:",
    "        return value",
    "",
    "    if type_ == np.ndarray:",
    "        return value",
    "",
    "    if torch is not None:",
    "        if type_ == torch.nn.parameter.Parameter:",
    "            return value.data.cpu().numpy()",
    "        if type_ == torch.Tensor:",
    "            return value.data.cpu().numpy()",
    "",
    "    assert False, f\"Unknown type {type_}\"",
    "",
    "",
    "class Indicator:",
    "    def __init__(self, *, name: str, is_print: bool):",
    "        self.is_print = is_print",
    "        self.name = name",
    "",
    "    def clear(self):",
    "        pass",
    "",
    "    def is_empty(self) -> bool:",
    "        raise NotImplementedError()",
    "",
    "    def to_dict(self) -> Dict:",
    "        return dict(class_name=self.__class__.__name__,",
    "                    name=self.name,",
    "                    is_print=self.is_print)",
    "",
    "    def collect_value(self, value):",
    "        raise NotImplementedError()",
    "",
    "    def get_mean(self) -> Optional[float]:",
    "        return None",
    "",
    "    def get_histogram(self):",
    "        return None",
    "",
    "    @property",
    "    def mean_key(self):",
    "        return f'{self.name}'",
    "",
    "    def get_index_mean(self):",
    "        return None, None",
    "",
    "",
    "class Queue(Indicator):",
    "    def __init__(self, name: str, queue_size=10, is_print=False):",
    "        super().__init__(name=name, is_print=is_print)",
    "        self._values = deque(maxlen=queue_size)",
    "",
    "    def collect_value(self, value):",
    "        self._values.append(_to_numpy(value))",
    "",
    "    def to_dict(self) -> Dict:",
    "        res = super().to_dict().copy()",
    "        res.update({'queue_size': self._values.maxlen})",
    "        return res",
    "",
    "    def is_empty(self) -> bool:",
    "        return len(self._values) == 0",
    "",
    "    def get_mean(self) -> float:",
    "        return float(np.mean(self._values))",
    "",
    "    def get_histogram(self):",
    "        return self._values",
    "",
    "    @property",
    "    def mean_key(self):",
    "        return f'{self.name}.mean'",
    "",
    "",
    "class _Collection(Indicator):",
    "    def __init__(self, name: str, is_print=False):",
    "        super().__init__(name=name, is_print=is_print)",
    "        self._values = []",
    "",
    "    def collect_value(self, value):",
    "        self._values.append(_to_numpy(value))",
    "",
    "    def clear(self):",
    "        self._values = []",
    "",
    "    def is_empty(self) -> bool:",
    "        return len(self._values) == 0",
    "",
    "    def get_mean(self) -> float:",
    "        return float(np.mean(self._values))",
    "",
    "    def get_histogram(self):",
    "        return self._values",
    "",
    "",
    "class Histogram(_Collection):",
    "    @property",
    "    def mean_key(self):",
    "        return f'{self.name}.mean'",
    "",
    "",
    "class Scalar(_Collection):",
    "    def get_histogram(self):",
    "        return None",
    "",
    "",
    "class _IndexedCollection(Indicator):",
    "    def __init__(self, name: str):",
    "        super().__init__(name=name, is_print=False)",
    "        self._values = []",
    "        self._indexes = []",
    "",
    "    def clear(self):",
    "        self._values = []",
    "        self._indexes = []",
    "",
    "    def collect_value(self, value):",
    "        if type(value) == tuple:",
    "            assert len(value) == 2",
    "            if type(value[0]) == int:",
    "                self._indexes.append(value[0])",
    "                self._values.append(value[1])",
    "            else:",
    "                assert type(value[0]) == list",
    "                assert len(value[0]) == len(value[1])",
    "                self._indexes += value[0]",
    "                self._values += value[1]",
    "        else:",
    "            assert type(value) == list",
    "            self._indexes += [v[0] for v in value]",
    "            self._values += [v[1] for v in value]",
    "",
    "    def is_empty(self) -> bool:",
    "        return len(self._values) == 0",
    "",
    "    def get_mean(self) -> float:",
    "        return float(np.mean(self._values))",
    "",
    "    def get_index_mean(self):",
    "        summary = {}",
    "        for ind, values in zip(self._indexes, self._values):",
    "            if ind not in summary:",
    "                summary[ind] = []",
    "            summary[ind].append(values)",
    "",
    "        indexes = []",
    "        means = []",
    "        for ind, values in summary.items():",
    "            indexes.append(ind)",
    "            means.append(float(np.mean(values)))",
    "",
    "        return indexes, means",
    "",
    "",
    "class IndexedScalar(_IndexedCollection):",
    "    def get_histogram(self):",
    "        return None",
    ""
  ],
  "lab/logger/internal.py": [
    "import pathlib",
    "import typing",
    "from pathlib import PurePath",
    "from typing import List, Optional, Tuple, Union",
    "",
    "import numpy as np",
    "",
    "from .colors import Text, StyleCode",
    "from .delayed_keyboard_interrupt import DelayedKeyboardInterrupt",
    "from .destinations.factory import create_destination",
    "from .indicators import Indicator",
    "from .iterator import Iterator",
    "from .loop import Loop",
    "from .sections import Section, OuterSection",
    "from .store import Store",
    "from .writers import Writer, ScreenWriter",
    "",
    "",
    "class CheckpointSaver:",
    "    def save(self, global_step):",
    "        raise NotImplementedError()",
    "",
    "",
    "class LoggerInternal:",
    "    \"\"\"",
    "    ## 🖨 Logger class",
    "    \"\"\"",
    "",
    "    def __init__(self):",
    "        \"\"\"",
    "        ### Initializer",
    "        \"\"\"",
    "        self.__store = Store(self)",
    "        self.__writers: List[Writer] = []",
    "",
    "        self.__loop: Optional[Loop] = None",
    "        self.__sections: List[Section] = []",
    "",
    "        self.__indicators_print = []",
    "",
    "        self.__screen_writer = ScreenWriter()",
    "        self.__writers.append(self.__screen_writer)",
    "",
    "        self.__checkpoint_saver: Optional[CheckpointSaver] = None",
    "",
    "        self.__start_global_step: Optional[int] = None",
    "        self.__global_step: Optional[int] = None",
    "        self.__last_global_step: Optional[int] = None",
    "",
    "        self.__data_path = None",
    "        self.__numpy_path = None",
    "",
    "        self.__destination = create_destination()",
    "",
    "    def set_data_path(self, data_path: PurePath):",
    "        self.__data_path = data_path",
    "",
    "    def get_data_path(self) -> PurePath:",
    "        return self.__data_path",
    "",
    "    def set_numpy_path(self, numpy_path: PurePath):",
    "        self.__numpy_path = numpy_path",
    "",
    "    def save_numpy(self, name: str, array: np.ndarray):",
    "        \"\"\"",
    "        ## Save a single numpy array",
    "",
    "        This is used to save processed data",
    "        \"\"\"",
    "        numpy_path = pathlib.Path(self.__numpy_path)",
    "        if not numpy_path.exists():",
    "            numpy_path.mkdir(parents=True)",
    "        file_name = name + \".npy\"",
    "        np.save(str(numpy_path / file_name), array)",
    "",
    "    def set_checkpoint_saver(self, saver: CheckpointSaver):",
    "        self.__checkpoint_saver = saver",
    "",
    "    @property",
    "    def global_step(self) -> int:",
    "        if self.__global_step is not None:",
    "            return self.__global_step",
    "",
    "        global_step = 0",
    "        if self.__start_global_step is not None:",
    "            global_step = self.__start_global_step",
    "",
    "        if self.__is_looping():",
    "            return global_step + self.__loop.counter",
    "",
    "        if self.__last_global_step is not None:",
    "            return self.__last_global_step",
    "",
    "        return global_step",
    "",
    "    def add_writer(self, writer: Writer):",
    "        self.__writers.append(writer)",
    "",
    "    def log(self, parts: List[Union[str, Tuple[str, StyleCode]]], *,",
    "            is_new_line=True):",
    "        \"\"\"",
    "        ### Print a message with different colors.",
    "        \"\"\"",
    "",
    "        self.__destination.log(parts, is_new_line=is_new_line)",
    "",
    "    def add_indicator(self, indicator: Indicator):",
    "        self.__store.add_indicator(indicator)",
    "",
    "    def save_indicators(self, file: PurePath):",
    "        self.__store.save_indicators(file)",
    "",
    "    def store(self, *args, **kwargs):",
    "        \"\"\"",
    "        ### Stores a value in the logger.",
    "",
    "        This may be added to a queue, a list or stored as",
    "        a TensorBoard histogram depending on the",
    "        type of the indicator.",
    "        \"\"\"",
    "",
    "        self.__store.store(*args, **kwargs)",
    "",
    "    def set_global_step(self, global_step):",
    "        self.__global_step = global_step",
    "",
    "    def set_start_global_step(self, global_step):",
    "        self.__start_global_step = global_step",
    "",
    "    def add_global_step(self, global_step: int = 1):",
    "        if self.__global_step is None:",
    "            if self.__start_global_step is not None:",
    "                self.__global_step = self.__start_global_step",
    "            else:",
    "                self.__global_step = 0",
    "",
    "        self.__global_step += global_step",
    "",
    "    def new_line(self):",
    "        self.__destination.new_line()",
    "",
    "    def __is_looping(self):",
    "        if self.__loop is not None and self.__loop.is_started:",
    "            return True",
    "        else:",
    "            return False",
    "",
    "    def write(self):",
    "        \"\"\"",
    "        ### Output the stored log values to screen and TensorBoard summaries.",
    "        \"\"\"",
    "",
    "        global_step = self.global_step",
    "",
    "        for w in self.__writers:",
    "            self.__store.write(w, global_step)",
    "        self.__indicators_print = self.__store.write(self.__screen_writer, global_step)",
    "        self.__store.clear()",
    "",
    "        parts = [(f\"{self.global_step :8,}:  \", Text.highlight)]",
    "        if self.__is_looping():",
    "            self.__log_looping_line()",
    "        else:",
    "            parts += self.__indicators_print",
    "            self.log(parts, is_new_line=False)",
    "",
    "    def save_checkpoint(self):",
    "        if self.__checkpoint_saver is None:",
    "            return",
    "",
    "        self.__checkpoint_saver.save(self.global_step)",
    "",
    "    def iterate(self, name, iterable: Union[typing.Iterable, typing.Sized, int],",
    "                total_steps: Optional[int] = None, *,",
    "                is_silent: bool = False,",
    "                is_timed: bool = True):",
    "        return Iterator(logger=self,",
    "                        name=name,",
    "                        iterable=iterable,",
    "                        is_silent=is_silent,",
    "                        is_timed=is_timed,",
    "                        total_steps=total_steps,",
    "                        is_enumarate=False)",
    "",
    "    def enum(self, name, iterable: typing.Sized, *,",
    "             is_silent: bool = False,",
    "             is_timed: bool = True):",
    "        return Iterator(logger=self,",
    "                        name=name,",
    "                        iterable=iterable,",
    "                        is_silent=is_silent,",
    "                        is_timed=is_timed,",
    "                        total_steps=None,",
    "                        is_enumarate=True)",
    "",
    "    def section(self, name, *,",
    "                is_silent: bool = False,",
    "                is_timed: bool = True,",
    "                is_partial: bool = False,",
    "                is_new_line: bool = True,",
    "                total_steps: float = 1.0):",
    "",
    "        if self.__is_looping():",
    "            if len(self.__sections) != 0:",
    "                raise RuntimeError(\"No nested sections within loop\")",
    "",
    "            section = self.__loop.get_section(name=name,",
    "                                              is_silent=is_silent,",
    "                                              is_timed=is_timed,",
    "                                              is_partial=is_partial,",
    "                                              total_steps=total_steps)",
    "            self.__sections.append(section)",
    "        else:",
    "            self.__sections.append(OuterSection(logger=self,",
    "                                                name=name,",
    "                                                is_silent=is_silent,",
    "                                                is_timed=is_timed,",
    "                                                is_partial=is_partial,",
    "                                                is_new_line=is_new_line,",
    "                                                total_steps=total_steps,",
    "                                                level=len(self.__sections)))",
    "",
    "        return self.__sections[-1]",
    "",
    "    def progress(self, steps: float):",
    "        if len(self.__sections) == 0:",
    "            raise RuntimeError(\"You must be within a section to report progress\")",
    "",
    "        if self.__sections[-1].progress(steps):",
    "            self.__log_line()",
    "",
    "    def set_successful(self, is_successful=True):",
    "        if len(self.__sections) == 0:",
    "            raise RuntimeError(\"You must be within a section to report success\")",
    "",
    "        self.__sections[-1].is_successful = is_successful",
    "        self.__log_line()",
    "",
    "    def loop(self, iterator_: range, *,",
    "             is_print_iteration_time=True):",
    "        if len(self.__sections) != 0:",
    "            raise RuntimeError(\"Cannot start a loop within a section\")",
    "",
    "        self.__loop = Loop(iterator=iterator_, logger=self,",
    "                           is_print_iteration_time=is_print_iteration_time)",
    "        return self.__loop",
    "",
    "    def finish_loop(self):",
    "        if len(self.__sections) != 0:",
    "            raise RuntimeError(\"Cannot be within a section when finishing the loop\")",
    "        self.__last_global_step = self.global_step",
    "        self.__loop = None",
    "",
    "    def section_enter(self, section):",
    "        if len(self.__sections) == 0:",
    "            raise RuntimeError(\"Entering a section without creating a section.\\n\"",
    "                               \"Always use logger.section to create a section\")",
    "",
    "        if section is not self.__sections[-1]:",
    "            raise RuntimeError(\"Entering a section other than the one last_created\\n\"",
    "                               \"Always user with logger.section(...):\")",
    "",
    "        if len(self.__sections) > 1 and not self.__sections[-2].is_parented:",
    "            self.__sections[-2].make_parent()",
    "            self.new_line()",
    "",
    "        self.__log_line()",
    "",
    "    def __log_looping_line(self):",
    "        parts = [(f\"{self.global_step :8,}:  \", Text.highlight)]",
    "        parts += self.__loop.log_sections()",
    "        parts += self.__indicators_print",
    "        parts += self.__loop.log_progress()",
    "",
    "        self.log(parts, is_new_line=False)",
    "",
    "    def __log_line(self):",
    "        if self.__is_looping():",
    "            self.__log_looping_line()",
    "            return",
    "",
    "        if len(self.__sections) == 0:",
    "            return",
    "",
    "        self.log(self.__sections[-1].log(), is_new_line=False)",
    "",
    "    def section_exit(self, section):",
    "        if len(self.__sections) == 0:",
    "            raise RuntimeError(\"Impossible\")",
    "",
    "        if section is not self.__sections[-1]:",
    "            raise RuntimeError(\"Impossible\")",
    "",
    "        self.__log_line()",
    "        self.__sections.pop(-1)",
    "",
    "    def delayed_keyboard_interrupt(self):",
    "        \"\"\"",
    "        ### Create a section with a delayed keyboard interrupt",
    "        \"\"\"",
    "        return DelayedKeyboardInterrupt(self)",
    "",
    "    def _log_key_value(self, items: List[Tuple[any, any]], is_show_count=True):",
    "        max_key_len = 0",
    "        for k, v in items:",
    "            max_key_len = max(max_key_len, len(str(k)))",
    "",
    "        count = 0",
    "        for k, v in items:",
    "            count += 1",
    "            spaces = \" \" * (max_key_len - len(str(k)))",
    "            self.log([(f\"{spaces}{k}: \", Text.key),",
    "                      (str(v), Text.value)])",
    "",
    "        if is_show_count:",
    "            self.log([",
    "                \"Total \",",
    "                (str(count), Text.meta),",
    "                \" item(s)\"])",
    "",
    "    def info(self, *args, **kwargs):",
    "        \"\"\"",
    "        ### 🎨 Pretty prints a set of values.",
    "        \"\"\"",
    "",
    "        if len(args) == 0:",
    "            self._log_key_value([(k, v) for k, v in kwargs.items()], False)",
    "        elif len(args) == 1:",
    "            assert len(kwargs.keys()) == 0",
    "            arg = args[0]",
    "            if type(arg) == list:",
    "                self._log_key_value([(i, v) for i, v in enumerate(arg)])",
    "            elif type(arg) == dict:",
    "                self._log_key_value([(k, v) for k, v in arg.items()])",
    "        else:",
    "            assert len(kwargs.keys()) == 0",
    "            self._log_key_value([(i, v) for i, v in enumerate(args)], False)",
    ""
  ],
  "lab/logger/iterator.py": [
    "import typing",
    "from typing import Optional, Iterable",
    "",
    "from lab.logger import internal",
    "",
    "",
    "class Iterator:",
    "    def __init__(self, *,",
    "                 logger: 'internal.LoggerInternal',",
    "                 name: str,",
    "                 iterable: typing.Union[Iterable, typing.Sized, int],",
    "                 is_silent: bool,",
    "                 is_timed: bool,",
    "                 total_steps: Optional[int],",
    "                 is_enumarate: bool):",
    "        if is_enumarate:",
    "            total_steps = len(iterable)",
    "            iterable = enumerate(iterable)",
    "        if type(iterable) is int:",
    "            total_steps = iterable",
    "            iterable = range(total_steps)",
    "        if total_steps is None:",
    "            sized: typing.Sized = iterable",
    "            total_steps = len(sized)",
    "",
    "        self._logger = logger",
    "        self._name = name",
    "        self._iterable: Iterable = iterable",
    "        self._iterator = Optional[typing.Iterator]",
    "        self._total_steps = total_steps",
    "        self._section = None",
    "        self._is_silent = is_silent",
    "        self._is_timed = is_timed",
    "        self._counter = -1",
    "",
    "    def __iter__(self):",
    "        self._section = self._logger.section(",
    "            self._name,",
    "            is_silent=self._is_silent,",
    "            is_timed=self._is_timed,",
    "            is_partial=False,",
    "            total_steps=self._total_steps)",
    "        self._iterator = iter(self._iterable)",
    "        self._section.__enter__()",
    "",
    "        return self",
    "",
    "    def __next__(self):",
    "        try:",
    "            self._counter += 1",
    "            self._logger.progress(self._counter)",
    "            next_value = next(self._iterator)",
    "        except StopIteration as e:",
    "            self._section.__exit__(None, None, None)",
    "            raise e",
    "",
    "        return next_value",
    ""
  ],
  "lab/logger/loop.py": [
    "import math",
    "import time",
    "from typing import Optional, Dict",
    "",
    "from lab.logger import internal",
    "from lab.logger.sections import LoopingSection",
    "from .colors import Text",
    "",
    "",
    "class Loop:",
    "    def __init__(self, iterator: range, *,",
    "                 logger: 'internal.LoggerInternal',",
    "                 is_print_iteration_time: bool):",
    "        \"\"\"",
    "        Creates an iterator with a range `iterator`.",
    "",
    "        See example for usage.",
    "        \"\"\"",
    "        self.iterator = iterator",
    "        self._start_time = 0.",
    "        self._iter_start_time = 0.",
    "        self._init_time = 0.",
    "        self._iter_time = 0.",
    "        self._beta_pow = 1.",
    "        self._beta = 0.9",
    "        self.steps = len(iterator)",
    "        self.counter = 0",
    "        self.logger = logger",
    "        self.__global_step: Optional[int] = None",
    "        self.__looping_sections: Dict[str, LoopingSection] = {}",
    "        self._is_print_iteration_time = is_print_iteration_time",
    "        self.is_started = False",
    "",
    "    def __iter__(self):",
    "        self.is_started = True",
    "        self.iterator_iter = iter(self.iterator)",
    "        self._start_time = time.time()",
    "        self._init_time = 0.",
    "        self._iter_time = 0.",
    "        self.counter = 0",
    "        return self",
    "",
    "    def __next__(self):",
    "        try:",
    "            next_value = next(self.iterator_iter)",
    "        except StopIteration as e:",
    "            self.logger.finish_loop()",
    "            raise e",
    "",
    "        now = time.time()",
    "        if self.counter == 0:",
    "            self.__init_time = now - self._start_time",
    "        else:",
    "            self._beta_pow *= self._beta",
    "            self._iter_time *= self._beta",
    "            self._iter_time += (1 - self._beta) * (now - self._iter_start_time)",
    "",
    "        self._iter_start_time = now",
    "",
    "        self.counter = next_value",
    "",
    "        return next_value",
    "",
    "    def log_progress(self):",
    "        \"\"\"",
    "        Show progress",
    "        \"\"\"",
    "        now = time.time()",
    "        spent = now - self._start_time",
    "",
    "        if not math.isclose(self._iter_time, 0.):",
    "            estimate = self._iter_time / (1 - self._beta_pow)",
    "        else:",
    "            estimate = sum([s.get_estimated_time()",
    "                            for s in self.__looping_sections.values()])",
    "",
    "        total_time = estimate * self.steps + self._init_time",
    "        total_time = max(total_time, spent)",
    "        remain = total_time - spent",
    "",
    "        remain /= 60",
    "        spent /= 60",
    "        estimate *= 1000",
    "",
    "        spent_h = int(spent // 60)",
    "        spent_m = int(spent % 60)",
    "        remain_h = int(remain // 60)",
    "        remain_m = int(remain % 60)",
    "",
    "        to_print = [(\"  \", None)]",
    "        if self._is_print_iteration_time:",
    "            to_print.append((f\"{estimate:,.0f}ms\", Text.meta))",
    "        to_print.append((f\"{spent_h:3d}:{spent_m:02d}m/{remain_h:3d}:{remain_m:02d}m  \",",
    "                         Text.meta2))",
    "",
    "        return to_print",
    "",
    "    def get_section(self, *, name: str,",
    "                    is_silent: bool,",
    "                    is_timed: bool,",
    "                    is_partial: bool,",
    "                    total_steps: float):",
    "        if name not in self.__looping_sections:",
    "            self.__looping_sections[name] = LoopingSection(logger=self.logger,",
    "                                                           name=name,",
    "                                                           is_silent=is_silent,",
    "                                                           is_timed=is_timed,",
    "                                                           is_partial=is_partial,",
    "                                                           total_steps=total_steps)",
    "        return self.__looping_sections[name]",
    "",
    "    def log_sections(self):",
    "        parts = []",
    "        for name, section in self.__looping_sections.items():",
    "            parts += section.log()",
    "",
    "        return parts",
    ""
  ],
  "lab/logger/sections.py": [
    "import math",
    "import time",
    "",
    "from . import internal",
    "from .colors import Text",
    "",
    "",
    "class Section:",
    "    def __init__(self, *,",
    "                 logger: 'internal.LoggerInternal',",
    "                 name: str,",
    "                 is_silent: bool,",
    "                 is_timed: bool,",
    "                 is_partial: bool,",
    "                 total_steps: float):",
    "        self._logger = logger",
    "        self._name = name",
    "        self._is_silent = is_silent",
    "        self._is_timed = is_timed",
    "        self._is_partial = is_partial",
    "        self._total_steps = total_steps",
    "",
    "        self._state = 'none'",
    "        self._has_entered_ever = False",
    "",
    "        self._start_time = 0",
    "        self._end_time = -1",
    "        self._progress = 0.",
    "        self._start_progress = 0",
    "        self._end_progress = 0",
    "        self._is_parented = False",
    "",
    "        self.is_successful = True",
    "",
    "    def get_estimated_time(self):",
    "        raise NotImplementedError()",
    "",
    "    def __enter__(self):",
    "        self._state = 'entered'",
    "        self._has_entered_ever = True",
    "        self.is_successful = True",
    "",
    "        if not self._is_partial:",
    "            self._progress = 0",
    "",
    "        self._start_progress = self._progress",
    "",
    "        if self._is_timed:",
    "            self._start_time = time.time()",
    "",
    "        self._logger.section_enter(self)",
    "",
    "        return self",
    "",
    "    def __exit__(self, exc_type, exc_val, exc_tb):",
    "        self._state = 'exited'",
    "        if self._is_timed:",
    "            self._end_time = time.time()",
    "",
    "        if not self._is_partial:",
    "            self._progress = 1.",
    "",
    "        self._end_progress = self._progress",
    "",
    "        self._logger.section_exit(self)",
    "",
    "    def log(self):",
    "        raise NotImplementedError()",
    "",
    "    def progress(self, steps):",
    "        old_progress = self._progress",
    "        self._progress = steps / self._total_steps",
    "",
    "        if self._is_silent:",
    "            return False",
    "",
    "        if math.floor(self._progress * 100) != math.floor(old_progress * 100):",
    "            return True",
    "        else:",
    "            return False",
    "",
    "    @property",
    "    def is_parented(self):",
    "        return self._is_parented",
    "",
    "    def make_parent(self):",
    "        self._is_parented = True",
    "",
    "",
    "class OuterSection(Section):",
    "    def __init__(self, *,",
    "                 logger: 'internal.LoggerInternal',",
    "                 name: str,",
    "                 is_silent: bool,",
    "                 is_timed: bool,",
    "                 is_partial: bool,",
    "                 is_new_line: bool,",
    "                 total_steps: float,",
    "                 level: int):",
    "        if is_partial:",
    "            raise RuntimeError(\"Only sections within the loop can be partial.\")",
    "",
    "        super().__init__(logger=logger,",
    "                         name=name,",
    "                         is_silent=is_silent,",
    "                         is_timed=is_timed,",
    "                         is_partial=is_partial,",
    "                         total_steps=total_steps)",
    "",
    "        self._level = level",
    "        self._is_new_line = is_new_line",
    "",
    "    def get_estimated_time(self):",
    "        if self._state is 'entered':",
    "            if self._progress == 0.:",
    "                return time.time() - self._start_time",
    "            else:",
    "                return (time.time() - self._start_time) / self._progress",
    "        else:",
    "            return self._end_time - self._start_time",
    "",
    "    def log(self):",
    "        if self._is_silent:",
    "            return",
    "",
    "        if self._state is 'none':",
    "            return",
    "",
    "        parts = [(\"  \" * self._level + f\"{self._name}\", None)]",
    "",
    "        if self._state == 'entered':",
    "            if self._progress == 0.:",
    "                parts.append(\"...\")",
    "            else:",
    "                parts.append((f\" {math.floor(self._progress * 100) :4.0f}%\", Text.meta2))",
    "        else:",
    "            if self.is_successful:",
    "                parts.append((\"...[DONE]\", Text.success))",
    "            else:",
    "                parts.append((\"...[FAIL]\", Text.danger))",
    "",
    "        if self._is_timed and self._progress > 0.:",
    "            duration_ms = 1000 * self.get_estimated_time()",
    "            parts.append((f\"\\t{duration_ms :,.2f}ms\",",
    "                          Text.meta))",
    "",
    "        if self._state != 'entered' and self._is_new_line:",
    "            parts.append((\"\\n\", None))",
    "",
    "        return parts",
    "",
    "",
    "class LoopingSection(Section):",
    "    def __init__(self, *,",
    "                 logger: 'internal.LoggerInternal',",
    "                 name: str,",
    "                 is_silent: bool,",
    "                 is_timed: bool,",
    "                 is_partial: bool,",
    "                 total_steps: float):",
    "        super().__init__(logger=logger,",
    "                         name=name,",
    "                         is_silent=is_silent,",
    "                         is_timed=is_timed,",
    "                         is_partial=is_partial,",
    "                         total_steps=total_steps)",
    "        self._beta_pow = 1.",
    "        self._beta = 0.9",
    "        self._estimated_time = 0.",
    "        self._time_length = 7",
    "        self._last_end_time = -1.",
    "        self._last_start_time = -1.",
    "        self._last_step_time = 0.",
    "",
    "    def get_estimated_time(self):",
    "        et = self._estimated_time * self._beta",
    "        et += (1 - self._beta) * self._last_step_time",
    "        return et / (1 - self._beta_pow * self._beta)",
    "",
    "    def _calc_estimated_time(self):",
    "        if self._state != 'entered':",
    "            if self._last_end_time == self._end_time:",
    "                return self.get_estimated_time()",
    "            end_time = self._end_time",
    "            end_progress = self._end_progress",
    "            self._last_end_time = self._end_time",
    "        else:",
    "            end_time = time.time()",
    "            end_progress = self._progress",
    "",
    "        if end_progress - self._start_progress < 1e-6:",
    "            return self.get_estimated_time()",
    "",
    "        current_estimate = ((end_time - self._start_time) /",
    "                            (end_progress - self._start_progress))",
    "",
    "        if self._last_start_time == self._start_time:",
    "            # print(current_estimate)",
    "            self._last_step_time = current_estimate",
    "        else:",
    "            if self._last_step_time >= 0.:",
    "                self._beta_pow *= self._beta",
    "                self._estimated_time *= self._beta",
    "                self._estimated_time += (1 - self._beta) * self._last_step_time",
    "            # print(self._last_step_time, current_estimate)",
    "            self._last_step_time = current_estimate",
    "            self._last_start_time = self._start_time",
    "",
    "        return self.get_estimated_time()",
    "",
    "    def log(self):",
    "        if self._is_silent:",
    "            return []",
    "",
    "        if self._state == 'none':",
    "            return []",
    "",
    "        parts = [(f\"{self._name}:\", None)]",
    "        color = None",
    "",
    "        if not self.is_successful:",
    "            color = Text.danger",
    "",
    "        if self._progress == 0.:",
    "            parts.append((\"  ...\", Text.subtle))",
    "        else:",
    "            parts.append((f\"{math.floor(self._progress * 100) :4.0f}%\",",
    "                          color or Text.subtle))",
    "",
    "        if self._is_timed:",
    "            duration_ms = 1000 * self._calc_estimated_time()",
    "            s = f\" {duration_ms:,.0f}ms  \"",
    "            tl = len(s)",
    "            if tl > self._time_length:",
    "                self._time_length = tl",
    "            else:",
    "                s = (\" \" * (self._time_length - tl)) + s",
    "",
    "            parts.append((s, color or Text.meta))",
    "",
    "        return parts",
    ""
  ],
  "lab/logger/store.py": [
    "from pathlib import PurePath",
    "from typing import Dict, List",
    "",
    "from lab import util",
    "from lab.logger import internal",
    "from .indicators import Indicator, Scalar",
    "from .writers import Writer",
    "",
    "",
    "class Store:",
    "    indicators: Dict[str, Indicator]",
    "",
    "    def __init__(self, logger: 'internal.LoggerInternal'):",
    "        self.values = {}",
    "        # self.queues = {}",
    "        # self.histograms = {}",
    "        # self.pairs: Dict[str, List[Tuple[int, int]]] = {}",
    "        # self.scalars = {}",
    "        self.__logger = logger",
    "        self.indicators = {}",
    "        self.__indicators_file = None",
    "",
    "    def save_indicators(self, file: PurePath):",
    "        self.__indicators_file = file",
    "",
    "        indicators = {k: ind.to_dict() for k, ind in self.indicators.items()}",
    "        with open(str(file), \"w\") as file:",
    "            file.write(util.yaml_dump(indicators))",
    "",
    "    def add_indicator(self, indicator: Indicator):",
    "        \"\"\"",
    "        ### Add an indicator",
    "        \"\"\"",
    "",
    "        assert indicator.name not in self.indicators, f\"{indicator.name} already used\"",
    "",
    "        self.indicators[indicator.name] = indicator",
    "",
    "        indicator.clear()",
    "",
    "        if self.__indicators_file is not None:",
    "            self.save_indicators(self.__indicators_file)",
    "",
    "    def _store_list(self, items: List[Dict[str, float]]):",
    "        for item in items:",
    "            self.store(**item)",
    "",
    "    def _store_kv(self, k, v):",
    "        if k not in self.indicators:",
    "            self.__logger.add_indicator(Scalar(k, True))",
    "",
    "        self.indicators[k].collect_value(v)",
    "",
    "    def _store_kvs(self, **kwargs):",
    "        for k, v in kwargs.items():",
    "            self._store_kv(k, v)",
    "",
    "    def store(self, *args, **kwargs):",
    "        \"\"\"",
    "        ### Stores a value in the logger.",
    "",
    "        This may be added to a queue, a list or stored as",
    "        a TensorBoard histogram depending on the",
    "        type of the indicator.",
    "        \"\"\"",
    "        assert len(args) <= 2",
    "",
    "        if len(args) == 0:",
    "            self._store_kvs(**kwargs)",
    "        elif len(args) == 1:",
    "            assert not kwargs",
    "            assert isinstance(args[0], list)",
    "            self._store_list(args[0])",
    "        elif len(args) == 2:",
    "            assert isinstance(args[0], str)",
    "            if isinstance(args[1], list):",
    "                for v in args[1]:",
    "                    self._store_kv(args[0], v)",
    "            else:",
    "                self._store_kv(args[0], args[1])",
    "",
    "    def clear(self):",
    "        for k, v in self.indicators.items():",
    "            v.clear()",
    "",
    "    def write(self, writer: Writer, global_step):",
    "        return writer.write(global_step=global_step,",
    "                            indicators=self.indicators)",
    ""
  ],
  "lab/logger/util/__init__.py": [
    ""
  ],
  "lab/logger/util/pytorch.py": [
    "import torch",
    "",
    "from lab import logger",
    "from lab.logger.indicators import Histogram",
    "",
    "",
    "def add_model_indicators(model: torch.nn.Module, model_name: str = \"model\"):",
    "    for name, param in model.named_parameters():",
    "        if param.requires_grad:",
    "            logger.add_indicator(Histogram(f\"{model_name}.{name}\"))",
    "            logger.add_indicator(Histogram(f\"{model_name}.{name}.grad\"))",
    "",
    "",
    "def store_model_indicators(model: torch.nn.Module, model_name: str = \"model\"):",
    "    for name, param in model.named_parameters():",
    "        if param.requires_grad:",
    "            logger.store(f\"{model_name}.{name}\", param)",
    "            logger.store(f\"{model_name}.{name}.grad\", param.grad)",
    ""
  ],
  "lab/logger/writers/__init__.py": [
    "from typing import Dict",
    "",
    "import numpy as np",
    "",
    "from ..colors import Text",
    "from ..indicators import Indicator",
    "",
    "",
    "class Writer:",
    "    def write(self, *,",
    "              global_step: int,",
    "              indicators: Dict[str, Indicator]):",
    "        raise NotImplementedError()",
    "",
    "",
    "class ScreenWriter(Writer):",
    "    def __init__(self):",
    "        super().__init__()",
    "",
    "        self._estimates = {}",
    "        self._beta = 0.9",
    "        self._beta_pow = {}",
    "",
    "    def update_estimate(self, k, v):",
    "        if k not in self._estimates:",
    "            self._estimates[k] = 0",
    "            self._beta_pow[k] = 1.",
    "",
    "        self._estimates[k] *= self._beta",
    "        self._estimates[k] += (1 - self._beta) * v",
    "        self._beta_pow[k] *= self._beta",
    "",
    "    def get_empty_string(self, length, decimals):",
    "        return ' ' * (length - 2 - decimals) + '-.' + '-' * decimals",
    "",
    "    def get_value_string(self, k, v):",
    "        if k not in self._estimates:",
    "            assert v is None",
    "            return self.get_empty_string(8, 2)",
    "",
    "        estimate = self._estimates[k] / (1 - self._beta_pow[k])",
    "        if abs(estimate) < 1e-9:",
    "            lg = 0",
    "        else:",
    "            lg = int(np.ceil(np.log10(estimate))) + 1",
    "",
    "        decimals = 7 - lg",
    "        decimals = max(1, decimals)",
    "        decimals = min(6, decimals)",
    "",
    "        fmt = \"{v:8.\" + str(decimals) + \"f}\"",
    "        if v is None:",
    "            return self.get_empty_string(8, decimals)",
    "        else:",
    "            return fmt.format(v=v)",
    "",
    "    def write(self, *,",
    "              global_step: int,",
    "              indicators: Dict[str, Indicator]):",
    "        parts = []",
    "",
    "        for ind in indicators.values():",
    "            if not ind.is_print:",
    "                continue",
    "",
    "            parts.append((f\" {ind.name}: \", None))",
    "",
    "            if ind.is_empty():",
    "                value = self.get_value_string(ind.name, None)",
    "            else:",
    "                v = ind.get_mean()",
    "                self.update_estimate(ind.name, v)",
    "                value = self.get_value_string(ind.name, v)",
    "",
    "            parts.append((value, Text.value))",
    "",
    "        return parts",
    ""
  ],
  "lab/logger/writers/sqlite.py": [
    "import sqlite3",
    "from pathlib import PurePath",
    "from typing import Dict, Optional",
    "",
    "from . import Writer as WriteBase",
    "from ..indicators import Indicator",
    "",
    "",
    "class Writer(WriteBase):",
    "    conn: Optional[sqlite3.Connection]",
    "",
    "    def __init__(self, sqlite_path: PurePath):",
    "        super().__init__()",
    "",
    "        self.sqlite_path = sqlite_path",
    "        self.conn = None",
    "",
    "    def __connect(self):",
    "        if self.conn is not None:",
    "            return",
    "",
    "        self.conn = sqlite3.connect(str(self.sqlite_path))",
    "",
    "        try:",
    "            self.conn.execute(f\"CREATE TABLE scalars \"",
    "                              f\"(indicator text, step integer, value real)\")",
    "            self.conn.execute(f\"CREATE TABLE indexed_scalars \"",
    "                              f\"(indicator text, step integer, idx integer, value real)\")",
    "",
    "        except sqlite3.OperationalError:",
    "            print('Scalar table exists')",
    "",
    "    @staticmethod",
    "    def _parse_key(key: str):",
    "        return key",
    "        # if we name tables",
    "        # return key.replace('.', '_')",
    "",
    "    def write(self, *,",
    "              global_step: int,",
    "              indicators: Dict[str, Indicator]):",
    "        self.__connect()",
    "",
    "        for ind in indicators.values():",
    "            if ind.is_empty():",
    "                continue",
    "",
    "            value = ind.get_mean()",
    "            if value is not None:",
    "                key = self._parse_key(ind.mean_key)",
    "                self.conn.execute(",
    "                    f\"INSERT INTO scalars VALUES (?, ?, ?)\",",
    "                    (key, global_step, value))",
    "",
    "            idx, value = ind.get_index_mean()",
    "            if idx is not None:",
    "                key = self._parse_key(ind.mean_key)",
    "                data = [(key, global_step, i, v) for i, v in zip(idx, value)]",
    "                self.conn.executemany(",
    "                    f\"INSERT INTO indexed_scalars VALUES (?, ?, ?, ?)\",",
    "                    data)",
    "",
    "        self.conn.commit()",
    ""
  ],
  "lab/logger/writers/tensorboard.py": [
    "# import os",
    "from pathlib import PurePath",
    "from typing import Dict",
    "",
    "import tensorflow as tf",
    "",
    "from . import Writer as WriteBase",
    "from ..indicators import Indicator",
    "",
    "tf.config.experimental.set_visible_devices([], \"GPU\")",
    "",
    "",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"",
    "",
    "",
    "class Writer(WriteBase):",
    "    def __init__(self, log_path: PurePath):",
    "        super().__init__()",
    "",
    "        self.__log_path = log_path",
    "        self.__writer = None",
    "",
    "    def __connect(self):",
    "        if self.__writer is not None:",
    "            return",
    "",
    "        self.__writer = tf.summary.create_file_writer(str(self.__log_path))",
    "",
    "    @staticmethod",
    "    def _parse_key(key: str):",
    "        return key.replace('.', '/')",
    "",
    "    def write(self, *,",
    "              global_step: int,",
    "              indicators: Dict[str, Indicator]):",
    "        self.__connect()",
    "",
    "        with self.__writer.as_default():",
    "            for ind in indicators.values():",
    "                if ind.is_empty():",
    "                    continue",
    "",
    "                hist_data = ind.get_histogram()",
    "                if hist_data is not None:",
    "                    tf.summary.histogram(self._parse_key(ind.name), hist_data, step=global_step)",
    "",
    "                mean_value = ind.get_mean()",
    "                if mean_value is not None:",
    "                    tf.summary.scalar(self._parse_key(ind.mean_key), mean_value,",
    "                                      step=global_step)",
    ""
  ],
  "lab/training_loop.py": [
    "import signal",
    "from typing import Any, Tuple, Optional",
    "",
    "from . import logger",
    "from .logger.colors import Text",
    "from .configs import Configs",
    "from .logger.loop import Loop",
    "",
    "",
    "class TrainingLoopConfigs(Configs):",
    "    loop_count: int = 10",
    "    loop_step: int = 1",
    "    is_save_models: bool = False",
    "    log_new_line_interval: int = 1",
    "    log_write_interval: int = 1",
    "    save_models_interval: int = 1",
    "    is_loop_on_interrupt: bool = True",
    "",
    "    training_loop: 'TrainingLoop'",
    "",
    "",
    "@TrainingLoopConfigs.calc(TrainingLoopConfigs.training_loop)",
    "def _loop_configs(c: TrainingLoopConfigs):",
    "    return TrainingLoop(loop_count=c.loop_count,",
    "                        loop_step=c.loop_step,",
    "                        is_save_models=c.is_save_models,",
    "                        log_new_line_interval=c.log_new_line_interval,",
    "                        log_write_interval=c.log_write_interval,",
    "                        save_models_interval=c.save_models_interval,",
    "                        is_loop_on_interrupt=c.is_loop_on_interrupt)",
    "",
    "",
    "class TrainingLoop:",
    "    __loop: Loop",
    "    __signal_received: Optional[Tuple[Any, Any]]",
    "",
    "    def __init__(self, *,",
    "                 loop_count: int,",
    "                 loop_step: int,",
    "                 is_save_models: bool,",
    "                 log_new_line_interval: int,",
    "                 log_write_interval: int,",
    "                 save_models_interval: int,",
    "                 is_loop_on_interrupt: bool):",
    "        self.__loop_count = loop_count",
    "        self.__loop_step = loop_step",
    "        self.__is_save_models = is_save_models",
    "        self.__log_new_line_interval = log_new_line_interval",
    "        self.__log_write_interval = log_write_interval",
    "        self.__save_models_interval = save_models_interval",
    "        self.__signal_received = None",
    "        self.__is_loop_on_interrupt = is_loop_on_interrupt",
    "",
    "    def __iter__(self):",
    "        self.__loop = logger.loop(range(logger.get_global_step(),",
    "                                        self.__loop_count,",
    "                                        self.__loop_step))",
    "        iter(self.__loop)",
    "        try:",
    "            self.old_handler = signal.signal(signal.SIGINT, self.__handler)",
    "        except ValueError:",
    "            pass",
    "        return self",
    "",
    "    def __finish(self):",
    "        try:",
    "            signal.signal(signal.SIGINT, self.old_handler)",
    "        except ValueError:",
    "            pass",
    "        logger.write()",
    "        logger.new_line()",
    "        if self.__is_save_models:",
    "            logger.log(\"Saving model...\")",
    "            logger.save_checkpoint()",
    "",
    "    def __is_interval(self, global_step, interval):",
    "        if global_step - self.__loop_step < 0:",
    "            return False",
    "",
    "        if global_step // interval > (global_step - self.__loop_step) // interval:",
    "            return True",
    "        else:",
    "            return False",
    "",
    "    def __next__(self):",
    "        if self.__signal_received is not None:",
    "            logger.log('\\nKilling Loop.',",
    "                       color=Text.danger)",
    "            logger.finish_loop()",
    "            self.__finish()",
    "            raise StopIteration(\"SIGINT\")",
    "",
    "        try:",
    "            global_step = next(self.__loop)",
    "        except StopIteration as e:",
    "            self.__finish()",
    "            raise e",
    "",
    "        logger.set_global_step(global_step)",
    "",
    "        if self.__is_interval(global_step, self.__log_write_interval):",
    "            logger.write()",
    "        if self.__is_interval(global_step, self.__log_new_line_interval):",
    "            logger.new_line()",
    "",
    "        if (self.__is_save_models and",
    "                self.__is_interval(global_step, self.__save_models_interval)):",
    "            logger.save_checkpoint()",
    "",
    "        return global_step",
    "",
    "    def __handler(self, sig, frame):",
    "        # Pass second interrupt without delaying",
    "        if self.__signal_received is not None:",
    "            logger.log('\\nSIGINT received twice. Stopping...',",
    "                       color=Text.danger)",
    "            self.old_handler(*self.__signal_received)",
    "            return",
    "",
    "        if self.__is_loop_on_interrupt:",
    "            # Store the interrupt signal for later",
    "            self.__signal_received = (sig, frame)",
    "            logger.log('\\nSIGINT received. Delaying KeyboardInterrupt.',",
    "                       color=Text.danger)",
    "        else:",
    "            self.__finish()",
    "            logger.log('Killing loop...', Text.danger)",
    "            self.old_handler(sig, frame)",
    "",
    "    def __str__(self):",
    "        return \"LabTrainingLoop\"",
    ""
  ],
  "lab/util/__init__.py": [
    "import pathlib",
    "import random",
    "import string",
    "from typing import Dict",
    "",
    "import yaml",
    "",
    "",
    "def yaml_load(s: str):",
    "    return yaml.load(s, Loader=yaml.FullLoader)",
    "",
    "",
    "def yaml_dump(obj: any):",
    "    return yaml.dump(obj, default_flow_style=False)",
    "",
    "",
    "def rm_tree(path_to_remove: pathlib.Path):",
    "    if path_to_remove.is_dir():",
    "        for f in path_to_remove.iterdir():",
    "            if f.is_dir():",
    "                rm_tree(f)",
    "            else:",
    "                f.unlink()",
    "        path_to_remove.rmdir()",
    "    else:",
    "        path_to_remove.unlink()",
    "",
    "",
    "def random_string(length=10):",
    "    letters = string.ascii_lowercase",
    "    return ''.join(random.choice(letters) for _ in range(length))",
    "",
    "",
    "def is_ipynb():",
    "    try:",
    "        cfg = get_ipython().config",
    "        if cfg['IPKernelApp'] is None:",
    "            return False",
    "",
    "        app: Dict = cfg['IPKernelApp']",
    "        if len(app.keys()) > 0:",
    "            return True",
    "        else:",
    "            return False",
    "    except NameError:",
    "        return False",
    "",
    "",
    "if __name__ == '__main__':",
    "    print(is_ipynb())"
  ],
  "lab/util/pytorch.py": [
    "import torch",
    "",
    "from lab import logger",
    "from lab.configs import Configs",
    "from lab.logger.colors import Text",
    "",
    "",
    "def get_device(use_cuda: bool, cuda_device: int):",
    "    is_cuda = use_cuda and torch.cuda.is_available()",
    "    if not is_cuda:",
    "        return torch.device('cpu')",
    "    else:",
    "        if cuda_device < torch.cuda.device_count():",
    "            return torch.device('cuda', cuda_device)",
    "        else:",
    "            logger.log(f\"Cuda device index {cuda_device} higher than \"",
    "                       f\"device count {torch.cuda.device_count()}\", Text.warning)",
    "            return torch.device('cuda', torch.cuda.device_count() - 1)",
    "",
    "",
    "def get_modules(configs: Configs):",
    "    keys = dir(configs)",
    "",
    "    modules = {}",
    "    for k in keys:",
    "        value = getattr(configs, k)",
    "        if isinstance(value, torch.nn.Module):",
    "            modules[k] = value",
    "",
    "    return modules",
    ""
  ],
  "samples/logger_indicators.py": [
    "import math",
    "import time",
    "",
    "from lab import logger",
    "from lab.logger.indicators import Queue",
    "",
    "",
    "def loop():",
    "    logger.info(a=2, b=1)",
    "",
    "    logger.add_indicator(Queue(\"reward\", 10, True))",
    "    for i in range(100):",
    "        logger.add_global_step(1)",
    "        logger.store(loss=100 / (i + 1), reward=math.pow(2, (i + 1)))",
    "        logger.write()",
    "        if (i + 1) % 2 == 0:",
    "            logger.new_line()",
    "",
    "        time.sleep(0.3)",
    "",
    "",
    "if __name__ == '__main__':",
    "    loop()",
    ""
  ],
  "samples/logger_sections.py": [
    "import time",
    "",
    "from lab import logger",
    "",
    "",
    "def simple_section():",
    "    with logger.section(\"Simple section\"):",
    "        # code to load data",
    "        time.sleep(2)",
    "",
    "",
    "def unsuccessful_section():",
    "    with logger.section(\"Unsuccessful section\"):",
    "        time.sleep(1)",
    "        logger.set_successful(False)",
    "",
    "",
    "def progress():",
    "    with logger.section(\"Progress\", total_steps=100):",
    "        for i in range(100):",
    "            time.sleep(0.1)",
    "            # Multiple training steps in the inner loop",
    "            logger.progress(i)",
    "",
    "",
    "def loop_section():",
    "    for step in logger.loop(range(0, 10)):",
    "        with logger.section(\"Step\"):",
    "            time.sleep(0.5)",
    "        with logger.section(\"Step2\"):",
    "            time.sleep(0.1)",
    "        logger.write()",
    "    logger.new_line()",
    "",
    "",
    "def loop_partial_section():",
    "    for step in logger.loop(range(0, 10)):",
    "        with logger.section(\"Step\", is_partial=True):",
    "            time.sleep(0.5)",
    "            logger.progress((step % 5 + 1) / 5)",
    "        logger.write()",
    "",
    "",
    "if __name__ == '__main__':",
    "    simple_section()",
    "    unsuccessful_section()",
    "    progress()",
    "    loop_section()",
    "    loop_partial_section()",
    ""
  ],
  "samples/logger_styles.py": [
    "from lab import logger",
    "from lab.logger.colors import Text, Color",
    "",
    "if __name__ == '__main__':",
    "    logger.log(\"Colors are missing when views on github\", Text.highlight)",
    "",
    "    logger.log([",
    "        ('Styles\\n', Text.heading),",
    "        ('Danger\\n', Text.danger),",
    "        ('Warning\\n', Text.warning),",
    "        ('Meta\\n', Text.meta),",
    "        ('Key\\n', Text.key),",
    "        ('Meta2\\n', Text.meta2),",
    "        ('Title\\n', Text.title),",
    "        ('Heading\\n', Text.heading),",
    "        ('Value\\n', Text.value),",
    "        ('Highlight\\n', Text.highlight),",
    "        ('Subtle\\n', Text.subtle)",
    "    ])",
    "",
    "    logger.log([",
    "        ('Colors\\n', Text.heading),",
    "        ('Red\\n', Color.red),",
    "        ('Black\\n', Color.black),",
    "        ('Blue\\n', Color.blue),",
    "        ('Cyan\\n', Color.cyan),",
    "        ('Green\\n', Color.green),",
    "        ('Orange\\n', Color.orange),",
    "        ('Purple Heading\\n', [Color.purple, Text.heading]),",
    "        ('White\\n', Color.white),",
    "    ])",
    ""
  ],
  "samples/mnist_configs.py": [
    "from typing import Dict",
    "",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "import torch.optim as optim",
    "import torch.utils.data",
    "from torchvision import datasets, transforms",
    "",
    "from lab import logger, configs",
    "from lab.experiment.pytorch import Experiment",
    "from lab.logger.indicators import Queue, Histogram",
    "from lab.logger.util import pytorch as logger_util",
    "",
    "",
    "class Net(nn.Module):",
    "    def __init__(self):",
    "        super().__init__()",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)",
    "        self.fc2 = nn.Linear(500, 10)",
    "",
    "    def forward(self, x):",
    "        x = F.relu(self.conv1(x))",
    "        x = F.max_pool2d(x, 2, 2)",
    "        x = F.relu(self.conv2(x))",
    "        x = F.max_pool2d(x, 2, 2)",
    "        x = x.view(-1, 4 * 4 * 50)",
    "        x = F.relu(self.fc1(x))",
    "        x = self.fc2(x)",
    "        return F.log_softmax(x, dim=1)",
    "",
    "",
    "class MNISTLoop:",
    "    def __init__(self, c: 'Configs'):",
    "        self.model = c.model",
    "        self.device = c.device",
    "        self.train_loader = c.train_loader",
    "        self.test_loader = c.test_loader",
    "        self.optimizer = c.optimizer",
    "        self.log_interval = c.log_interval",
    "        self.__epochs = c.epochs",
    "        self.__is_save_models = c.is_save_models",
    "        self.__is_log_parameters = c.is_log_parameters",
    "        self.__log_new_line_interval = c.log_new_line_interval",
    "",
    "    def startup(self):",
    "        logger_util.add_model_indicators(self.model)",
    "",
    "        logger.add_indicator(Queue(\"train_loss\", 20, True))",
    "        logger.add_indicator(Histogram(\"test_loss\", True))",
    "        logger.add_indicator(Histogram(\"accuracy\", True))",
    "",
    "    def _train(self):",
    "        self.model.train()",
    "        for i, (data, target) in logger.enum(\"Train\", self.train_loader):",
    "            data, target = data.to(self.device), target.to(self.device)",
    "            self.optimizer.zero_grad()",
    "            output = self.model(data)",
    "            loss = F.nll_loss(output, target)",
    "            loss.backward()",
    "            self.optimizer.step()",
    "",
    "            # Add training loss to the logger.",
    "            # The logger will queue the values and output the mean",
    "            logger.store(train_loss=loss)",
    "            logger.add_global_step()",
    "",
    "            # Print output to the console",
    "            if i % self.log_interval == 0:",
    "                # Output the indicators",
    "                logger.write()",
    "",
    "    def _test(self):",
    "        self.model.eval()",
    "        test_loss = 0",
    "        correct = 0",
    "        with torch.no_grad():",
    "            for data, target in logger.iterate(\"Test\", self.test_loader):",
    "                data, target = data.to(self.device), target.to(self.device)",
    "                output = self.model(data)",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()",
    "                pred = output.argmax(dim=1, keepdim=True)",
    "                correct += pred.eq(target.view_as(pred)).sum().item()",
    "",
    "        # Add test loss and accuracy to logger",
    "        logger.store(test_loss=test_loss / len(self.test_loader.dataset))",
    "        logger.store(accuracy=correct / len(self.test_loader.dataset))",
    "",
    "    def __log_model_params(self):",
    "        if not self.__is_log_parameters:",
    "            return",
    "",
    "        # Add histograms with model parameter values and gradients",
    "        logger_util.store_model_indicators(self.model)",
    "",
    "    def loop(self):",
    "        # Loop through the monitored iterator",
    "        for epoch in logger.loop(range(0, self.__epochs)):",
    "            self._train()",
    "            self._test()",
    "",
    "            self.__log_model_params()",
    "",
    "            # Clear line and output to console",
    "            logger.write()",
    "",
    "            # Clear line and go to the next line;",
    "            # that is, we add a new line to the output",
    "            # at the end of each epoch",
    "            if (epoch + 1) % self.__log_new_line_interval == 0:",
    "                logger.new_line()",
    "",
    "            if self.__is_save_models:",
    "                logger.save_checkpoint()",
    "",
    "    def __call__(self):",
    "        self.startup()",
    "        self.loop()",
    "",
    "    def __str__(self):",
    "        return 'MNISTLoop'",
    "",
    "",
    "class LoopConfigs(configs.Configs):",
    "    epochs: int = 10",
    "    is_save_models: bool = False",
    "    is_log_parameters: bool = True",
    "    log_new_line_interval: int = 1",
    "",
    "",
    "class LoaderConfigs(configs.Configs):",
    "    train_loader: torch.utils.data.DataLoader",
    "    test_loader: torch.utils.data.DataLoader",
    "",
    "",
    "class Configs(LoopConfigs, LoaderConfigs):",
    "    batch_size: int = 64",
    "    test_batch_size: int = 1000",
    "",
    "    # Reset epochs so that it'll be computed",
    "    epochs: int = 10",
    "    use_cuda: bool = True",
    "    cuda_device: int = 0",
    "    seed: int = 5",
    "    log_interval: int = 10",
    "",
    "    loop: MNISTLoop",
    "",
    "    device: torch.device",
    "",
    "    data_loader_args: Dict",
    "",
    "    model: nn.Module",
    "",
    "    learning_rate: float = 0.01",
    "    momentum: float = 0.5",
    "    optimizer: optim.SGD",
    "",
    "    set_seed = None",
    "",
    "    not_used: bool = 10",
    "",
    "",
    "# The config is inferred from the function name",
    "@Configs.calc()",
    "def data_loader_args(c: Configs):",
    "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}",
    "",
    "",
    "# Get dependencies from parameters.",
    "# The code looks cleaner, but might cause problems when you want to refactor",
    "# later.",
    "# It will be harder to use static analysis tools to find the usage of configs.",
    "@Configs.calc()",
    "def device(*, use_cuda, cuda_device):",
    "    from lab.util.pytorch import get_device",
    "",
    "    return get_device(use_cuda, cuda_device)",
    "",
    "",
    "def _data_loader(is_train, batch_size, dl_args):",
    "    return torch.utils.data.DataLoader(",
    "        datasets.MNIST(str(logger.get_data_path()),",
    "                       train=is_train,",
    "                       download=True,",
    "                       transform=transforms.Compose([",
    "                           transforms.ToTensor(),",
    "                           transforms.Normalize((0.1307,), (0.3081,))",
    "                       ])),",
    "        batch_size=batch_size, shuffle=True, **dl_args)",
    "",
    "",
    "# Multiple configs can be computed from a single function",
    "@Configs.calc(['train_loader', 'test_loader'])",
    "def data_loaders(c: Configs):",
    "    train = _data_loader(True, c.batch_size, c.data_loader_args)",
    "",
    "    test = _data_loader(False, c.test_batch_size, c.data_loader_args)",
    "",
    "    return train, test",
    "",
    "",
    "# Compute multiple results from a single function",
    "@Configs.calc()",
    "def model(c: Configs):",
    "    m: Net = Net()",
    "    m.to(c.device)",
    "    return m",
    "",
    "",
    "# Multiple options for configs can be provided. Option name is inferred from function name,",
    "# unless explicitly provided",
    "@Configs.calc('optimizer')",
    "def sgd_optimizer(c: Configs):",
    "    return optim.SGD(c.model.parameters(), lr=c.learning_rate, momentum=c.momentum)",
    "",
    "",
    "@Configs.calc('optimizer')",
    "def adam_optimizer(c: Configs):",
    "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)",
    "",
    "",
    "# Returns nothing",
    "@Configs.calc()",
    "def set_seed(c: Configs):",
    "    torch.manual_seed(c.seed)",
    "",
    "",
    "def main():",
    "    conf = Configs()",
    "    experiment = Experiment(writers={'sqlite'})",
    "    experiment.calc_configs(conf,",
    "                            {'optimizer': 'sgd_optimizer'},",
    "                            ['set_seed', 'loop'])",
    "    experiment.add_models(dict(model=conf.model))",
    "    experiment.start()",
    "    conf.loop()",
    "",
    "",
    "if __name__ == '__main__':",
    "    main()",
    ""
  ],
  "samples/mnist_indexed_logs.py": [
    "from typing import Dict",
    "",
    "import numpy as np",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "import torch.optim as optim",
    "import torch.utils.data",
    "from torchvision import datasets, transforms",
    "",
    "from lab import logger, configs",
    "from lab import training_loop",
    "from lab.experiment.pytorch import Experiment",
    "from lab.logger.indicators import Queue, Histogram, IndexedScalar",
    "from lab.logger.util import pytorch as logger_util",
    "",
    "",
    "class Net(nn.Module):",
    "    def __init__(self):",
    "        super().__init__()",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)",
    "        self.fc2 = nn.Linear(500, 10)",
    "",
    "    def forward(self, x):",
    "        x = F.relu(self.conv1(x))",
    "        x = F.max_pool2d(x, 2, 2)",
    "        x = F.relu(self.conv2(x))",
    "        x = F.max_pool2d(x, 2, 2)",
    "        x = x.view(-1, 4 * 4 * 50)",
    "        x = F.relu(self.fc1(x))",
    "        x = self.fc2(x)",
    "        return F.log_softmax(x, dim=1)",
    "",
    "",
    "class MNIST:",
    "    def __init__(self, c: 'Configs'):",
    "        self.model = c.model",
    "        self.device = c.device",
    "        self.train_loader = c.train_loader",
    "        self.test_loader = c.test_loader",
    "        self.optimizer = c.optimizer",
    "        self.train_log_interval = c.train_log_interval",
    "        self.loop = c.training_loop",
    "        self.__is_log_parameters = c.is_log_parameters",
    "",
    "    def _train(self):",
    "        self.model.train()",
    "        for i, (data, target) in logger.enum(\"Train\", self.train_loader):",
    "            data, target = data.to(self.device), target.to(self.device)",
    "",
    "            self.optimizer.zero_grad()",
    "            output = self.model(data)",
    "            loss = F.nll_loss(output, target)",
    "            loss.backward()",
    "            self.optimizer.step()",
    "",
    "            # Add training loss to the logger.",
    "            # The logger will queue the values and output the mean",
    "            logger.store(train_loss=loss)",
    "            logger.add_global_step()",
    "",
    "            # Print output to the console",
    "            if i % self.train_log_interval == 0:",
    "                # Output the indicators",
    "                logger.write()",
    "",
    "    def _test(self):",
    "        self.model.eval()",
    "        test_loss = 0",
    "        correct = 0",
    "        idx = 0",
    "        with torch.no_grad():",
    "            for data, target in logger.iterate(\"Test\", self.test_loader):",
    "                data, target = data.to(self.device), target.to(self.device)",
    "                output = self.model(data)",
    "                loss = F.nll_loss(output, target, reduction='none')",
    "                indexes = [idx + i for i in range(self.test_loader.batch_size)]",
    "                values = list(loss.cpu().numpy())",
    "                logger.store('test_sample_loss', (indexes, values))",
    "",
    "                test_loss += float(np.sum(loss.cpu().numpy()))",
    "                pred = output.argmax(dim=1, keepdim=True)",
    "                values = list(pred.cpu().numpy())",
    "                logger.store('test_sample_pred', (indexes, values))",
    "                correct += pred.eq(target.view_as(pred)).sum().item()",
    "",
    "                idx += self.test_loader.batch_size",
    "",
    "        # Add test loss and accuracy to logger",
    "        logger.store(test_loss=test_loss / len(self.test_loader.dataset))",
    "        logger.store(accuracy=correct / len(self.test_loader.dataset))",
    "",
    "    def __log_model_params(self):",
    "        if not self.__is_log_parameters:",
    "            return",
    "",
    "        # Add histograms with model parameter values and gradients",
    "        logger_util.store_model_indicators(self.model)",
    "",
    "    def __call__(self):",
    "        # Training and testing",
    "        logger_util.add_model_indicators(self.model)",
    "",
    "        logger.add_indicator(Queue(\"train_loss\", 20, True))",
    "        logger.add_indicator(Histogram(\"test_loss\", True))",
    "        logger.add_indicator(Histogram(\"accuracy\", True))",
    "        logger.add_indicator(IndexedScalar('test_sample_loss'))",
    "        logger.add_indicator(IndexedScalar('test_sample_pred'))",
    "",
    "        test_data = np.array([d[0].numpy() for d in self.test_loader.dataset])",
    "        logger.save_numpy(\"test_data\", test_data)",
    "",
    "        for _ in self.loop:",
    "            self._train()",
    "            self._test()",
    "            self.__log_model_params()",
    "",
    "",
    "class LoaderConfigs(configs.Configs):",
    "    train_loader: torch.utils.data.DataLoader",
    "    test_loader: torch.utils.data.DataLoader",
    "",
    "",
    "class Configs(training_loop.TrainingLoopConfigs, LoaderConfigs):",
    "    epochs: int = 10",
    "",
    "    loop_step = None",
    "    loop_count = None",
    "",
    "    is_save_models = True",
    "    batch_size: int = 64",
    "    test_batch_size: int = 1000",
    "",
    "    # Reset epochs so that it'll be computed",
    "    use_cuda: float = True",
    "    cuda_device: int = 0",
    "    seed: int = 5",
    "    train_log_interval: int = 10",
    "",
    "    is_log_parameters: bool = True",
    "",
    "    main: MNIST",
    "",
    "    device: any",
    "",
    "    data_loader_args: Dict",
    "",
    "    model: nn.Module",
    "",
    "    learning_rate: float = 0.01",
    "    momentum: float = 0.5",
    "    optimizer: optim.SGD",
    "",
    "    set_seed = None",
    "",
    "",
    "# The config is inferred from the function name",
    "@Configs.calc()",
    "def data_loader_args(c: Configs):",
    "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}",
    "",
    "",
    "# Get dependencies from parameters.",
    "# The code looks cleaner, but might cause problems when you want to refactor",
    "# later.",
    "# It will be harder to use static analysis tools to find the usage of configs.",
    "@Configs.calc()",
    "def device(*, use_cuda, cuda_device):",
    "    from lab.util.pytorch import get_device",
    "",
    "    return get_device(use_cuda, cuda_device)",
    "",
    "",
    "def _data_loader(is_train, batch_size, dl_args):",
    "    return torch.utils.data.DataLoader(",
    "        datasets.MNIST(str(logger.get_data_path()),",
    "                       train=is_train,",
    "                       download=True,",
    "                       transform=transforms.Compose([",
    "                           transforms.ToTensor(),",
    "                           transforms.Normalize((0.1307,), (0.3081,))",
    "                       ])),",
    "        batch_size=batch_size, shuffle=True, **dl_args)",
    "",
    "",
    "# Multiple configs can be computed from a single function",
    "@Configs.calc(['train_loader', 'test_loader'])",
    "def data_loaders(c: Configs):",
    "    train = _data_loader(True, c.batch_size, c.data_loader_args)",
    "    test = _data_loader(False, c.test_batch_size, c.data_loader_args)",
    "",
    "    return train, test",
    "",
    "",
    "# Compute multiple results from a single function",
    "@Configs.calc()",
    "def model(c: Configs):",
    "    m: Net = Net()",
    "    m.to(c.device)",
    "    return m",
    "",
    "",
    "# Multiple options for configs can be provided. Option name is inferred from function name,",
    "# unless explicitly provided",
    "@Configs.calc('optimizer')",
    "def sgd_optimizer(c: Configs):",
    "    return optim.SGD(c.model.parameters(), lr=c.learning_rate, momentum=c.momentum)",
    "",
    "",
    "@Configs.calc('optimizer')",
    "def adam_optimizer(c: Configs):",
    "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)",
    "",
    "",
    "# Returns nothing",
    "@Configs.calc()",
    "def set_seed(c: Configs):",
    "    torch.manual_seed(c.seed)",
    "",
    "",
    "@Configs.calc()",
    "def loop_count(c: Configs):",
    "    return c.epochs * len(c.train_loader)",
    "",
    "",
    "@Configs.calc()",
    "def loop_step(c: Configs):",
    "    return len(c.train_loader)",
    "",
    "",
    "def main():",
    "    conf = Configs()",
    "    experiment = Experiment(writers={'sqlite'})",
    "    experiment.calc_configs(conf,",
    "                            {'optimizer': 'sgd_optimizer'},",
    "                            ['set_seed', 'main'])",
    "    experiment.add_models(dict(model=conf.model))",
    "    experiment.start()",
    "    conf.main()",
    "",
    "",
    "if __name__ == '__main__':",
    "    main()",
    ""
  ],
  "samples/mnist_loop.py": [
    "from typing import Dict",
    "",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "import torch.optim as optim",
    "import torch.utils.data",
    "from torchvision import datasets, transforms",
    "",
    "from lab import logger, configs",
    "from lab import training_loop",
    "from lab.experiment.pytorch import Experiment",
    "from lab.logger.indicators import Queue, Histogram",
    "from lab.logger.util import pytorch as logger_util",
    "",
    "",
    "class Net(nn.Module):",
    "    def __init__(self):",
    "        super().__init__()",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)",
    "        self.fc2 = nn.Linear(500, 10)",
    "",
    "    def forward(self, x):",
    "        x = F.relu(self.conv1(x))",
    "        x = F.max_pool2d(x, 2, 2)",
    "        x = F.relu(self.conv2(x))",
    "        x = F.max_pool2d(x, 2, 2)",
    "        x = x.view(-1, 4 * 4 * 50)",
    "        x = F.relu(self.fc1(x))",
    "        x = self.fc2(x)",
    "        return F.log_softmax(x, dim=1)",
    "",
    "",
    "class MNIST:",
    "    def __init__(self, c: 'Configs'):",
    "        self.model = c.model",
    "        self.device = c.device",
    "        self.train_loader = c.train_loader",
    "        self.test_loader = c.test_loader",
    "        self.optimizer = c.optimizer",
    "        self.train_log_interval = c.train_log_interval",
    "        self.loop = c.training_loop",
    "        self.__is_log_parameters = c.is_log_parameters",
    "",
    "    def _train(self):",
    "        self.model.train()",
    "        for i, (data, target) in logger.enum(\"Train\", self.train_loader):",
    "            data, target = data.to(self.device), target.to(self.device)",
    "",
    "            self.optimizer.zero_grad()",
    "            output = self.model(data)",
    "            loss = F.nll_loss(output, target)",
    "            loss.backward()",
    "            self.optimizer.step()",
    "",
    "            logger.store(train_loss=loss)",
    "            logger.add_global_step()",
    "",
    "            if i % self.train_log_interval == 0:",
    "                logger.write()",
    "",
    "    def _test(self):",
    "        self.model.eval()",
    "        test_loss = 0",
    "        correct = 0",
    "        with torch.no_grad():",
    "            for data, target in logger.iterate(\"Test\", self.test_loader):",
    "                data, target = data.to(self.device), target.to(self.device)",
    "                output = self.model(data)",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()",
    "                pred = output.argmax(dim=1, keepdim=True)",
    "                correct += pred.eq(target.view_as(pred)).sum().item()",
    "",
    "        logger.store(test_loss=test_loss / len(self.test_loader.dataset))",
    "        logger.store(accuracy=correct / len(self.test_loader.dataset))",
    "",
    "    def __log_model_params(self):",
    "        if not self.__is_log_parameters:",
    "            return",
    "",
    "        logger_util.store_model_indicators(self.model)",
    "",
    "    def __call__(self):",
    "        logger_util.add_model_indicators(self.model)",
    "",
    "        logger.add_indicator(Queue(\"train_loss\", 20, True))",
    "        logger.add_indicator(Histogram(\"test_loss\", True))",
    "        logger.add_indicator(Histogram(\"accuracy\", True))",
    "",
    "        for _ in self.loop:",
    "            self._train()",
    "            self._test()",
    "            self.__log_model_params()",
    "",
    "",
    "class LoaderConfigs(configs.Configs):",
    "    train_loader: torch.utils.data.DataLoader",
    "    test_loader: torch.utils.data.DataLoader",
    "",
    "",
    "class Configs(training_loop.TrainingLoopConfigs, LoaderConfigs):",
    "    epochs: int = 10",
    "",
    "    loop_step = 'loop_step'",
    "    loop_count = 'loop_count'",
    "",
    "    is_save_models = True",
    "    batch_size: int = 64",
    "    test_batch_size: int = 1000",
    "",
    "    use_cuda: float = True",
    "    cuda_device: int = 0",
    "    seed: int = 5",
    "    train_log_interval: int = 10",
    "",
    "    is_log_parameters: bool = True",
    "",
    "    device: any",
    "",
    "    data_loader_args: Dict",
    "",
    "    model: nn.Module",
    "",
    "    learning_rate: float = 0.01",
    "    momentum: float = 0.5",
    "    optimizer: optim.SGD",
    "",
    "    set_seed = 'set_seed'",
    "",
    "    main: MNIST",
    "",
    "",
    "@Configs.calc(Configs.data_loader_args)",
    "def data_loader_args(c: Configs):",
    "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}",
    "",
    "",
    "@Configs.calc(Configs.device)",
    "def device(*, use_cuda, cuda_device):",
    "    from lab.util.pytorch import get_device",
    "",
    "    return get_device(use_cuda, cuda_device)",
    "",
    "",
    "def _data_loader(is_train, batch_size, dl_args):",
    "    return torch.utils.data.DataLoader(",
    "        datasets.MNIST(str(logger.get_data_path()),",
    "                       train=is_train,",
    "                       download=True,",
    "                       transform=transforms.Compose([",
    "                           transforms.ToTensor(),",
    "                           transforms.Normalize((0.1307,), (0.3081,))",
    "                       ])),",
    "        batch_size=batch_size, shuffle=True, **dl_args)",
    "",
    "",
    "@Configs.calc([Configs.train_loader, Configs.test_loader])",
    "def data_loaders(c: Configs):",
    "    train = _data_loader(True, c.batch_size, c.data_loader_args)",
    "    test = _data_loader(False, c.test_batch_size, c.data_loader_args)",
    "",
    "    return train, test",
    "",
    "",
    "@Configs.calc(Configs.model)",
    "def model(c: Configs):",
    "    m: Net = Net()",
    "    m.to(c.device)",
    "    return m",
    "",
    "",
    "@Configs.calc('optimizer')",
    "def sgd_optimizer(c: Configs):",
    "    return optim.SGD(c.model.parameters(), lr=c.learning_rate, momentum=c.momentum)",
    "",
    "",
    "@Configs.calc(Configs.optimizer)",
    "def adam_optimizer(c: Configs):",
    "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)",
    "",
    "",
    "@Configs.calc(Configs.set_seed)",
    "def set_seed(c: Configs):",
    "    torch.manual_seed(c.seed)",
    "",
    "",
    "@Configs.calc(Configs.loop_count)",
    "def loop_count(c: Configs):",
    "    return c.epochs * len(c.train_loader)",
    "",
    "",
    "@Configs.calc()",
    "def loop_step(c: Configs):",
    "    return len(c.train_loader)",
    "",
    "",
    "def main():",
    "    conf = Configs()",
    "    experiment = Experiment(writers={'sqlite'})",
    "    experiment.calc_configs(conf,",
    "                            {'optimizer': 'adam_optimizer'},",
    "                            ['set_seed', 'main'])",
    "    experiment.add_models(dict(model=conf.model))",
    "    experiment.start()",
    "    conf.main()",
    "",
    "",
    "if __name__ == '__main__':",
    "    main()",
    ""
  ],
  "setup.py": [
    "import setuptools",
    "",
    "with open(\"readme.md\", \"r\") as f:",
    "    long_description = f.read()",
    "",
    "print(setuptools.find_packages())",
    "",
    "setuptools.setup(",
    "    name='machine_learning_lab',",
    "    version='3.0',",
    "    author=\"Varuna Jayasiri\",",
    "    author_email=\"vpjayasiri@gmail.com\",",
    "    description=\"🧪 Organize Machine Learning Experiments\",",
    "    long_description=long_description,",
    "    long_description_content_type=\"text/markdown\",",
    "    url=\"https://github.com/vpj/lab\",",
    "    packages=setuptools.find_packages(exclude=('samples', 'samples.*')),",
    "    install_requires=['gitpython',",
    "                      'pyyaml',",
    "                      'numpy'],",
    "    classifiers=[",
    "        \"Programming Language :: Python :: 3\",",
    "        \"License :: OSI Approved :: MIT License\",",
    "        'Intended Audience :: Developers',",
    "        'Intended Audience :: Science/Research',",
    "        'Topic :: Scientific/Engineering',",
    "        'Topic :: Scientific/Engineering :: Mathematics',",
    "        'Topic :: Scientific/Engineering :: Artificial Intelligence',",
    "        'Topic :: Software Development',",
    "        'Topic :: Software Development :: Libraries',",
    "        'Topic :: Software Development :: Libraries :: Python Modules',",
    "    ],",
    "    keywords='machine learning',",
    ")",
    ""
  ],
  "test/__init__.py": [
    ""
  ],
  "test/logger.py": [
    "from lab import logger",
    "from lab.logger.colors import Text, Color",
    "",
    "logger.log(\"Colors are missing when views on github\", Text.highlight)",
    "",
    "logger.log([",
    "    ('Styles\\n', Text.heading),",
    "    ('Danger\\n', Text.danger),",
    "    ('Warning\\n', Text.warning),",
    "    ('Meta\\n', Text.meta),",
    "    ('Key\\n', Text.key),",
    "    ('Meta2\\n', Text.meta2),",
    "    ('Title\\n', Text.title),",
    "    ('Heading\\n', Text.heading),",
    "    ('Value\\n', Text.value),",
    "    ('Highlight\\n', Text.highlight),",
    "    ('Subtle\\n', Text.subtle)",
    "])",
    "",
    "logger.log([",
    "    ('Colors\\n', Text.heading),",
    "    ('Red\\n', Color.red),",
    "    ('Black\\n', Color.black),",
    "    ('Blue\\n', Color.blue),",
    "    ('Cyan\\n', Color.cyan),",
    "    ('Green\\n', Color.green),",
    "    ('Orange\\n', Color.orange),",
    "    ('Purple Heading\\n', [Color.purple, Text.heading]),",
    "    ('White\\n', Color.white),",
    "])"
  ],
  "test.py": [
    "from lab.experiment.pytorch import Experiment",
    "",
    "",
    "def create_exp():",
    "    return Experiment(name=\"test\",",
    "                      comment=\"Test\")",
    ""
  ]
}
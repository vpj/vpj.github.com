{
  "lab/logger/__init__.py": [
    {
      "pre": [
        "        _internal = _LoggerInternal()",
        "",
        "    return _internal",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def add_indicator(indicator: Indicator):",
        "    internal().add_indicator(indicator)",
        ""
      ],
      "code": [
        "def log(message: Union[str, List[Union[str, Tuple[str, StyleCode]]]],",
        "        color: List[StyleCode] or StyleCode or None = None,",
        "        *,",
        "        is_new_line=True):",
        "    if type(message) == str:",
        "        internal().log([(message, color)], is_new_line=is_new_line)",
        "    elif type(message) == list:",
        "        internal().log(message, is_new_line=is_new_line)"
      ],
      "note": "#### Print a message to the console",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "def store(*args, **kwargs):",
        "    internal().store(*args, **kwargs)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def new_line():",
        "    internal().new_line()",
        ""
      ],
      "code": [
        "def write():",
        "    internal().write()"
      ],
      "note": "#### Output the stored log values to screen, SQLite and TensorBoard",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [],
      "post": [
        "",
        "_internal: Optional[_LoggerInternal] = None",
        "",
        "",
        "def internal() -> _LoggerInternal:"
      ],
      "code": [
        "from typing import Union, List, Tuple, Optional, Iterable, Sized",
        "",
        "import numpy as np",
        "",
        "from .colors import StyleCode",
        "from .indicators import Indicator",
        "from .internal import LoggerInternal as _LoggerInternal"
      ],
      "note": "#### Imports",
      "collapsed": false,
      "codeCollapsed": true
    },
    {
      "pre": [
        "",
        "from .colors import StyleCode",
        "from .indicators import Indicator",
        "from .internal import LoggerInternal as _LoggerInternal",
        ""
      ],
      "post": [
        "",
        "",
        "def log(message: Union[str, List[Union[str, Tuple[str, StyleCode]]]],",
        "        color: List[StyleCode] or StyleCode or None = None,",
        "        *,"
      ],
      "code": [
        "_internal: Optional[_LoggerInternal] = None",
        "",
        "",
        "def internal() -> _LoggerInternal:",
        "    global _internal",
        "    if _internal is None:",
        "        _internal = _LoggerInternal()",
        "",
        "    return _internal"
      ],
      "note": "#### Internals\n\nInternals of the logger are hidden",
      "collapsed": true,
      "codeCollapsed": true
    },
    {
      "pre": [
        "        internal().log([(message, color)], is_new_line=is_new_line)",
        "    elif type(message) == list:",
        "        internal().log(message, is_new_line=is_new_line)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def store(*args, **kwargs):",
        "    internal().store(*args, **kwargs)",
        ""
      ],
      "code": [
        "def add_indicator(indicator: Indicator):",
        "    internal().add_indicator(indicator)"
      ],
      "note": "#### Setup a log indicator",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "def add_indicator(indicator: Indicator):",
        "    internal().add_indicator(indicator)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def write():",
        "    internal().write()",
        ""
      ],
      "code": [
        "def store(*args, **kwargs):",
        "    internal().store(*args, **kwargs)"
      ],
      "note": "#### Store values in the logger\n\nThese may be collected to a queue or a list depending on the log indicator configurations.",
      "collapsed": false,
      "codeCollapsed": false
    }
  ],
  "samples/mnist_loop.py": [
    {
      "pre": [
        "from lab.experiment.pytorch import Experiment",
        "from lab.logger.indicators import Queue, Histogram",
        "from lab.logger.util import pytorch as logger_util",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "class MNIST:",
        "    def __init__(self, c: 'Configs'):",
        "        self.model = c.model"
      ],
      "code": [
        "class Net(nn.Module):",
        "    def __init__(self):",
        "        super().__init__()",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)",
        "        self.fc1 = nn.Linear(4 * 4 * 50, 500)",
        "        self.fc2 = nn.Linear(500, 10)",
        "",
        "    def forward(self, x):",
        "        x = F.relu(self.conv1(x))",
        "        x = F.max_pool2d(x, 2, 2)",
        "        x = F.relu(self.conv2(x))",
        "        x = F.max_pool2d(x, 2, 2)",
        "        x = x.view(-1, 4 * 4 * 50)",
        "        x = F.relu(self.fc1(x))",
        "        x = self.fc2(x)",
        "        return F.log_softmax(x, dim=1)"
      ],
      "note": "#### Model",
      "collapsed": false,
      "codeCollapsed": true
    },
    {
      "pre": [
        "            self._train()",
        "            self._test()",
        "            self.__log_model_params()",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "class Configs(training_loop.TrainingLoopConfigs, LoaderConfigs):",
        "    epochs: int = 10",
        ""
      ],
      "code": [
        "class LoaderConfigs(configs.Configs):",
        "    train_loader: torch.utils.data.DataLoader",
        "    test_loader: torch.utils.data.DataLoader"
      ],
      "note": "#### Data loader configurations\n\nConfigs can be sub-classed",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [],
      "post": [
        "",
        "",
        "class Net(nn.Module):",
        "    def __init__(self):",
        "        super().__init__()"
      ],
      "code": [
        "from typing import Dict",
        "",
        "import torch",
        "import torch.nn as nn",
        "import torch.nn.functional as F",
        "import torch.optim as optim",
        "import torch.utils.data",
        "from torchvision import datasets, transforms",
        "",
        "from lab import logger, configs",
        "from lab import training_loop",
        "from lab.experiment.pytorch import Experiment",
        "from lab.logger.indicators import Queue, Histogram",
        "from lab.logger.util import pytorch as logger_util"
      ],
      "note": "#### Imports",
      "collapsed": false,
      "codeCollapsed": true
    },
    {
      "pre": [
        "            output = self.model(data)",
        "            loss = F.nll_loss(output, target)",
        "            loss.backward()",
        "            self.optimizer.step()",
        ""
      ],
      "post": [
        "            logger.add_global_step()",
        "",
        "            if i % self.train_log_interval == 0:",
        "                logger.write()",
        ""
      ],
      "code": [
        "            logger.store(train_loss=loss)"
      ],
      "note": "This adds the training loss to the logger. It will queue them and output the mean.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "            logger.store(train_loss=loss)",
        "            logger.add_global_step()",
        "",
        "            if i % self.train_log_interval == 0:"
      ],
      "post": [
        "",
        "    def _test(self):",
        "        self.model.eval()",
        "        test_loss = 0",
        "        correct = 0"
      ],
      "code": [
        "                logger.write()"
      ],
      "note": "This writes the values stored in the logger to the screen and other destinations",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "",
        "def main():",
        "    conf = Configs()",
        "    experiment = Experiment(writers={'sqlite'})"
      ],
      "post": [
        "    experiment.add_models(dict(model=conf.model))",
        "    experiment.start()",
        "    conf.main()",
        "",
        ""
      ],
      "code": [
        "    experiment.calc_configs(conf,",
        "                            {'optimizer': 'adam_optimizer'},",
        "                            ['set_seed', 'main'])"
      ],
      "note": "We can provide a set of configs to be overridden in a dictionary.\n\nThe third parameter is a list of configs to be evaluated in order. It will try to evaluate `set_seed` as soon as possible and then evaluate `main` and it's dependencies.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "                output = self.model(data)",
        "                test_loss += F.nll_loss(output, target, reduction='sum').item()",
        "                pred = output.argmax(dim=1, keepdim=True)",
        "                correct += pred.eq(target.view_as(pred)).sum().item()",
        ""
      ],
      "post": [
        "",
        "    def __log_model_params(self):",
        "        if not self.__is_log_parameters:",
        "            return",
        ""
      ],
      "code": [
        "        logger.store(test_loss=test_loss / len(self.test_loader.dataset))",
        "        logger.store(accuracy=correct / len(self.test_loader.dataset))"
      ],
      "note": "Add test loss and accuracy to the logger",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "    def __log_model_params(self):",
        "        if not self.__is_log_parameters:",
        "            return",
        ""
      ],
      "post": [
        "",
        "    def __call__(self):",
        "        logger_util.add_model_indicators(self.model)",
        "",
        "        logger.add_indicator(Queue(\"train_loss\", 20, True))"
      ],
      "code": [
        "        logger_util.store_model_indicators(self.model)"
      ],
      "note": "Add histograms of model parameters and gradients",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "            return",
        "",
        "        logger_util.store_model_indicators(self.model)",
        "",
        "    def __call__(self):"
      ],
      "post": [
        "",
        "        logger.add_indicator(Queue(\"train_loss\", 20, True))",
        "        logger.add_indicator(Histogram(\"test_loss\", True))",
        "        logger.add_indicator(Histogram(\"accuracy\", True))",
        ""
      ],
      "code": [
        "        logger_util.add_model_indicators(self.model)"
      ],
      "note": "Setup logger indicators for model parameters and gradients",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "        logger_util.store_model_indicators(self.model)",
        "",
        "    def __call__(self):",
        "        logger_util.add_model_indicators(self.model)",
        ""
      ],
      "post": [
        "",
        "        for _ in self.loop:",
        "            self._train()",
        "            self._test()",
        "            self.__log_model_params()"
      ],
      "code": [
        "        logger.add_indicator(Queue(\"train_loss\", 20, True))",
        "        logger.add_indicator(Histogram(\"test_loss\", True))",
        "        logger.add_indicator(Histogram(\"accuracy\", True))"
      ],
      "note": "Setup log indicators",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "@Configs.calc(Configs.data_loader_args)",
        "def data_loader_args(c: Configs):",
        "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def _data_loader(is_train, batch_size, dl_args):",
        "    return torch.utils.data.DataLoader(",
        "        datasets.MNIST(str(logger.get_data_path()),"
      ],
      "code": [
        "@Configs.calc(Configs.device)",
        "def device(*, use_cuda, cuda_device):",
        "    from lab.util.pytorch import get_device",
        "",
        "    return get_device(use_cuda, cuda_device)"
      ],
      "note": "Here the function takes in a bunch of configs as **keyword only arguments*, instead of a single `Configs` object.\n\nAlthough the code looks cleaner this is not the advised way because it becomes harder to refactor later. It will be harder to find usages of a config for example.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "                           transforms.Normalize((0.1307,), (0.3081,))",
        "                       ])),",
        "        batch_size=batch_size, shuffle=True, **dl_args)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc(Configs.model)",
        "def model(c: Configs):",
        "    m: Net = Net()"
      ],
      "code": [
        "@Configs.calc([Configs.train_loader, Configs.test_loader])",
        "def data_loaders(c: Configs):",
        "    train = _data_loader(True, c.batch_size, c.data_loader_args)",
        "    test = _data_loader(False, c.test_batch_size, c.data_loader_args)",
        "",
        "    return train, test"
      ],
      "note": "Multiple configs can be computed from a single function",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "    m: Net = Net()",
        "    m.to(c.device)",
        "    return m",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc(Configs.set_seed)",
        "def set_seed(c: Configs):",
        "    torch.manual_seed(c.seed)"
      ],
      "code": [
        "@Configs.calc('optimizer')",
        "def sgd_optimizer(c: Configs):",
        "    return optim.SGD(c.model.parameters(), lr=c.learning_rate, momentum=c.momentum)",
        "",
        "",
        "@Configs.calc(Configs.optimizer)",
        "def adam_optimizer(c: Configs):",
        "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)"
      ],
      "note": "Multiple options for configs can be provided.\nName of the option is inferred from the function name unless explicitly provided.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "    m: Net = Net()",
        "    m.to(c.device)",
        "    return m",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc(Configs.optimizer)",
        "def adam_optimizer(c: Configs):",
        "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)"
      ],
      "code": [
        "@Configs.calc('optimizer')",
        "def sgd_optimizer(c: Configs):",
        "    return optim.SGD(c.model.parameters(), lr=c.learning_rate, momentum=c.momentum)"
      ],
      "note": "⚠️ Config name can be passed as a string too. However this is not advised since it doesn't work well with type hints and harder to refractor.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "@Configs.calc(Configs.optimizer)",
        "def adam_optimizer(c: Configs):",
        "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc(Configs.loop_count)",
        "def loop_count(c: Configs):",
        "    return c.epochs * len(c.train_loader)"
      ],
      "code": [
        "@Configs.calc(Configs.set_seed)",
        "def set_seed(c: Configs):",
        "    torch.manual_seed(c.seed)"
      ],
      "note": "This is actually not a configuration. It sets the seed. Note that the function doesn't return any value.\n\nWhen we run the experiment we can inform the lab to run this as soon as possible. ",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "@Configs.calc(Configs.loop_count)",
        "def loop_count(c: Configs):",
        "    return c.epochs * len(c.train_loader)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def main():",
        "    conf = Configs()",
        "    experiment = Experiment(writers={'sqlite'})"
      ],
      "code": [
        "@Configs.calc()",
        "def loop_step(c: Configs):",
        "    return len(c.train_loader)"
      ],
      "note": "⚠️ Name of the config can be inferred from the function name. However this is also not advised since it doesn't work well with type hints and static analysis tools.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "    is_save_models = True",
        "    batch_size: int = 64",
        "    test_batch_size: int = 1000",
        ""
      ],
      "post": [
        "",
        "    is_log_parameters: bool = True",
        "",
        "    device: any",
        ""
      ],
      "code": [
        "    use_cuda: float = True",
        "    cuda_device: int = 0",
        "    seed: int = 5",
        "    train_log_interval: int = 10"
      ],
      "note": "Configurations can be set directly in declaration",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "class LoaderConfigs(configs.Configs):",
        "    train_loader: torch.utils.data.DataLoader",
        "    test_loader: torch.utils.data.DataLoader",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc(Configs.data_loader_args)",
        "def data_loader_args(c: Configs):",
        "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}"
      ],
      "code": [
        "class Configs(training_loop.TrainingLoopConfigs, LoaderConfigs):",
        "    epochs: int = 10",
        "",
        "    loop_step = 'loop_step'",
        "    loop_count = 'loop_count'",
        "",
        "    is_save_models = True",
        "    batch_size: int = 64",
        "    test_batch_size: int = 1000",
        "",
        "    use_cuda: float = True",
        "    cuda_device: int = 0",
        "    seed: int = 5",
        "    train_log_interval: int = 10",
        "",
        "    is_log_parameters: bool = True",
        "",
        "    device: any",
        "",
        "    data_loader_args: Dict",
        "",
        "    model: nn.Module",
        "",
        "    learning_rate: float = 0.01",
        "    momentum: float = 0.5",
        "    optimizer: optim.SGD",
        "",
        "    set_seed = 'set_seed'",
        "",
        "    main: MNIST"
      ],
      "note": "### Configurations\n\nThis extends `TrainingLoopConfigs` and `LoaderConfigs`",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "    set_seed = 'set_seed'",
        "",
        "    main: MNIST",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc(Configs.device)",
        "def device(*, use_cuda, cuda_device):",
        "    from lab.util.pytorch import get_device"
      ],
      "code": [
        "@Configs.calc(Configs.data_loader_args)",
        "def data_loader_args(c: Configs):",
        "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}"
      ],
      "note": "`data_loader_args` configurations are computed based on other configs.\nThe name of the config is taken from the function name.",
      "collapsed": false,
      "codeCollapsed": false
    }
  ],
  "lab/experiment/__init__.py": [
    {
      "pre": [
        "from lab.logger.internal import CheckpointSaver",
        "from lab.logger.writers import sqlite, tensorboard",
        "from lab.util import is_ipynb",
        "",
        ""
      ],
      "post": [
        ""
      ],
      "code": [
        "class Experiment:",
        "    \"\"\"",
        "    ## Experiment",
        "",
        "    Each experiment has different configurations or algorithms.",
        "    An experiment can have multiple trials.",
        "    \"\"\"",
        "    run: Run",
        "    configs_processor: Optional[ConfigProcessor]",
        "",
        "    # whether not to start the experiment if there are uncommitted changes.",
        "    check_repo_dirty: bool",
        "",
        "    def __init__(self, *,",
        "                 name: Optional[str],",
        "                 python_file: Optional[str],",
        "                 comment: Optional[str],",
        "                 writers: Set[str] = None,",
        "                 ignore_callers: Set[str] = None):",
        "        \"\"\"",
        "        ### Create the experiment",
        "",
        "        :param name: name of the experiment",
        "        :param python_file: `__file__` that invokes this. This is stored in",
        "         the experiments list.",
        "        :param comment: a short description of the experiment",
        "",
        "        The experiments log keeps track of `python_file`, `name`, `comment` as",
        "         well as the git commit.",
        "",
        "        Experiment maintains the locations of checkpoints, logs, etc.",
        "        \"\"\"",
        "",
        "        if python_file is None:",
        "            python_file = self.__get_caller_file(ignore_callers)",
        "",
        "        if python_file.startswith('<ipython'):",
        "            assert is_ipynb()",
        "            if name is None:",
        "                raise ValueError(\"You must specify python_file or experiment name\"",
        "                                 \" when creating an experiment from a python notebook.\")",
        "            self.lab = Lab(os.getcwd())",
        "            python_file = 'notebook.ipynb'",
        "        else:",
        "            self.lab = Lab(python_file)",
        "",
        "            if name is None:",
        "                file_path = pathlib.PurePath(python_file)",
        "                name = file_path.stem",
        "",
        "        logger.internal().set_data_path(self.lab.data_path)",
        "",
        "        if comment is None:",
        "            comment = ''",
        "",
        "        self.name = name",
        "        self.experiment_path = self.lab.experiments / name",
        "",
        "        self.check_repo_dirty = self.lab.check_repo_dirty",
        "",
        "        self.configs_processor = None",
        "",
        "        experiment_path = pathlib.Path(self.experiment_path)",
        "        if not experiment_path.exists():",
        "            experiment_path.mkdir(parents=True)",
        "",
        "        self.run = Run.create(",
        "            experiment_path=self.experiment_path,",
        "            python_file=python_file,",
        "            trial_time=time.localtime(),",
        "            comment=comment)",
        "",
        "        repo = git.Repo(self.lab.path)",
        "",
        "        self.run.commit = repo.head.commit.hexsha",
        "        self.run.commit_message = repo.head.commit.message.strip()",
        "        self.run.is_dirty = repo.is_dirty()",
        "        self.run.diff = repo.git.diff()",
        "",
        "        checkpoint_saver = self._create_checkpoint_saver()",
        "        logger.internal().set_checkpoint_saver(checkpoint_saver)",
        "",
        "        if writers is None:",
        "            writers = {'sqlite', 'tensorboard'}",
        "",
        "        if 'sqlite' in writers:",
        "            logger.internal().add_writer(sqlite.Writer(self.run.sqlite_path))",
        "        if 'tensorboard' in writers:",
        "            logger.internal().add_writer(tensorboard.Writer(self.run.tensorboard_log_path))",
        "",
        "        logger.internal().set_numpy_path(self.run.numpy_path)",
        "",
        "    @staticmethod",
        "    def __get_caller_file(ignore_callers: Set[str] = None):",
        "        if ignore_callers is None:",
        "            ignore_callers = {}",
        "",
        "        import inspect",
        "",
        "        frames: List[inspect.FrameInfo] = inspect.stack()",
        "        lab_src = pathlib.PurePath(__file__).parent.parent",
        "",
        "        for f in frames:",
        "            module_path = pathlib.PurePath(f.filename)",
        "            if str(module_path).startswith(str(lab_src)):",
        "                continue",
        "            if str(module_path) in ignore_callers:",
        "                continue",
        "            return str(module_path)",
        "",
        "        return ''",
        "",
        "    def _create_checkpoint_saver(self) -> Optional[CheckpointSaver]:",
        "        return None",
        "",
        "    def __print_info_and_check_repo(self):",
        "        \"\"\"",
        "        ## 🖨 Print the experiment info and check git repo status",
        "        \"\"\"",
        "",
        "        logger.new_line()",
        "        logger.log([",
        "            (self.name, Text.title),",
        "            ': ',",
        "            (str(self.run.index), Text.meta)",
        "        ])",
        "",
        "        if self.run.comment != '':",
        "            logger.log(['\\t', (self.run.comment, Text.highlight)])",
        "",
        "        logger.log([",
        "            \"\\t\"",
        "            \"[dirty]\" if self.run.is_dirty else \"[clean]\",",
        "            \": \",",
        "            (f\"\\\"{self.run.commit_message.strip()}\\\"\", Text.highlight)",
        "        ])",
        "",
        "        # Exit if git repository is dirty",
        "        if self.check_repo_dirty and self.run.is_dirty:",
        "            logger.log([(\"[FAIL]\", Text.danger),",
        "                        \" Cannot trial an experiment with uncommitted changes.\"])",
        "            exit(1)",
        "",
        "    def _load_checkpoint(self, checkpoint_path: pathlib.PurePath):",
        "        raise NotImplementedError()",
        "",
        "    def calc_configs(self,",
        "                     configs: Optional[Configs],",
        "                     configs_dict: Dict[str, any] = None,",
        "                     run_order: Optional[List[Union[List[str], str]]] = None):",
        "        if configs_dict is None:",
        "            configs_dict = {}",
        "        self.configs_processor = ConfigProcessor(configs, configs_dict)",
        "        self.configs_processor(run_order)",
        "        logger.new_line()",
        "",
        "    def __start_from_checkpoint(self, run_index: int, checkpoint: int):",
        "        checkpoint_path, global_step = experiment_run.get_last_run_checkpoint(",
        "            self.experiment_path,",
        "            run_index,",
        "            checkpoint,",
        "            {self.run.index})",
        "",
        "        if global_step is None:",
        "            return 0",
        "        else:",
        "            with logger.section(\"Loading checkpoint\"):",
        "                self._load_checkpoint(checkpoint_path)",
        "",
        "        return global_step",
        "",
        "    def start(self, *,",
        "              run_index: Optional[int] = None,",
        "              checkpoint: Optional[int] = None):",
        "        if run_index is not None:",
        "            if checkpoint is None:",
        "                checkpoint = -1",
        "            global_step = self.__start_from_checkpoint(run_index, checkpoint)",
        "        else:",
        "            global_step = 0",
        "",
        "        self.run.start_step = global_step",
        "        logger.internal().set_start_global_step(global_step)",
        "",
        "        self.__print_info_and_check_repo()",
        "        if self.configs_processor is not None:",
        "            self.configs_processor.print()",
        "",
        "        self.run.save_info()",
        "",
        "        if self.configs_processor is not None:",
        "            self.configs_processor.save(self.run.configs_path)",
        "",
        "        logger.internal().save_indicators(self.run.indicators_path)"
      ],
      "note": "## Experiment\n\nEach experiment has major differences in configurations or algorithms.\nA single experiment can have multiple runs with different hyper-parameters.",
      "collapsed": false,
      "codeCollapsed": false
    }
  ],
  "samples/logger_sections.py": [
    {
      "pre": [
        "",
        "from lab import logger",
        "",
        "",
        "def simple_section():"
      ],
      "post": [
        "",
        "",
        "def unsuccessful_section():",
        "    with logger.section(\"Unsuccessful section\"):",
        "        time.sleep(1)"
      ],
      "code": [
        "    with logger.section(\"Simple section\"):",
        "        # code to load data",
        "        time.sleep(2)"
      ],
      "note": "Sections let you group code, and outputs the timings to the console.\n\n```\nSimple section...[DONE]\t2,003.33ms\n```",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "        # code to load data",
        "        time.sleep(2)",
        "",
        "",
        "def unsuccessful_section():"
      ],
      "post": [
        "",
        "",
        "def progress():",
        "    with logger.section(\"Progress\", total_steps=100):",
        "        for i in range(100):"
      ],
      "code": [
        "    with logger.section(\"Unsuccessful section\"):",
        "        time.sleep(1)",
        "        logger.set_successful(False)"
      ],
      "note": "You can mark if the section was unsuccessful.\n\n```\nUnsuccessful section...[FAIL]\t1,004.93ms\n```",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "        time.sleep(1)",
        "        logger.set_successful(False)",
        "",
        "",
        "def progress():"
      ],
      "post": [
        "",
        "",
        "def loop_section():",
        "    for step in logger.loop(range(0, 10)):",
        "        with logger.section(\"Step\"):"
      ],
      "code": [
        "    with logger.section(\"Progress\", total_steps=100):",
        "        for i in range(100):",
        "            time.sleep(0.1)",
        "            # Multiple training steps in the inner loop",
        "            logger.progress(i)"
      ],
      "note": "Sections can show progress.\n\n```\nProgress    4%\t4,333.51ms\n```",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "            # Multiple training steps in the inner loop",
        "            logger.progress(i)",
        "",
        "",
        "def loop_section():"
      ],
      "post": [
        "",
        "",
        "def loop_partial_section():",
        "    for step in logger.loop(range(0, 10)):",
        "        with logger.section(\"Step\", is_partial=True):"
      ],
      "code": [
        "    for step in logger.loop(range(0, 10)):",
        "        with logger.section(\"Step\"):",
        "            time.sleep(0.5)",
        "        with logger.section(\"Step2\"):",
        "            time.sleep(0.1)",
        "        logger.write()",
        "    logger.new_line()"
      ],
      "note": "Loop is a monitored iterator that keeps track of timing and manages console output. You can have sections inside a loop.\n\n```\n 9:  Step: 100% 478ms  Step2: 100% 98ms    608ms  0:00m/  0:00m\n```",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "        logger.write()",
        "    logger.new_line()",
        "",
        "",
        "def loop_partial_section():"
      ],
      "post": [
        "",
        "",
        "if __name__ == '__main__':",
        "    simple_section()",
        "    unsuccessful_section()"
      ],
      "code": [
        "    for step in logger.loop(range(0, 10)):",
        "        with logger.section(\"Step\", is_partial=True):",
        "            time.sleep(0.5)",
        "            logger.progress((step % 5 + 1) / 5)",
        "        logger.write()"
      ],
      "note": "A partial section can progressively do work inside a loop.\n\n```\n       9:  Step:  80% 2,337ms    503ms  0:00m/  0:00m  \n```",
      "collapsed": false,
      "codeCollapsed": false
    }
  ]
}